{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import lightgbm as lgb\n",
    "\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score, matthews_corrcoef, precision_score, recall_score, make_scorer\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation + Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42 \n",
    "LABEL_COLS = [\"CPPG.STATUS_SPS\", \"CPPG.STATUS_U0\", \"CPPG.STATUS_HEIGHT\", \"CPPG.STATUS_IMPEDANCE\", \"CPPG.STATUS_FORMATION\", \"CPPG.STATUS_MODUL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Data/TMTSA_Daten.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all Columns with only 1 value (no information gain for the algorithm) --> Removing 24 Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_without_information_gain = []\n",
    "\n",
    "for column in df.columns:\n",
    "    if len(df[column].value_counts()) == 1:\n",
    "        columns_without_information_gain.append(column)\n",
    "\n",
    "df_only_information_gain = df.drop(columns_without_information_gain, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CPW.Werk',\n",
       " 'CPW.CATHODE_FZLENGTH_LOWER',\n",
       " 'CPM.Werk',\n",
       " 'CPX.Werk',\n",
       " 'CPFORM.SenderName',\n",
       " 'CPFORM.PROCESSDESCRIPTION',\n",
       " 'CPFORM.Werk',\n",
       " 'CPPG.Werk',\n",
       " 'CPPG.STATUS_MODUL',\n",
       " 'CPX.Werk_1',\n",
       " 'CPX.Werk_2',\n",
       " 'CPW.QUALITY_INSPECTION',\n",
       " 'CPX.CONV_Result001_1',\n",
       " 'CPM.RejectScan',\n",
       " 'CPM.RejectScan1',\n",
       " 'CPX.CONV_Result001_2',\n",
       " 'CPX.CONV_ACOHs047_2',\n",
       " 'CPX.CONV_ACOHs048_2',\n",
       " 'CPM.RejectWelding',\n",
       " 'CPPG.REJECTED',\n",
       " 'CPW.Soll_DMC',\n",
       " 'CPX.CONV_ACOHs049',\n",
       " 'CPX.CONV_ACOHs050',\n",
       " 'CPM.WeldLogfile_Nr']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_without_information_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>CPW.Name</th>\n",
       "      <th>CPW.Linie</th>\n",
       "      <th>CPW.Timestamp</th>\n",
       "      <th>CPW.ANODE_FZLENGTH</th>\n",
       "      <th>CPW.ANODE_LENGTH</th>\n",
       "      <th>CPW.CATHODE_FZLENGTH_UPPER</th>\n",
       "      <th>CPW.CATHODE_LENGTH</th>\n",
       "      <th>CPW.DIAMTER_MAX</th>\n",
       "      <th>...</th>\n",
       "      <th>CPX.CONV_ACOHs043_2</th>\n",
       "      <th>CPX.CONV_ACOHs044_2</th>\n",
       "      <th>CPX.CONV_ACOHs045_2</th>\n",
       "      <th>CPX.CONV_ACOHs046_2</th>\n",
       "      <th>CPX.AI_ACOHs055</th>\n",
       "      <th>CPX.AI_ACOHs056</th>\n",
       "      <th>CPX.CONV_ACOHs045_1</th>\n",
       "      <th>CPX.CONV_ACOHs046_1</th>\n",
       "      <th>CPX.CONV_ACOHs047_1</th>\n",
       "      <th>CPX.CONV_ACOHs048_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>CPMW26</td>\n",
       "      <td>710.0</td>\n",
       "      <td>2022-08-29 02:00:32</td>\n",
       "      <td>61.268271</td>\n",
       "      <td>350.282969</td>\n",
       "      <td>9.179729</td>\n",
       "      <td>302.502813</td>\n",
       "      <td>12.28662</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CPMW23</td>\n",
       "      <td>707.0</td>\n",
       "      <td>2022-08-20 12:42:20</td>\n",
       "      <td>57.259561</td>\n",
       "      <td>348.452969</td>\n",
       "      <td>10.880899</td>\n",
       "      <td>301.902344</td>\n",
       "      <td>12.26344</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>CPMW53</td>\n",
       "      <td>725.0</td>\n",
       "      <td>2022-08-31 16:23:37</td>\n",
       "      <td>58.295284</td>\n",
       "      <td>351.588750</td>\n",
       "      <td>10.690954</td>\n",
       "      <td>302.626719</td>\n",
       "      <td>12.29760</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>CPMW27</td>\n",
       "      <td>711.0</td>\n",
       "      <td>2022-10-11 17:54:46</td>\n",
       "      <td>62.750492</td>\n",
       "      <td>351.843710</td>\n",
       "      <td>10.240900</td>\n",
       "      <td>303.179531</td>\n",
       "      <td>12.22806</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>CPMW23</td>\n",
       "      <td>707.0</td>\n",
       "      <td>2022-08-24 22:20:35</td>\n",
       "      <td>57.823748</td>\n",
       "      <td>350.698770</td>\n",
       "      <td>10.889323</td>\n",
       "      <td>310.547188</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 457 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ID CPW.Name  CPW.Linie       CPW.Timestamp  CPW.ANODE_FZLENGTH  \\\n",
       "0           0   1   CPMW26      710.0 2022-08-29 02:00:32           61.268271   \n",
       "1           1   2   CPMW23      707.0 2022-08-20 12:42:20           57.259561   \n",
       "2           2   3   CPMW53      725.0 2022-08-31 16:23:37           58.295284   \n",
       "3           3   4   CPMW27      711.0 2022-10-11 17:54:46           62.750492   \n",
       "4           4   5   CPMW23      707.0 2022-08-24 22:20:35           57.823748   \n",
       "\n",
       "   CPW.ANODE_LENGTH  CPW.CATHODE_FZLENGTH_UPPER  CPW.CATHODE_LENGTH  \\\n",
       "0        350.282969                    9.179729          302.502813   \n",
       "1        348.452969                   10.880899          301.902344   \n",
       "2        351.588750                   10.690954          302.626719   \n",
       "3        351.843710                   10.240900          303.179531   \n",
       "4        350.698770                   10.889323          310.547188   \n",
       "\n",
       "   CPW.DIAMTER_MAX  ...  CPX.CONV_ACOHs043_2  CPX.CONV_ACOHs044_2  \\\n",
       "0         12.28662  ...                  NaN                  NaN   \n",
       "1         12.26344  ...                  NaN                  NaN   \n",
       "2         12.29760  ...                  NaN                  NaN   \n",
       "3         12.22806  ...                  NaN                  NaN   \n",
       "4          0.00000  ...                  NaN                  NaN   \n",
       "\n",
       "   CPX.CONV_ACOHs045_2  CPX.CONV_ACOHs046_2  CPX.AI_ACOHs055  CPX.AI_ACOHs056  \\\n",
       "0                  NaN                  NaN              NaN              NaN   \n",
       "1                  NaN                  NaN              NaN              NaN   \n",
       "2                  NaN                  NaN              NaN              NaN   \n",
       "3                  NaN                  NaN              NaN              NaN   \n",
       "4                  NaN                  NaN              NaN              NaN   \n",
       "\n",
       "   CPX.CONV_ACOHs045_1  CPX.CONV_ACOHs046_1  CPX.CONV_ACOHs047_1  \\\n",
       "0                  NaN                  NaN                  NaN   \n",
       "1                  NaN                  NaN                  NaN   \n",
       "2                  NaN                  NaN                  NaN   \n",
       "3                  NaN                  NaN                  NaN   \n",
       "4                  NaN                  NaN                  NaN   \n",
       "\n",
       "   CPX.CONV_ACOHs048_1  \n",
       "0                  NaN  \n",
       "1                  NaN  \n",
       "2                  NaN  \n",
       "3                  NaN  \n",
       "4                  NaN  \n",
       "\n",
       "[5 rows x 457 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_only_information_gain.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually removing unnecessary and forbidden columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnecessary_columns = [\n",
    "        \"Unnamed: 0\", \n",
    "        \"ID\", \n",
    "        \"CPW.Linie\", \n",
    "        \"CPM.Linie\", \n",
    "        \"CPM.CellcodeGrade\", \n",
    "        \"CPM.Weld_Logfile_Nr\", \n",
    "        \"CPX.Linie\", \n",
    "        \"CPFORM.Filename\", \n",
    "        \"CPFORM.TestNumber\", \n",
    "        \"CPFORM.Reasoncode\", \n",
    "        \"CPFORM.Reasoncodename\", \n",
    "        \"CPX.TrayResult\"\n",
    "    ]\n",
    "\n",
    "not_allowed_columns = [\n",
    "        \"CPPG.SenderName\", \n",
    "        \"CPPG.SenderTimestamp\", \n",
    "        \"CPPG.Impedance_mOhm\", \n",
    "        \"CPPG.Height_mm\", \n",
    "        \"CPPG.U0_mV\", \n",
    "        \"CPPG.Cell Status\",\n",
    "        \"CPPG.STATUS_SPS\",\n",
    "        \"CPPG.STATUS_U0\",\n",
    "        \"CPPG.STATUS_HEIGHT\",\n",
    "        \"CPPG.STATUS_IMPEDANCE\",\n",
    "        \"CPPG.STATUS_FORMATION\"\n",
    "    ]\n",
    "\n",
    "df_removed_cols = df_only_information_gain.drop(unnecessary_columns + not_allowed_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CPW.Name</th>\n",
       "      <th>CPW.Timestamp</th>\n",
       "      <th>CPW.ANODE_FZLENGTH</th>\n",
       "      <th>CPW.ANODE_LENGTH</th>\n",
       "      <th>CPW.CATHODE_FZLENGTH_UPPER</th>\n",
       "      <th>CPW.CATHODE_LENGTH</th>\n",
       "      <th>CPW.DIAMTER_MAX</th>\n",
       "      <th>CPW.DIAMTER_MIN</th>\n",
       "      <th>CPW.DIAMTER_AVERAGE</th>\n",
       "      <th>CPW.ANODETABPOSITION</th>\n",
       "      <th>...</th>\n",
       "      <th>CPX.CONV_ACOHs043_2</th>\n",
       "      <th>CPX.CONV_ACOHs044_2</th>\n",
       "      <th>CPX.CONV_ACOHs045_2</th>\n",
       "      <th>CPX.CONV_ACOHs046_2</th>\n",
       "      <th>CPX.AI_ACOHs055</th>\n",
       "      <th>CPX.AI_ACOHs056</th>\n",
       "      <th>CPX.CONV_ACOHs045_1</th>\n",
       "      <th>CPX.CONV_ACOHs046_1</th>\n",
       "      <th>CPX.CONV_ACOHs047_1</th>\n",
       "      <th>CPX.CONV_ACOHs048_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CPMW26</td>\n",
       "      <td>2022-08-29 02:00:32</td>\n",
       "      <td>61.268271</td>\n",
       "      <td>350.282969</td>\n",
       "      <td>9.179729</td>\n",
       "      <td>302.502813</td>\n",
       "      <td>12.28662</td>\n",
       "      <td>12.03286</td>\n",
       "      <td>12.12436</td>\n",
       "      <td>91.861033</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CPMW23</td>\n",
       "      <td>2022-08-20 12:42:20</td>\n",
       "      <td>57.259561</td>\n",
       "      <td>348.452969</td>\n",
       "      <td>10.880899</td>\n",
       "      <td>301.902344</td>\n",
       "      <td>12.26344</td>\n",
       "      <td>12.13656</td>\n",
       "      <td>12.20000</td>\n",
       "      <td>49.241435</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CPMW53</td>\n",
       "      <td>2022-08-31 16:23:37</td>\n",
       "      <td>58.295284</td>\n",
       "      <td>351.588750</td>\n",
       "      <td>10.690954</td>\n",
       "      <td>302.626719</td>\n",
       "      <td>12.29760</td>\n",
       "      <td>12.04140</td>\n",
       "      <td>12.14632</td>\n",
       "      <td>120.583939</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CPMW27</td>\n",
       "      <td>2022-10-11 17:54:46</td>\n",
       "      <td>62.750492</td>\n",
       "      <td>351.843710</td>\n",
       "      <td>10.240900</td>\n",
       "      <td>303.179531</td>\n",
       "      <td>12.22806</td>\n",
       "      <td>12.04628</td>\n",
       "      <td>12.11948</td>\n",
       "      <td>368.920286</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CPMW23</td>\n",
       "      <td>2022-08-24 22:20:35</td>\n",
       "      <td>57.823748</td>\n",
       "      <td>350.698770</td>\n",
       "      <td>10.889323</td>\n",
       "      <td>310.547188</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>196.501964</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 434 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  CPW.Name       CPW.Timestamp  CPW.ANODE_FZLENGTH  CPW.ANODE_LENGTH  \\\n",
       "0   CPMW26 2022-08-29 02:00:32           61.268271        350.282969   \n",
       "1   CPMW23 2022-08-20 12:42:20           57.259561        348.452969   \n",
       "2   CPMW53 2022-08-31 16:23:37           58.295284        351.588750   \n",
       "3   CPMW27 2022-10-11 17:54:46           62.750492        351.843710   \n",
       "4   CPMW23 2022-08-24 22:20:35           57.823748        350.698770   \n",
       "\n",
       "   CPW.CATHODE_FZLENGTH_UPPER  CPW.CATHODE_LENGTH  CPW.DIAMTER_MAX  \\\n",
       "0                    9.179729          302.502813         12.28662   \n",
       "1                   10.880899          301.902344         12.26344   \n",
       "2                   10.690954          302.626719         12.29760   \n",
       "3                   10.240900          303.179531         12.22806   \n",
       "4                   10.889323          310.547188          0.00000   \n",
       "\n",
       "   CPW.DIAMTER_MIN  CPW.DIAMTER_AVERAGE  CPW.ANODETABPOSITION  ...  \\\n",
       "0         12.03286             12.12436             91.861033  ...   \n",
       "1         12.13656             12.20000             49.241435  ...   \n",
       "2         12.04140             12.14632            120.583939  ...   \n",
       "3         12.04628             12.11948            368.920286  ...   \n",
       "4          0.00000              0.00000            196.501964  ...   \n",
       "\n",
       "   CPX.CONV_ACOHs043_2  CPX.CONV_ACOHs044_2  CPX.CONV_ACOHs045_2  \\\n",
       "0                  NaN                  NaN                  NaN   \n",
       "1                  NaN                  NaN                  NaN   \n",
       "2                  NaN                  NaN                  NaN   \n",
       "3                  NaN                  NaN                  NaN   \n",
       "4                  NaN                  NaN                  NaN   \n",
       "\n",
       "   CPX.CONV_ACOHs046_2  CPX.AI_ACOHs055  CPX.AI_ACOHs056  CPX.CONV_ACOHs045_1  \\\n",
       "0                  NaN              NaN              NaN                  NaN   \n",
       "1                  NaN              NaN              NaN                  NaN   \n",
       "2                  NaN              NaN              NaN                  NaN   \n",
       "3                  NaN              NaN              NaN                  NaN   \n",
       "4                  NaN              NaN              NaN                  NaN   \n",
       "\n",
       "   CPX.CONV_ACOHs046_1  CPX.CONV_ACOHs047_1  CPX.CONV_ACOHs048_1  \n",
       "0                  NaN                  NaN                  NaN  \n",
       "1                  NaN                  NaN                  NaN  \n",
       "2                  NaN                  NaN                  NaN  \n",
       "3                  NaN                  NaN                  NaN  \n",
       "4                  NaN                  NaN                  NaN  \n",
       "\n",
       "[5 rows x 434 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_removed_cols.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster the retests and save the information as bool/num variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ray_0_cols = [f\"CPX.AI_ACOHs00{i}\" if i < 10 else f\"CPX.AI_ACOHs0{i}\" for i in range(1, 53)] + [\n",
    "    \"CPX.AI_Result001\", \n",
    "    \"CPX.Tab_Overhang001\", \n",
    "    \"CPX.DIFF_Y1001\", \n",
    "    \"CPX.DIFF_Y2001\",\n",
    "    \"CPX.Failure_Code001\",\n",
    "    \"CPX.Name\",\n",
    "    \"CPX.Xray_Timestamp\",\n",
    "    \"CPX.Result\"\n",
    "]\n",
    "\n",
    "x_ray_1_cols = [f\"CPX.AI_ACOHs00{i}_1\" if i < 10 else f\"CPX.AI_ACOHs0{i}_1\" for i in range(1, 53)] + [\n",
    "    \"CPX.AI_Result001_1\", \n",
    "    \"CPX.Tab_Overhang001_1\", \n",
    "    \"CPX.DIFF_Y1001_1\", \n",
    "    \"CPX.DIFF_Y2001_1\",\n",
    "    \"CPX.Failure_Code001_1\",\n",
    "    \"CPX.Name_1\",\n",
    "    \"CPX.Xray_Timestamp_1\",\n",
    "    \"CPX.Result_1\"\n",
    "]\n",
    "\n",
    "x_ray_2_cols = [f\"CPX.AI_ACOHs00{i}_2\" if i < 10 else f\"CPX.AI_ACOHs0{i}_2\" for i in range(1, 53)] + [\n",
    "    \"CPX.AI_Result001_2\", \n",
    "    \"CPX.Tab_Overhang001_2\", \n",
    "    \"CPX.DIFF_Y1001_2\", \n",
    "    \"CPX.DIFF_Y2001_2\",\n",
    "    \"CPX.Failure_Code001_2\",\n",
    "    \"CPX.Name_2\",\n",
    "    \"CPX.Xray_Timestamp_2\",\n",
    "    \"CPX.Result_2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x_ray_cluster = df_removed_cols.copy()\n",
    "retest_bool = []\n",
    "retest_num = []\n",
    "\n",
    "for index, row in df_removed_cols.iterrows():\n",
    "    for position, column in enumerate(x_ray_0_cols):\n",
    "        boolean = None\n",
    "        number = None\n",
    "        if not pd.isna(row[x_ray_2_cols[position]]):\n",
    "            df_x_ray_cluster.loc[index, column] = row[x_ray_2_cols[position]]  \n",
    "            boolean = True\n",
    "            number = 2\n",
    "        elif not pd.isna(row[x_ray_1_cols[position]]):\n",
    "            df_x_ray_cluster.loc[index, column] = row[x_ray_1_cols[position]]\n",
    "            boolean = True\n",
    "            number = 1\n",
    "        else:\n",
    "            boolean = False\n",
    "            number = 0\n",
    "    retest_bool.append(boolean)\n",
    "    retest_num.append(number)\n",
    "\n",
    "df_x_ray_cluster[\"CPX.Retest_Bool\"] = retest_bool\n",
    "df_x_ray_cluster[\"CPX.Retest_Num\"] = retest_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 120 columns (still some x ray cols included which will be removed based on feature importance)\n",
    "df_x_ray_cleaned = df_x_ray_cluster.drop(x_ray_1_cols + x_ray_2_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CPX.Retest_Num\n",
       "0    981788\n",
       "1     18099\n",
       "2       113\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x_ray_cleaned[\"CPX.Retest_Num\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CPW.Name</th>\n",
       "      <th>CPW.Timestamp</th>\n",
       "      <th>CPW.ANODE_FZLENGTH</th>\n",
       "      <th>CPW.ANODE_LENGTH</th>\n",
       "      <th>CPW.CATHODE_FZLENGTH_UPPER</th>\n",
       "      <th>CPW.CATHODE_LENGTH</th>\n",
       "      <th>CPW.DIAMTER_MAX</th>\n",
       "      <th>CPW.DIAMTER_MIN</th>\n",
       "      <th>CPW.DIAMTER_AVERAGE</th>\n",
       "      <th>CPW.ANODETABPOSITION</th>\n",
       "      <th>...</th>\n",
       "      <th>CPX.CONV_ACOHs045_2</th>\n",
       "      <th>CPX.CONV_ACOHs046_2</th>\n",
       "      <th>CPX.AI_ACOHs055</th>\n",
       "      <th>CPX.AI_ACOHs056</th>\n",
       "      <th>CPX.CONV_ACOHs045_1</th>\n",
       "      <th>CPX.CONV_ACOHs046_1</th>\n",
       "      <th>CPX.CONV_ACOHs047_1</th>\n",
       "      <th>CPX.CONV_ACOHs048_1</th>\n",
       "      <th>CPX.Retest_Bool</th>\n",
       "      <th>CPX.Retest_Num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CPMW26</td>\n",
       "      <td>2022-08-29 02:00:32</td>\n",
       "      <td>61.268271</td>\n",
       "      <td>350.282969</td>\n",
       "      <td>9.179729</td>\n",
       "      <td>302.502813</td>\n",
       "      <td>12.28662</td>\n",
       "      <td>12.03286</td>\n",
       "      <td>12.12436</td>\n",
       "      <td>91.861033</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CPMW23</td>\n",
       "      <td>2022-08-20 12:42:20</td>\n",
       "      <td>57.259561</td>\n",
       "      <td>348.452969</td>\n",
       "      <td>10.880899</td>\n",
       "      <td>301.902344</td>\n",
       "      <td>12.26344</td>\n",
       "      <td>12.13656</td>\n",
       "      <td>12.20000</td>\n",
       "      <td>49.241435</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CPMW53</td>\n",
       "      <td>2022-08-31 16:23:37</td>\n",
       "      <td>58.295284</td>\n",
       "      <td>351.588750</td>\n",
       "      <td>10.690954</td>\n",
       "      <td>302.626719</td>\n",
       "      <td>12.29760</td>\n",
       "      <td>12.04140</td>\n",
       "      <td>12.14632</td>\n",
       "      <td>120.583939</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CPMW27</td>\n",
       "      <td>2022-10-11 17:54:46</td>\n",
       "      <td>62.750492</td>\n",
       "      <td>351.843710</td>\n",
       "      <td>10.240900</td>\n",
       "      <td>303.179531</td>\n",
       "      <td>12.22806</td>\n",
       "      <td>12.04628</td>\n",
       "      <td>12.11948</td>\n",
       "      <td>368.920286</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CPMW23</td>\n",
       "      <td>2022-08-24 22:20:35</td>\n",
       "      <td>57.823748</td>\n",
       "      <td>350.698770</td>\n",
       "      <td>10.889323</td>\n",
       "      <td>310.547188</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>196.501964</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 316 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  CPW.Name       CPW.Timestamp  CPW.ANODE_FZLENGTH  CPW.ANODE_LENGTH  \\\n",
       "0   CPMW26 2022-08-29 02:00:32           61.268271        350.282969   \n",
       "1   CPMW23 2022-08-20 12:42:20           57.259561        348.452969   \n",
       "2   CPMW53 2022-08-31 16:23:37           58.295284        351.588750   \n",
       "3   CPMW27 2022-10-11 17:54:46           62.750492        351.843710   \n",
       "4   CPMW23 2022-08-24 22:20:35           57.823748        350.698770   \n",
       "\n",
       "   CPW.CATHODE_FZLENGTH_UPPER  CPW.CATHODE_LENGTH  CPW.DIAMTER_MAX  \\\n",
       "0                    9.179729          302.502813         12.28662   \n",
       "1                   10.880899          301.902344         12.26344   \n",
       "2                   10.690954          302.626719         12.29760   \n",
       "3                   10.240900          303.179531         12.22806   \n",
       "4                   10.889323          310.547188          0.00000   \n",
       "\n",
       "   CPW.DIAMTER_MIN  CPW.DIAMTER_AVERAGE  CPW.ANODETABPOSITION  ...  \\\n",
       "0         12.03286             12.12436             91.861033  ...   \n",
       "1         12.13656             12.20000             49.241435  ...   \n",
       "2         12.04140             12.14632            120.583939  ...   \n",
       "3         12.04628             12.11948            368.920286  ...   \n",
       "4          0.00000              0.00000            196.501964  ...   \n",
       "\n",
       "   CPX.CONV_ACOHs045_2  CPX.CONV_ACOHs046_2  CPX.AI_ACOHs055  CPX.AI_ACOHs056  \\\n",
       "0                  NaN                  NaN              NaN              NaN   \n",
       "1                  NaN                  NaN              NaN              NaN   \n",
       "2                  NaN                  NaN              NaN              NaN   \n",
       "3                  NaN                  NaN              NaN              NaN   \n",
       "4                  NaN                  NaN              NaN              NaN   \n",
       "\n",
       "   CPX.CONV_ACOHs045_1  CPX.CONV_ACOHs046_1  CPX.CONV_ACOHs047_1  \\\n",
       "0                  NaN                  NaN                  NaN   \n",
       "1                  NaN                  NaN                  NaN   \n",
       "2                  NaN                  NaN                  NaN   \n",
       "3                  NaN                  NaN                  NaN   \n",
       "4                  NaN                  NaN                  NaN   \n",
       "\n",
       "   CPX.CONV_ACOHs048_1  CPX.Retest_Bool  CPX.Retest_Num  \n",
       "0                  NaN            False               0  \n",
       "1                  NaN            False               0  \n",
       "2                  NaN            False               0  \n",
       "3                  NaN            False               0  \n",
       "4                  NaN            False               0  \n",
       "\n",
       "[5 rows x 316 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x_ray_cleaned.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duration aus Timestamps berechnen und originale Timestamps entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_x_ray_cluster[\"CPW.Duration\"] = \n",
    "cpw_duration = df_x_ray_cleaned[\"CPM.Timestamp\"]-df_x_ray_cleaned[\"CPW.Timestamp\"]\n",
    "cpw_duration = cpw_duration.dt.total_seconds()\n",
    "\n",
    "cpm_duration = df_x_ray_cleaned[\"CPX.Xray_Timestamp\"]-df_x_ray_cleaned[\"CPM.Timestamp\"]\n",
    "cpm_duration = cpm_duration.dt.total_seconds()\n",
    "\n",
    "df_x_ray_cleaned[\"CPW.Duration\"] = cpw_duration\n",
    "df_x_ray_cleaned[\"CPM.Duration\"] = cpm_duration\n",
    "\n",
    "df_prepared = df_x_ray_cleaned.drop([\"CPW.Timestamp\", \"CPM.Timestamp\", \"CPX.Xray_Timestamp\"], axis=1) # Remove unnecessary timestamps (other timestamps will be removed later as CPForm data will be removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CPW.Name</th>\n",
       "      <th>CPW.ANODE_FZLENGTH</th>\n",
       "      <th>CPW.ANODE_LENGTH</th>\n",
       "      <th>CPW.CATHODE_FZLENGTH_UPPER</th>\n",
       "      <th>CPW.CATHODE_LENGTH</th>\n",
       "      <th>CPW.DIAMTER_MAX</th>\n",
       "      <th>CPW.DIAMTER_MIN</th>\n",
       "      <th>CPW.DIAMTER_AVERAGE</th>\n",
       "      <th>CPW.ANODETABPOSITION</th>\n",
       "      <th>CPW.CATHODETABPOSITION</th>\n",
       "      <th>...</th>\n",
       "      <th>CPX.AI_ACOHs055</th>\n",
       "      <th>CPX.AI_ACOHs056</th>\n",
       "      <th>CPX.CONV_ACOHs045_1</th>\n",
       "      <th>CPX.CONV_ACOHs046_1</th>\n",
       "      <th>CPX.CONV_ACOHs047_1</th>\n",
       "      <th>CPX.CONV_ACOHs048_1</th>\n",
       "      <th>CPX.Retest_Bool</th>\n",
       "      <th>CPX.Retest_Num</th>\n",
       "      <th>CPW.Duration</th>\n",
       "      <th>CPM.Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CPMW26</td>\n",
       "      <td>61.268271</td>\n",
       "      <td>350.282969</td>\n",
       "      <td>9.179729</td>\n",
       "      <td>302.502813</td>\n",
       "      <td>12.28662</td>\n",
       "      <td>12.03286</td>\n",
       "      <td>12.12436</td>\n",
       "      <td>91.861033</td>\n",
       "      <td>1.173663</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>395044.0</td>\n",
       "      <td>33503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CPMW23</td>\n",
       "      <td>57.259561</td>\n",
       "      <td>348.452969</td>\n",
       "      <td>10.880899</td>\n",
       "      <td>301.902344</td>\n",
       "      <td>12.26344</td>\n",
       "      <td>12.13656</td>\n",
       "      <td>12.20000</td>\n",
       "      <td>49.241435</td>\n",
       "      <td>402.458856</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>734387.0</td>\n",
       "      <td>2131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CPMW53</td>\n",
       "      <td>58.295284</td>\n",
       "      <td>351.588750</td>\n",
       "      <td>10.690954</td>\n",
       "      <td>302.626719</td>\n",
       "      <td>12.29760</td>\n",
       "      <td>12.04140</td>\n",
       "      <td>12.14632</td>\n",
       "      <td>120.583939</td>\n",
       "      <td>53.235782</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>335387.0</td>\n",
       "      <td>19343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CPMW27</td>\n",
       "      <td>62.750492</td>\n",
       "      <td>351.843710</td>\n",
       "      <td>10.240900</td>\n",
       "      <td>303.179531</td>\n",
       "      <td>12.22806</td>\n",
       "      <td>12.04628</td>\n",
       "      <td>12.11948</td>\n",
       "      <td>368.920286</td>\n",
       "      <td>268.658312</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>126924.0</td>\n",
       "      <td>32637.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CPMW23</td>\n",
       "      <td>57.823748</td>\n",
       "      <td>350.698770</td>\n",
       "      <td>10.889323</td>\n",
       "      <td>310.547188</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>196.501964</td>\n",
       "      <td>106.837010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>476852.0</td>\n",
       "      <td>32222.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 315 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  CPW.Name  CPW.ANODE_FZLENGTH  CPW.ANODE_LENGTH  CPW.CATHODE_FZLENGTH_UPPER  \\\n",
       "0   CPMW26           61.268271        350.282969                    9.179729   \n",
       "1   CPMW23           57.259561        348.452969                   10.880899   \n",
       "2   CPMW53           58.295284        351.588750                   10.690954   \n",
       "3   CPMW27           62.750492        351.843710                   10.240900   \n",
       "4   CPMW23           57.823748        350.698770                   10.889323   \n",
       "\n",
       "   CPW.CATHODE_LENGTH  CPW.DIAMTER_MAX  CPW.DIAMTER_MIN  CPW.DIAMTER_AVERAGE  \\\n",
       "0          302.502813         12.28662         12.03286             12.12436   \n",
       "1          301.902344         12.26344         12.13656             12.20000   \n",
       "2          302.626719         12.29760         12.04140             12.14632   \n",
       "3          303.179531         12.22806         12.04628             12.11948   \n",
       "4          310.547188          0.00000          0.00000              0.00000   \n",
       "\n",
       "   CPW.ANODETABPOSITION  CPW.CATHODETABPOSITION  ...  CPX.AI_ACOHs055  \\\n",
       "0             91.861033                1.173663  ...              NaN   \n",
       "1             49.241435              402.458856  ...              NaN   \n",
       "2            120.583939               53.235782  ...              NaN   \n",
       "3            368.920286              268.658312  ...              NaN   \n",
       "4            196.501964              106.837010  ...              NaN   \n",
       "\n",
       "   CPX.AI_ACOHs056  CPX.CONV_ACOHs045_1  CPX.CONV_ACOHs046_1  \\\n",
       "0              NaN                  NaN                  NaN   \n",
       "1              NaN                  NaN                  NaN   \n",
       "2              NaN                  NaN                  NaN   \n",
       "3              NaN                  NaN                  NaN   \n",
       "4              NaN                  NaN                  NaN   \n",
       "\n",
       "   CPX.CONV_ACOHs047_1  CPX.CONV_ACOHs048_1  CPX.Retest_Bool  CPX.Retest_Num  \\\n",
       "0                  NaN                  NaN            False               0   \n",
       "1                  NaN                  NaN            False               0   \n",
       "2                  NaN                  NaN            False               0   \n",
       "3                  NaN                  NaN            False               0   \n",
       "4                  NaN                  NaN            False               0   \n",
       "\n",
       "   CPW.Duration  CPM.Duration  \n",
       "0      395044.0       33503.0  \n",
       "1      734387.0        2131.0  \n",
       "2      335387.0       19343.0  \n",
       "3      126924.0       32637.0  \n",
       "4      476852.0       32222.0  \n",
       "\n",
       "[5 rows x 315 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepared.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the cp_form step values (12 Columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_form_features = [\"CPFORM.SenderTimestamp\", \"CPFORM.StartTime\", \"CPFORM.Duration_s\", \"CPFORM.TrayXID\", \"CPFORM.TrayPosition\", \"CPFORM.QualityStatus\", \"CPFORM.Grade\", \"CPFORM.ChgCap_mAh_Cycle0/0\", \"CPFORM.DsgCap_mAh_Cycle0/0\", \"CPFORM.OCV1_mV\", \"CPFORM.OCV1_Timestamp\", \"CPFORM.Rack\"]\n",
    "\n",
    "df_no_cp_form = df_prepared.drop(cp_form_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CPW.Name</th>\n",
       "      <th>CPW.ANODE_FZLENGTH</th>\n",
       "      <th>CPW.ANODE_LENGTH</th>\n",
       "      <th>CPW.CATHODE_FZLENGTH_UPPER</th>\n",
       "      <th>CPW.CATHODE_LENGTH</th>\n",
       "      <th>CPW.DIAMTER_MAX</th>\n",
       "      <th>CPW.DIAMTER_MIN</th>\n",
       "      <th>CPW.DIAMTER_AVERAGE</th>\n",
       "      <th>CPW.ANODETABPOSITION</th>\n",
       "      <th>CPW.CATHODETABPOSITION</th>\n",
       "      <th>...</th>\n",
       "      <th>CPX.AI_ACOHs055</th>\n",
       "      <th>CPX.AI_ACOHs056</th>\n",
       "      <th>CPX.CONV_ACOHs045_1</th>\n",
       "      <th>CPX.CONV_ACOHs046_1</th>\n",
       "      <th>CPX.CONV_ACOHs047_1</th>\n",
       "      <th>CPX.CONV_ACOHs048_1</th>\n",
       "      <th>CPX.Retest_Bool</th>\n",
       "      <th>CPX.Retest_Num</th>\n",
       "      <th>CPW.Duration</th>\n",
       "      <th>CPM.Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CPMW26</td>\n",
       "      <td>61.268271</td>\n",
       "      <td>350.282969</td>\n",
       "      <td>9.179729</td>\n",
       "      <td>302.502813</td>\n",
       "      <td>12.28662</td>\n",
       "      <td>12.03286</td>\n",
       "      <td>12.12436</td>\n",
       "      <td>91.861033</td>\n",
       "      <td>1.173663</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>395044.0</td>\n",
       "      <td>33503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CPMW23</td>\n",
       "      <td>57.259561</td>\n",
       "      <td>348.452969</td>\n",
       "      <td>10.880899</td>\n",
       "      <td>301.902344</td>\n",
       "      <td>12.26344</td>\n",
       "      <td>12.13656</td>\n",
       "      <td>12.20000</td>\n",
       "      <td>49.241435</td>\n",
       "      <td>402.458856</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>734387.0</td>\n",
       "      <td>2131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CPMW53</td>\n",
       "      <td>58.295284</td>\n",
       "      <td>351.588750</td>\n",
       "      <td>10.690954</td>\n",
       "      <td>302.626719</td>\n",
       "      <td>12.29760</td>\n",
       "      <td>12.04140</td>\n",
       "      <td>12.14632</td>\n",
       "      <td>120.583939</td>\n",
       "      <td>53.235782</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>335387.0</td>\n",
       "      <td>19343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CPMW27</td>\n",
       "      <td>62.750492</td>\n",
       "      <td>351.843710</td>\n",
       "      <td>10.240900</td>\n",
       "      <td>303.179531</td>\n",
       "      <td>12.22806</td>\n",
       "      <td>12.04628</td>\n",
       "      <td>12.11948</td>\n",
       "      <td>368.920286</td>\n",
       "      <td>268.658312</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>126924.0</td>\n",
       "      <td>32637.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CPMW23</td>\n",
       "      <td>57.823748</td>\n",
       "      <td>350.698770</td>\n",
       "      <td>10.889323</td>\n",
       "      <td>310.547188</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>196.501964</td>\n",
       "      <td>106.837010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>476852.0</td>\n",
       "      <td>32222.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  CPW.Name  CPW.ANODE_FZLENGTH  CPW.ANODE_LENGTH  CPW.CATHODE_FZLENGTH_UPPER  \\\n",
       "0   CPMW26           61.268271        350.282969                    9.179729   \n",
       "1   CPMW23           57.259561        348.452969                   10.880899   \n",
       "2   CPMW53           58.295284        351.588750                   10.690954   \n",
       "3   CPMW27           62.750492        351.843710                   10.240900   \n",
       "4   CPMW23           57.823748        350.698770                   10.889323   \n",
       "\n",
       "   CPW.CATHODE_LENGTH  CPW.DIAMTER_MAX  CPW.DIAMTER_MIN  CPW.DIAMTER_AVERAGE  \\\n",
       "0          302.502813         12.28662         12.03286             12.12436   \n",
       "1          301.902344         12.26344         12.13656             12.20000   \n",
       "2          302.626719         12.29760         12.04140             12.14632   \n",
       "3          303.179531         12.22806         12.04628             12.11948   \n",
       "4          310.547188          0.00000          0.00000              0.00000   \n",
       "\n",
       "   CPW.ANODETABPOSITION  CPW.CATHODETABPOSITION  ...  CPX.AI_ACOHs055  \\\n",
       "0             91.861033                1.173663  ...              NaN   \n",
       "1             49.241435              402.458856  ...              NaN   \n",
       "2            120.583939               53.235782  ...              NaN   \n",
       "3            368.920286              268.658312  ...              NaN   \n",
       "4            196.501964              106.837010  ...              NaN   \n",
       "\n",
       "   CPX.AI_ACOHs056  CPX.CONV_ACOHs045_1  CPX.CONV_ACOHs046_1  \\\n",
       "0              NaN                  NaN                  NaN   \n",
       "1              NaN                  NaN                  NaN   \n",
       "2              NaN                  NaN                  NaN   \n",
       "3              NaN                  NaN                  NaN   \n",
       "4              NaN                  NaN                  NaN   \n",
       "\n",
       "   CPX.CONV_ACOHs047_1  CPX.CONV_ACOHs048_1  CPX.Retest_Bool  CPX.Retest_Num  \\\n",
       "0                  NaN                  NaN            False               0   \n",
       "1                  NaN                  NaN            False               0   \n",
       "2                  NaN                  NaN            False               0   \n",
       "3                  NaN                  NaN            False               0   \n",
       "4                  NaN                  NaN            False               0   \n",
       "\n",
       "   CPW.Duration  CPM.Duration  \n",
       "0      395044.0       33503.0  \n",
       "1      734387.0        2131.0  \n",
       "2      335387.0       19343.0  \n",
       "3      126924.0       32637.0  \n",
       "4      476852.0       32222.0  \n",
       "\n",
       "[5 rows x 303 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_cp_form.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the min an max value for each column, which includes error values\n",
    "# The idea is that each error value will be 0 after replacement + normalization so the \"error\" needs to be replaced \n",
    "# --> so this value will be replaced with the min value of the column - d * range of the column\n",
    "\n",
    "replacement_dict = {}\n",
    "\n",
    "for i in range(1, 49):\n",
    "    for j in range(3):\n",
    "        col_name = None\n",
    "        if (i == 47 or i == 48) and j == 2:\n",
    "            continue\n",
    "        if i < 10:\n",
    "            if j == 0:\n",
    "                col_name = f\"CPX.CONV_ACOHs00{i}\"\n",
    "            else:\n",
    "                col_name = f\"CPX.CONV_ACOHs00{i}_{j}\"\n",
    "        else:\n",
    "            if j == 0:\n",
    "                col_name = f\"CPX.CONV_ACOHs0{i}\"\n",
    "            else:\n",
    "                col_name = f\"CPX.CONV_ACOHs0{i}_{j}\"\n",
    "        column = df_no_cp_form[col_name]\n",
    "        column_no_error = column[~column.isin([\"error\"])].mean()\n",
    "        column = column.replace(\"error\", column_no_error.mean())\n",
    "        replacement_dict[col_name] = [column.min(), column.max()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually encode columns with mixed datatypes (f.e. floating values in object column --> no label encoding in raw form possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_replaced = df_no_cp_form.copy()\n",
    "d = 0.1\n",
    "\n",
    "for i in range(1, 49):\n",
    "    for j in range(3):\n",
    "        col_name = None\n",
    "        if (i == 47 or i == 48) and j == 2:\n",
    "            continue\n",
    "        if i < 10:\n",
    "            if j == 0:\n",
    "                col_name = f\"CPX.CONV_ACOHs00{i}\"\n",
    "            else:\n",
    "                col_name = f\"CPX.CONV_ACOHs00{i}_{j}\"\n",
    "        else:\n",
    "            if j == 0:\n",
    "                col_name = f\"CPX.CONV_ACOHs0{i}\"\n",
    "            else:\n",
    "                col_name = f\"CPX.CONV_ACOHs0{i}_{j}\"\n",
    "        max_val = replacement_dict[col_name][1]\n",
    "        min_val = replacement_dict[col_name][0]\n",
    "        replacement_value = min_val - d * (max_val - min_val)\n",
    "        df_replaced[col_name] = df_no_cp_form[col_name].replace(\"error\", replacement_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode remaining values to categorical --> save classes in dict for possible decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPW.Name\n",
      "CPM.MaxTimeWT\n",
      "CPM.Name\n",
      "CPX.AI_Result001\n",
      "CPX.Name\n",
      "CPX.Result\n",
      "CPX.Linie_1\n",
      "CPX.TrayResult_1\n",
      "CPX.CONV_Result001\n",
      "CPX.Linie_2\n",
      "CPX.TrayResult_2\n"
     ]
    }
   ],
   "source": [
    "df_encoded = df_replaced.copy()\n",
    "encoding_dict = {}\n",
    "\n",
    "for index, val in enumerate(df_replaced.dtypes):\n",
    "    if val == \"object\" or val == \"datetime64[ns]\":\n",
    "        col_name = df_replaced.columns[index]\n",
    "        print(col_name)\n",
    "        le = LabelEncoder()\n",
    "        df_encoded[col_name] = le.fit_transform(df_replaced[col_name])\n",
    "        encoding_dict[col_name] = le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CPW.Name</th>\n",
       "      <th>CPW.ANODE_FZLENGTH</th>\n",
       "      <th>CPW.ANODE_LENGTH</th>\n",
       "      <th>CPW.CATHODE_FZLENGTH_UPPER</th>\n",
       "      <th>CPW.CATHODE_LENGTH</th>\n",
       "      <th>CPW.DIAMTER_MAX</th>\n",
       "      <th>CPW.DIAMTER_MIN</th>\n",
       "      <th>CPW.DIAMTER_AVERAGE</th>\n",
       "      <th>CPW.ANODETABPOSITION</th>\n",
       "      <th>CPW.CATHODETABPOSITION</th>\n",
       "      <th>...</th>\n",
       "      <th>CPX.AI_ACOHs055</th>\n",
       "      <th>CPX.AI_ACOHs056</th>\n",
       "      <th>CPX.CONV_ACOHs045_1</th>\n",
       "      <th>CPX.CONV_ACOHs046_1</th>\n",
       "      <th>CPX.CONV_ACOHs047_1</th>\n",
       "      <th>CPX.CONV_ACOHs048_1</th>\n",
       "      <th>CPX.Retest_Bool</th>\n",
       "      <th>CPX.Retest_Num</th>\n",
       "      <th>CPW.Duration</th>\n",
       "      <th>CPM.Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>61.268271</td>\n",
       "      <td>350.282969</td>\n",
       "      <td>9.179729</td>\n",
       "      <td>302.502813</td>\n",
       "      <td>12.28662</td>\n",
       "      <td>12.03286</td>\n",
       "      <td>12.12436</td>\n",
       "      <td>91.861033</td>\n",
       "      <td>1.173663</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>395044.0</td>\n",
       "      <td>33503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>57.259561</td>\n",
       "      <td>348.452969</td>\n",
       "      <td>10.880899</td>\n",
       "      <td>301.902344</td>\n",
       "      <td>12.26344</td>\n",
       "      <td>12.13656</td>\n",
       "      <td>12.20000</td>\n",
       "      <td>49.241435</td>\n",
       "      <td>402.458856</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>734387.0</td>\n",
       "      <td>2131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>58.295284</td>\n",
       "      <td>351.588750</td>\n",
       "      <td>10.690954</td>\n",
       "      <td>302.626719</td>\n",
       "      <td>12.29760</td>\n",
       "      <td>12.04140</td>\n",
       "      <td>12.14632</td>\n",
       "      <td>120.583939</td>\n",
       "      <td>53.235782</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>335387.0</td>\n",
       "      <td>19343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>62.750492</td>\n",
       "      <td>351.843710</td>\n",
       "      <td>10.240900</td>\n",
       "      <td>303.179531</td>\n",
       "      <td>12.22806</td>\n",
       "      <td>12.04628</td>\n",
       "      <td>12.11948</td>\n",
       "      <td>368.920286</td>\n",
       "      <td>268.658312</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>126924.0</td>\n",
       "      <td>32637.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>57.823748</td>\n",
       "      <td>350.698770</td>\n",
       "      <td>10.889323</td>\n",
       "      <td>310.547188</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>196.501964</td>\n",
       "      <td>106.837010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>476852.0</td>\n",
       "      <td>32222.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CPW.Name  CPW.ANODE_FZLENGTH  CPW.ANODE_LENGTH  CPW.CATHODE_FZLENGTH_UPPER  \\\n",
       "0         5           61.268271        350.282969                    9.179729   \n",
       "1         2           57.259561        348.452969                   10.880899   \n",
       "2        16           58.295284        351.588750                   10.690954   \n",
       "3         6           62.750492        351.843710                   10.240900   \n",
       "4         2           57.823748        350.698770                   10.889323   \n",
       "\n",
       "   CPW.CATHODE_LENGTH  CPW.DIAMTER_MAX  CPW.DIAMTER_MIN  CPW.DIAMTER_AVERAGE  \\\n",
       "0          302.502813         12.28662         12.03286             12.12436   \n",
       "1          301.902344         12.26344         12.13656             12.20000   \n",
       "2          302.626719         12.29760         12.04140             12.14632   \n",
       "3          303.179531         12.22806         12.04628             12.11948   \n",
       "4          310.547188          0.00000          0.00000              0.00000   \n",
       "\n",
       "   CPW.ANODETABPOSITION  CPW.CATHODETABPOSITION  ...  CPX.AI_ACOHs055  \\\n",
       "0             91.861033                1.173663  ...              NaN   \n",
       "1             49.241435              402.458856  ...              NaN   \n",
       "2            120.583939               53.235782  ...              NaN   \n",
       "3            368.920286              268.658312  ...              NaN   \n",
       "4            196.501964              106.837010  ...              NaN   \n",
       "\n",
       "   CPX.AI_ACOHs056  CPX.CONV_ACOHs045_1  CPX.CONV_ACOHs046_1  \\\n",
       "0              NaN                  NaN                  NaN   \n",
       "1              NaN                  NaN                  NaN   \n",
       "2              NaN                  NaN                  NaN   \n",
       "3              NaN                  NaN                  NaN   \n",
       "4              NaN                  NaN                  NaN   \n",
       "\n",
       "   CPX.CONV_ACOHs047_1  CPX.CONV_ACOHs048_1  CPX.Retest_Bool  CPX.Retest_Num  \\\n",
       "0                  NaN                  NaN            False               0   \n",
       "1                  NaN                  NaN            False               0   \n",
       "2                  NaN                  NaN            False               0   \n",
       "3                  NaN                  NaN            False               0   \n",
       "4                  NaN                  NaN            False               0   \n",
       "\n",
       "   CPW.Duration  CPM.Duration  \n",
       "0      395044.0       33503.0  \n",
       "1      734387.0        2131.0  \n",
       "2      335387.0       19343.0  \n",
       "3      126924.0       32637.0  \n",
       "4      476852.0       32222.0  \n",
       "\n",
       "[5 rows x 303 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min Max Normalization for each column --> save min and max value for each column befor normalization for possible decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_dict = {}\n",
    "\n",
    "for index, name in enumerate(df_encoded.columns):\n",
    "    normalization_dict[name] = [df_encoded[name].min(), df_encoded[name].max()]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = df_encoded.apply(lambda column: pd.Series(scaler.fit_transform(column.values.reshape(-1, 1)).flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CPW.Name</th>\n",
       "      <th>CPW.ANODE_FZLENGTH</th>\n",
       "      <th>CPW.ANODE_LENGTH</th>\n",
       "      <th>CPW.CATHODE_FZLENGTH_UPPER</th>\n",
       "      <th>CPW.CATHODE_LENGTH</th>\n",
       "      <th>CPW.DIAMTER_MAX</th>\n",
       "      <th>CPW.DIAMTER_MIN</th>\n",
       "      <th>CPW.DIAMTER_AVERAGE</th>\n",
       "      <th>CPW.ANODETABPOSITION</th>\n",
       "      <th>CPW.CATHODETABPOSITION</th>\n",
       "      <th>...</th>\n",
       "      <th>CPX.AI_ACOHs055</th>\n",
       "      <th>CPX.AI_ACOHs056</th>\n",
       "      <th>CPX.CONV_ACOHs045_1</th>\n",
       "      <th>CPX.CONV_ACOHs046_1</th>\n",
       "      <th>CPX.CONV_ACOHs047_1</th>\n",
       "      <th>CPX.CONV_ACOHs048_1</th>\n",
       "      <th>CPX.Retest_Bool</th>\n",
       "      <th>CPX.Retest_Num</th>\n",
       "      <th>CPW.Duration</th>\n",
       "      <th>CPM.Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.301151</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.686540</td>\n",
       "      <td>0.006666</td>\n",
       "      <td>0.977767</td>\n",
       "      <td>0.981295</td>\n",
       "      <td>0.984448</td>\n",
       "      <td>0.209158</td>\n",
       "      <td>0.002674</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.118489</td>\n",
       "      <td>0.007054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.285979</td>\n",
       "      <td>0.003219</td>\n",
       "      <td>0.813174</td>\n",
       "      <td>0.006653</td>\n",
       "      <td>0.975922</td>\n",
       "      <td>0.989752</td>\n",
       "      <td>0.990589</td>\n",
       "      <td>0.112118</td>\n",
       "      <td>0.916349</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.220644</td>\n",
       "      <td>0.004977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.289899</td>\n",
       "      <td>0.003248</td>\n",
       "      <td>0.799034</td>\n",
       "      <td>0.006668</td>\n",
       "      <td>0.978641</td>\n",
       "      <td>0.981992</td>\n",
       "      <td>0.986231</td>\n",
       "      <td>0.274556</td>\n",
       "      <td>0.121213</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100530</td>\n",
       "      <td>0.006116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.306761</td>\n",
       "      <td>0.003250</td>\n",
       "      <td>0.765533</td>\n",
       "      <td>0.006681</td>\n",
       "      <td>0.973107</td>\n",
       "      <td>0.982390</td>\n",
       "      <td>0.984051</td>\n",
       "      <td>0.839987</td>\n",
       "      <td>0.611703</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037775</td>\n",
       "      <td>0.006997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.288114</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>0.813801</td>\n",
       "      <td>0.006843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447412</td>\n",
       "      <td>0.243256</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.143117</td>\n",
       "      <td>0.006969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CPW.Name  CPW.ANODE_FZLENGTH  CPW.ANODE_LENGTH  CPW.CATHODE_FZLENGTH_UPPER  \\\n",
       "0  0.192308            0.301151          0.003236                    0.686540   \n",
       "1  0.076923            0.285979          0.003219                    0.813174   \n",
       "2  0.615385            0.289899          0.003248                    0.799034   \n",
       "3  0.230769            0.306761          0.003250                    0.765533   \n",
       "4  0.076923            0.288114          0.003240                    0.813801   \n",
       "\n",
       "   CPW.CATHODE_LENGTH  CPW.DIAMTER_MAX  CPW.DIAMTER_MIN  CPW.DIAMTER_AVERAGE  \\\n",
       "0            0.006666         0.977767         0.981295             0.984448   \n",
       "1            0.006653         0.975922         0.989752             0.990589   \n",
       "2            0.006668         0.978641         0.981992             0.986231   \n",
       "3            0.006681         0.973107         0.982390             0.984051   \n",
       "4            0.006843         0.000000         0.000000             0.000000   \n",
       "\n",
       "   CPW.ANODETABPOSITION  CPW.CATHODETABPOSITION  ...  CPX.AI_ACOHs055  \\\n",
       "0              0.209158                0.002674  ...              NaN   \n",
       "1              0.112118                0.916349  ...              NaN   \n",
       "2              0.274556                0.121213  ...              NaN   \n",
       "3              0.839987                0.611703  ...              NaN   \n",
       "4              0.447412                0.243256  ...              NaN   \n",
       "\n",
       "   CPX.AI_ACOHs056  CPX.CONV_ACOHs045_1  CPX.CONV_ACOHs046_1  \\\n",
       "0              NaN                  NaN                  NaN   \n",
       "1              NaN                  NaN                  NaN   \n",
       "2              NaN                  NaN                  NaN   \n",
       "3              NaN                  NaN                  NaN   \n",
       "4              NaN                  NaN                  NaN   \n",
       "\n",
       "   CPX.CONV_ACOHs047_1  CPX.CONV_ACOHs048_1  CPX.Retest_Bool  CPX.Retest_Num  \\\n",
       "0                  NaN                  NaN              0.0             0.0   \n",
       "1                  NaN                  NaN              0.0             0.0   \n",
       "2                  NaN                  NaN              0.0             0.0   \n",
       "3                  NaN                  NaN              0.0             0.0   \n",
       "4                  NaN                  NaN              0.0             0.0   \n",
       "\n",
       "   CPW.Duration  CPM.Duration  \n",
       "0      0.118489      0.007054  \n",
       "1      0.220644      0.004977  \n",
       "2      0.100530      0.006116  \n",
       "3      0.037775      0.006997  \n",
       "4      0.143117      0.006969  \n",
       "\n",
       "[5 rows x 303 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove N/A values --> Categorical data with modus / Numerical Data with mean (Categorical values are all stored in the encoding dict; manual encoding was all on numerical values)\n",
    "\n",
    "--> After checking the values, there is no missing value for the categorical data, so only the missing numerical values will be calculated (can be checked with code below but takes 30min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col_idx, column in enumerate(df_scaled.columns):\n",
    "#     for row_idx, cell in enumerate(df_scaled[column]):\n",
    "#         if pd.isna(df_scaled.loc[row_idx, column]):\n",
    "#             if column in encoding_dict.keys():\n",
    "#                 print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df_scaled.copy()\n",
    "\n",
    "for column in final_df.columns:\n",
    "    mean_value = final_df[column].mean()\n",
    "    final_df[column] = final_df[column].fillna(mean_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that no N/A is remaining\n",
    "for val in final_df.isna().sum():\n",
    "    if val != 0:\n",
    "        print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CPW.Name</th>\n",
       "      <th>CPW.ANODE_FZLENGTH</th>\n",
       "      <th>CPW.ANODE_LENGTH</th>\n",
       "      <th>CPW.CATHODE_FZLENGTH_UPPER</th>\n",
       "      <th>CPW.CATHODE_LENGTH</th>\n",
       "      <th>CPW.DIAMTER_MAX</th>\n",
       "      <th>CPW.DIAMTER_MIN</th>\n",
       "      <th>CPW.DIAMTER_AVERAGE</th>\n",
       "      <th>CPW.ANODETABPOSITION</th>\n",
       "      <th>CPW.CATHODETABPOSITION</th>\n",
       "      <th>...</th>\n",
       "      <th>CPX.AI_ACOHs055</th>\n",
       "      <th>CPX.AI_ACOHs056</th>\n",
       "      <th>CPX.CONV_ACOHs045_1</th>\n",
       "      <th>CPX.CONV_ACOHs046_1</th>\n",
       "      <th>CPX.CONV_ACOHs047_1</th>\n",
       "      <th>CPX.CONV_ACOHs048_1</th>\n",
       "      <th>CPX.Retest_Bool</th>\n",
       "      <th>CPX.Retest_Num</th>\n",
       "      <th>CPW.Duration</th>\n",
       "      <th>CPM.Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.301151</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.686540</td>\n",
       "      <td>0.006666</td>\n",
       "      <td>0.977767</td>\n",
       "      <td>0.981295</td>\n",
       "      <td>0.984448</td>\n",
       "      <td>0.209158</td>\n",
       "      <td>0.002674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346099</td>\n",
       "      <td>0.323453</td>\n",
       "      <td>0.407145</td>\n",
       "      <td>0.53664</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.118489</td>\n",
       "      <td>0.007054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.285979</td>\n",
       "      <td>0.003219</td>\n",
       "      <td>0.813174</td>\n",
       "      <td>0.006653</td>\n",
       "      <td>0.975922</td>\n",
       "      <td>0.989752</td>\n",
       "      <td>0.990589</td>\n",
       "      <td>0.112118</td>\n",
       "      <td>0.916349</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346099</td>\n",
       "      <td>0.323453</td>\n",
       "      <td>0.407145</td>\n",
       "      <td>0.53664</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.220644</td>\n",
       "      <td>0.004977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.289899</td>\n",
       "      <td>0.003248</td>\n",
       "      <td>0.799034</td>\n",
       "      <td>0.006668</td>\n",
       "      <td>0.978641</td>\n",
       "      <td>0.981992</td>\n",
       "      <td>0.986231</td>\n",
       "      <td>0.274556</td>\n",
       "      <td>0.121213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346099</td>\n",
       "      <td>0.323453</td>\n",
       "      <td>0.407145</td>\n",
       "      <td>0.53664</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100530</td>\n",
       "      <td>0.006116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.306761</td>\n",
       "      <td>0.003250</td>\n",
       "      <td>0.765533</td>\n",
       "      <td>0.006681</td>\n",
       "      <td>0.973107</td>\n",
       "      <td>0.982390</td>\n",
       "      <td>0.984051</td>\n",
       "      <td>0.839987</td>\n",
       "      <td>0.611703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346099</td>\n",
       "      <td>0.323453</td>\n",
       "      <td>0.407145</td>\n",
       "      <td>0.53664</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037775</td>\n",
       "      <td>0.006997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.288114</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>0.813801</td>\n",
       "      <td>0.006843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447412</td>\n",
       "      <td>0.243256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346099</td>\n",
       "      <td>0.323453</td>\n",
       "      <td>0.407145</td>\n",
       "      <td>0.53664</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.143117</td>\n",
       "      <td>0.006969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CPW.Name  CPW.ANODE_FZLENGTH  CPW.ANODE_LENGTH  CPW.CATHODE_FZLENGTH_UPPER  \\\n",
       "0  0.192308            0.301151          0.003236                    0.686540   \n",
       "1  0.076923            0.285979          0.003219                    0.813174   \n",
       "2  0.615385            0.289899          0.003248                    0.799034   \n",
       "3  0.230769            0.306761          0.003250                    0.765533   \n",
       "4  0.076923            0.288114          0.003240                    0.813801   \n",
       "\n",
       "   CPW.CATHODE_LENGTH  CPW.DIAMTER_MAX  CPW.DIAMTER_MIN  CPW.DIAMTER_AVERAGE  \\\n",
       "0            0.006666         0.977767         0.981295             0.984448   \n",
       "1            0.006653         0.975922         0.989752             0.990589   \n",
       "2            0.006668         0.978641         0.981992             0.986231   \n",
       "3            0.006681         0.973107         0.982390             0.984051   \n",
       "4            0.006843         0.000000         0.000000             0.000000   \n",
       "\n",
       "   CPW.ANODETABPOSITION  CPW.CATHODETABPOSITION  ...  CPX.AI_ACOHs055  \\\n",
       "0              0.209158                0.002674  ...         0.346099   \n",
       "1              0.112118                0.916349  ...         0.346099   \n",
       "2              0.274556                0.121213  ...         0.346099   \n",
       "3              0.839987                0.611703  ...         0.346099   \n",
       "4              0.447412                0.243256  ...         0.346099   \n",
       "\n",
       "   CPX.AI_ACOHs056  CPX.CONV_ACOHs045_1  CPX.CONV_ACOHs046_1  \\\n",
       "0         0.323453             0.407145              0.53664   \n",
       "1         0.323453             0.407145              0.53664   \n",
       "2         0.323453             0.407145              0.53664   \n",
       "3         0.323453             0.407145              0.53664   \n",
       "4         0.323453             0.407145              0.53664   \n",
       "\n",
       "   CPX.CONV_ACOHs047_1  CPX.CONV_ACOHs048_1  CPX.Retest_Bool  CPX.Retest_Num  \\\n",
       "0                  0.5                  0.0              0.0             0.0   \n",
       "1                  0.5                  0.0              0.0             0.0   \n",
       "2                  0.5                  0.0              0.0             0.0   \n",
       "3                  0.5                  0.0              0.0             0.0   \n",
       "4                  0.5                  0.0              0.0             0.0   \n",
       "\n",
       "   CPW.Duration  CPM.Duration  \n",
       "0      0.118489      0.007054  \n",
       "1      0.220644      0.004977  \n",
       "2      0.100530      0.006116  \n",
       "3      0.037775      0.006997  \n",
       "4      0.143117      0.006969  \n",
       "\n",
       "[5 rows x 303 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"final_df.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(final_df, f)\n",
    "\n",
    "# with open(\"initial_df.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(df, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------- LABELS ----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CPPG.STATUS_SPS</th>\n",
       "      <th>CPPG.STATUS_U0</th>\n",
       "      <th>CPPG.STATUS_HEIGHT</th>\n",
       "      <th>CPPG.STATUS_IMPEDANCE</th>\n",
       "      <th>CPPG.STATUS_FORMATION</th>\n",
       "      <th>CPPG.STATUS_MODUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>PASS</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CPPG.STATUS_SPS CPPG.STATUS_U0 CPPG.STATUS_HEIGHT CPPG.STATUS_IMPEDANCE  \\\n",
       "0            PASS           PASS               PASS                  PASS   \n",
       "1            PASS           PASS               PASS                  PASS   \n",
       "2            PASS           PASS               PASS                  PASS   \n",
       "3            PASS           PASS               PASS                  PASS   \n",
       "4            PASS           PASS               PASS                  PASS   \n",
       "\n",
       "  CPPG.STATUS_FORMATION CPPG.STATUS_MODUL  \n",
       "0                  PASS               NaN  \n",
       "1                   NaN              PASS  \n",
       "2                   NaN              PASS  \n",
       "3                  PASS               NaN  \n",
       "4                  PASS               NaN  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cppg_df = df[LABEL_COLS]\n",
    "cppg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPPG.STATUS_SPS\n",
      "PASS              973404\n",
      "IMPEDANCE_FAIL     12876\n",
      "VOLTAGE_FAIL        8392\n",
      "U0_FAIL             2560\n",
      "FORMATION_FAIL      1507\n",
      "HEIGHT_FAIL         1201\n",
      "NO CELL               51\n",
      "_FAIL                  9\n",
      "Name: count, dtype: int64\n",
      "CPPG.STATUS_U0\n",
      "PASS       987590\n",
      "FAIL        12359\n",
      "NO CELL        51\n",
      "Name: count, dtype: int64\n",
      "CPPG.STATUS_HEIGHT\n",
      "PASS       998619\n",
      "FAIL         1330\n",
      "NO CELL        51\n",
      "Name: count, dtype: int64\n",
      "CPPG.STATUS_IMPEDANCE\n",
      "PASS       985586\n",
      "FAIL        14363\n",
      "NO CELL        51\n",
      "Name: count, dtype: int64\n",
      "CPPG.STATUS_FORMATION\n",
      "PASS       400555\n",
      "FAIL         1507\n",
      "NO CELL        51\n",
      "Name: count, dtype: int64\n",
      "CPPG.STATUS_MODUL\n",
      "PASS    597887\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for column in cppg_df.columns:\n",
    "    print(cppg_df[column].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAHyCAYAAAA5oM6mAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA+dElEQVR4nO3de3zP9f//8ft7ZzObY2PMzDEMOaRGSwiNHyk+qJBjqZUPK5V0SUkpIR8VKkNyCCUlKssxh8p8nCmEHLY1ExsKOzx/f/js/e1tG5vkadvterm8Lhev5+v5er8er/c2u+/5er5eb4cxxggAAMASN9sFAACAoo0wAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMIIbyvbt29W3b1+FhobKx8dHfn5+atSokcaOHavff//d2e+uu+6Sw+FwLsWKFVODBg00ceJEZWZmOvv16dPHpZ+3t7dq1aqlkSNH6ty5c9mOv27dOj3wwAOqXLmyvL29Vbx4cdWtW1dPPfWUfvrppzydww8//KD77rvP+RqBgYEKDw/XU089JUmaOXOmS025LVWqVHF53UaNGsnhcGjcuHHOttWrV+fptRwOhyTppZdeksPhUHJyco61h4WF6a677nJpO3LkiB5//HHVrFlTxYoVU+nSpVWvXj0NHDhQR44cydN78tdaP/nkkxy3P/HEE846/+rbb79VeHi4fH19VbZsWfXp00dJSUmXPdbnn38uh8OhqVOn5tonNjZWDodDEyZMyPM59OnTJ9vX5Xo6f/683nnnHd1xxx0qVaqUvLy8VLFiRXXr1k1r1qyxVtdfxcfH66WXXtLWrVttl4ICxMN2AUCWDz74QI8//rhq1aqlYcOGqU6dOkpLS1NcXJymTp2qjRs36rPPPnP2r1q1qubMmSNJSkpK0tSpUzV06FAlJCTojTfecPYrVqyYVq5cKUk6efKk5s2bp1GjRumnn37S/Pnznf1eeOEFvfrqqwoPD9cLL7ygGjVqKD09Xdu3b9eHH36oCRMmKD09Xe7u7rmew9KlS9WpUyfdddddGjt2rCpUqKCEhATFxcXp448/1vjx49WhQwdt3LjRZb/w8HB17drVGVgkydvb2/nvrVu3asuWLZKkmJgYPf3005IuBpRLX+u+++5TtWrVXELL1Tp69KgaNWqkkiVL6qmnnlKtWrWUkpKi3bt3a8GCBTpw4ICCg4P/9nFys2bNGkVGRqpDhw76/PPPlZSUpGeffVatW7dWXFycy3v0Vx06dFD58uU1ffp0DRo0KMc+M2bMkKenp3r16vWP1X8tJScn65577tH27dvVr18/DRs2TKVLl9axY8f0+eefq3Xr1tq8ebMaNGhgtc74+Hi9/PLLqlKlim655RartaAAMcANYMOGDcbd3d3cc8895ty5c9m2nz9/3nz++efO9RYtWpi6deu69Llw4YKpWrWq8fX1NRcuXDDGGPPwww+b4sWLZ3u9iIgII8kcPXrUGGPM3LlzjSQzaNAgk5mZma1/Zmameeedd0x6evplz+POO+801apVM2lpadm2ZWRk5LqfJBMVFZXr9qioKCPJdOjQwUgy69evz7VvSEiI6dChQ47bRo4caSSZ48eP57i9bt26pkWLFs71F1980UgyBw4cyLH/5c7pUqtWrTKSzMKFC3PcnnWOf3XrrbeaOnXquLyf69evN5LM5MmTL3u8Z555xkgyO3bsyLbt5MmTxsfHx3Tp0iXP9Rtz8fspJCQkX/tcK5GRkcbDw8OsWLEix+0//vij+fXXX69zVdlt2rTJSDIzZsywXQoKEC7T4Ibw2muvyeFw6P3338/xr10vLy916tTpsq/h6empxo0b648//tDx48cv2/f222+XJP3666+SpNGjR6ts2bJ66623crxU4HA4FBUVddlREUk6ceKEypYtKw+P7IOObm5X9+N27tw5zZ07V40bN9Zbb70lSZo+ffpVvVZ+nThxQm5ubrrpppty3H6155QXx44d06ZNm9SrVy+X97NZs2aqWbOmyyhZTvr37y/p4gjIpebNm6dz586pX79+kqR3331Xd955p2666SYVL15c9erV09ixY5WWlnbZYxw6dEgOh0MzZ87Mts3hcOill15yadu3b58efPBB3XTTTfL29lbt2rX17rvvXvYYkrR582Z99dVX6t+/v1q1apVjn1tvvVWVK1d2ru/cuVP33nuvSpUqJR8fH91yyy368MMPXfbJumR46NAhl/asS2qrV692tt11110KCwvTpk2bFBERIV9fX1WtWlWvv/6689Lo6tWrdeutt0qS+vbt67xEmPU+HDhwQD169FBQUJDzEmbr1q25pAPmjMC+jIwMrVy5Uo0bN/7bQ/6//PKLPDw8VKpUqcv2279/vySpXLlyio+P1+7du9WmTRv5+Pj8reOHh4frhx9+0ODBg/XDDz9c8ZdZXixatEgnT55Uv379VKNGDd1xxx2aP3++zpw587df+0rCw8OVmZmp+++/X998841SU1P/8WNm2blzpySpfv362bbVr1/fuT03NWvW1B133KHZs2dn+zrMmDFDFStWVLt27SRd/L558MEH9dFHH+nLL79U//799eabb+rRRx+9Rmcj7d69W7feeqt27typ8ePH68svv1SHDh00ePBgvfzyy5fdd/ny5ZKkzp075+lYP//8s5o1a6Zdu3Zp0qRJWrRokerUqaM+ffpo7NixV30OiYmJeuihh9SzZ0998cUXioyM1PDhwzV79mxJFy8bZoW/F154QRs3btTGjRs1YMAASVL79u21efNmjR07VrGxsZoyZYoaNmyoU6dOXXVNKCRsD80AiYmJRpLp0aNHnvfJukyTlpZm0tLSTHx8vHnuueeMJPOvf/3L2S/rMk1Wv+PHj5v//Oc/xuFwmFtvvdUYY8z3339vJJnnnnsu23HS09Od+6alpeV4CeevkpOTzR133GEkGUnG09PTNGvWzIwZM8acPn061/10mcs0rVq1Mj4+PubkyZPGGGNmzJhhJJmYmJgc+1/LyzSZmZnm0UcfNW5ubkaScTgcpnbt2mbo0KHm4MGDuZ5PTvJ7mWbOnDlGktm4cWO2vo888ojx8vK64jGz3qtFixY523bu3GkkmREjRuS4T0ZGhklLSzOzZs0y7u7u5vfff3duu/QyzcGDB3O9JCHJjBw50rnerl07U6lSJZOSkuLS74knnjA+Pj4ux7nUoEGDjCTz008/XeGML+rRo4fx9vY2hw8fdmmPjIw0vr6+5tSpU8aY/3t/Lv1aZn2tVq1a5Wxr0aKFkWR++OEHl7516tQx7dq1c67ndpkmOTnZSDITJ07M0zmgaClQIyNr165Vx44dFRQUJIfDocWLF+f7NYwxGjdunGrWrClvb28FBwfrtddeu/bF4h+3a9cueXp6ytPTU0FBQRo/frweeughffDBBy79zp496+xXrlw5DRkyRJGRkVcc5pekMmXKOPf19PTUp59+esX+3333nTZt2qTXX39d9957r/bu3avhw4erXr16ud7FkpuDBw9q1apVuv/++1WyZElJ0r/+9S+VKFHiulyqyboj5cCBA5o8ebL69u2rtLQ0vfXWW6pbt+51uYMjp8tml2v/q27dumV7r6ZPny6Hw6G+ffs627Zs2aJOnTqpTJkycnd3l6enp3r37q2MjAzt3bv3b5/DuXPntGLFCt13333y9fVVenq6c2nfvr3OnTun77///m8fJ8vKlSvVunXrbCONffr00R9//JFt0nNelS9fXk2bNnVpq1+/vvNy5+WULl1a1apV05tvvqkJEyZoy5YtLne+oWgrUGHk7NmzatCggd55552rfo1///vfmjZtmsaNG6effvpJS5YsyfbDheurbNmy8vX11cGDB/O1X7Vq1bRp0ybFxcVp586dOnXqlGbPnq2AgACXfsWKFdOmTZu0adMmbd++XadOndLSpUtVsWJFSXL+h53Tf6irV6/Wpk2bLnuLaE6aNGmiZ599VgsXLlR8fLyGDh2qQ4cO5XuIfPr06TLGqGvXrjp16pROnTqltLQ0derUSevXr8/z7cZZsuZeZGRk5Lg9PT1dnp6e2dpDQkL02GOPKSYmRvv27dP8+fN17tw5DRs27Joe+69zQ8qUKSPp4ryVS/3+++8qXbr0FY/p6+urHj166Ouvv1ZiYqLS09M1e/ZstWjRQtWqVZMkHT58WBERETp27Jj+85//OMNk1lyOP//8M8/nmJsTJ04oPT1db7/9tku49fT0VPv27SXpskE1ay5IXn9GTpw4oQoVKmRrDwoKcm6/Gllfk7/y9vbO03vkcDi0YsUKtWvXTmPHjlWjRo1Urlw5DR48WKdPn76qelB4FKhbeyMjIxUZGZnr9gsXLuiFF17QnDlzdOrUKYWFhemNN95wPjdhz549mjJlinbu3KlatWpdp6pxJe7u7mrdurW++uorHT16VJUqVcrTfj4+PmrSpMkV+7m5uV22X1BQkOrWravY2FidO3fOZd5I1q2Jf2d+hqenp0aOHKm33nrrivMc/iozM9M5MfL+++/Psc/06dPzFXACAwMlXZwcmvXvLMYYJSQk5Ok97datm8aMGZOv8/nrsXNyaU1hYWGSpB07djh/YWfZsWOHc/uV9O/fXx988IFmzZqlmjVrKikpSePHj3duX7x4sc6ePatFixYpJCTE2Z6XSZVZ3yvnz593ab/0l32pUqXk7u6uXr16KSoqKsfXCg0NzfU47dq10/PPP6/FixfrnnvuuWJdZcqUUUJCQrb2+Ph4SRf/ALhc/fkdwcurkJAQxcTESJL27t2rBQsW6KWXXtKFCxfyHfhRuBSokZEr6du3r9avX6+PP/5Y27dv17/+9S/dc8892rdvnyRpyZIlqlq1qr788kuFhoaqSpUqGjBggMvDtGDH8OHDZYzRwIEDdeHChWzb09LStGTJkn/s+CNGjFBycrKio6NljLnq18npF4B0MQhL//eXaV588803Onr0qKKiorRq1apsS926dTVr1iylp6fn+TVbtWolh8Ph8nyVLF9//bVSU1N19913X/F8zpw5oyNHjuTrfGrUqKGQkBAtXLgw23t8/PhxrVq1yuXYFStWVNOmTTV79myX0ZTvv/9eP//8c64B7VK33XabwsLCNGPGDM2YMUMBAQHq0qWLc3vW5Z6/3sVljMl2uS8ngYGB8vHx0fbt213aP//8c5d1X19ftWzZUlu2bFH9+vXVpEmTbEtOow5ZGjVqpMjISMXExDifmXOpuLg4HT58WJLUunVrrVy50hk+ssyaNUu+vr7Ou8myHuB2af1ffPHFFc89N1nv45VGS2rWrKkXXnhB9erV03//+9+rPh4KCYvzVf4WSeazzz5zru/fv984HA5z7Ngxl36tW7c2w4cPN8YY8+ijjxpvb29z2223mbVr15pVq1aZW265xbRs2fJ6lo5cvP/++8bDw8OEhYWZd99916xevdrExsaasWPHmurVq5vOnTs7++b0nJGc5PackZyMGDHCSDLNmjUz77//vlm1apVZsWKFmTlzpmndurWRZL7++mtn/5dfftm4u7ub1atXO9vq1atnIiMjzeTJk83KlSvNt99+a8aNG2cqVKhg/Pz8zPbt23M8tnKYwNqlSxfj4eGR7Xs6y6RJk4wks3jxYpf2y01gNcaYJ5980jgcDvPII4+YxYsXm2+++caMHj3a+Pn5mSZNmpjz5887+0ZFRZlbbrnFjBkzxnz11Vdm9erVZsaMGaZx48ZGkpk+fXrub2gOFi5caBwOh2nZsqWZO3euWblypXnvvfdMaGioKVWqlNm/f79L/1WrVhkPDw9z3333mdjYWDNnzhwTHBxswsLCcnweTW4mTJjgnIA7aNAgl2179uwxXl5e5q677jLLli0zixYtMm3atDE1atTINokzp+eMDBgwwPj4+Jjx48ebb7/91rz22msmLCws2wTWXbt2mVKlSpmmTZuaGTNmmFWrVpkvvvjCTJgwIU//Bx0/ftw0btzYeHl5mUGDBpnPP//crF271syfP9/07NnTuLu7m61btxpjjPnpp59MiRIlTM2aNc3s2bPNsmXLzEMPPWQkmbFjxzpfMz093dSqVctUrlzZzJ0713z11VfmkUceMaGhoTlOYM3pZ+7S9+Ts2bOmWLFipnnz5mbVqlVm06ZN5tixY2bbtm0mIiLCTJo0yXz11VdmxYoVZsSIEcbNzc08//zzVzx/FG6FJowsWLDASDLFixd3WTw8PEy3bt2MMcYMHDjQSDI///yzc7/Nmzfna5Y6/llbt241Dz/8sKlcubLx8vIyxYsXNw0bNjQvvviiSUpKcvb7J8KIMcasXbvWdO/e3VSqVMl4enoaX19fU6dOHfPYY4+ZuLg4l75Zd6b89T/s+fPnmwcffNDUqFHD+Pn5GU9PT1O5cmXTq1cvs3v37lyPe2kYOX78uPHy8nIJYJc6efKkKVasmOnYsaNL+5XCSGZmppkyZYpp0qSJ8fX1NV5eXqZGjRrm2WefzXbHz/fff2+ioqJMgwYNTOnSpY27u7spV66cueeee8yyZctyPcblfPvtt6Zt27amZMmSxsPDw1SoUMH07NnT7Nu3L8f+y5cvN7fffrvx8fExpUuXNr179za//fZbvo6Z9X5KMj/++GO27UuWLDENGjQwPj4+pmLFimbYsGHmq6++ylMYSUlJMQMGDDCBgYGmePHipmPHjubQoUPZwogxF+++6devn6lYsaLx9PQ05cqVM82aNTOjR4/O03n8+eefZtKkSSY8PNz4+/sbDw8PExQUZO6//36zdOlSl747duwwHTt2NAEBAcbLy8s0aNAgx7t+9u7da9q2bWv8/f1NuXLlzJNPPmmWLl161WHEGGPmzZtnbr75ZuPp6el8H3777TfTp08fc/PNN5vixYsbPz8/U79+ffPWW29d8WGCKPwcxvyNMWmLHA6HPvvsM+d99/Pnz9dDDz2kXbt2ZXswlZ+fn8qXL6+RI0fqtddec3nmwJ9//ilfX18tX75cbdq0uZ6nAAAAVMAmsF5Ow4YNlZGRoaSkJEVEROTYp3nz5kpPT9cvv/zinEmfddveXyeuAQCA66dAjYycOXPG+eTMhg0basKECWrZsqVKly6typUrq2fPnlq/fr3Gjx+vhg0bKjk5WStXrlS9evXUvn17ZWZm6tZbb5Wfn5/z012joqLk7+/vfMIhgPwxxuR6u24Wd3f3PD0XBEDRVKDupomLi1PDhg3VsGFDSVJ0dLQaNmyoF198UdLFRzz37t3b+eminTp10g8//OB8joSbm5uWLFmismXL6s4771SHDh1Uu3Ztffzxx9bOCSjo1qxZk+3ZGZcul34mCgD8VYEaGQFw4zl9+rR+/vnny/YJDQ297K2rAIo2wggAALCqQExgzczMVHx8vEqUKMF1ZwAACghjjE6fPq2goCC5ueU+M6RAhJH4+Pi//dHyAADAjiNHjlz2oz4KRBgpUaKEpIsn4+/vb7kaAACQF6mpqQoODnb+Hs9NgQgjWZdm/P39CSMAABQwV5piUaBu7QUAAIUPYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVuU7jKxdu1YdO3ZUUFCQHA6HFi9efMV91qxZo8aNG8vHx0dVq1bV1KlTr6ZWAABQCOU7jJw9e1YNGjTQO++8k6f+Bw8eVPv27RUREaEtW7bo+eef1+DBg/Xpp5/mu1gAAFD45PsJrJGRkYqMjMxz/6lTp6py5cqaOHGiJKl27dqKi4vTuHHj1KVLl/weHgAAFDL/+JyRjRs3qm3bti5t7dq1U1xcnNLS0nLc5/z580pNTXVZAABA4fSPh5HExEQFBga6tAUGBio9PV3Jyck57jNmzBgFBAQ4Fz6xFwCAwuu63E1z6QfkGGNybM8yfPhwpaSkOJcjR4784zUCAAA7/vFP7S1fvrwSExNd2pKSkuTh4aEyZcrkuI+3t7e8vb3/6dIAAMAN4B8fGQkPD1dsbKxL2/Lly9WkSRN5enr+04cHAAA3uHyPjJw5c0b79+93rh88eFBbt25V6dKlVblyZQ0fPlzHjh3TrFmzJEmDBg3SO++8o+joaA0cOFAbN25UTEyM5s2bd+3O4gZR5bmltksoNA693sF2CQCA6yTfYSQuLk4tW7Z0rkdHR0uSHn74Yc2cOVMJCQk6fPiwc3toaKiWLVumoUOH6t1331VQUJAmTZrEbb0AAECS5DBZs0lvYKmpqQoICFBKSor8/f1tl5MrRkauHUZGAKDgy+vvbz6bBgAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYNVVhZHJkycrNDRUPj4+aty4sb777rvL9p8zZ44aNGggX19fVahQQX379tWJEyeuqmAAAFC45DuMzJ8/X0OGDNGIESO0ZcsWRUREKDIyUocPH86x/7p169S7d2/1799fu3bt0sKFC7Vp0yYNGDDgbxcPAAAKvnyHkQkTJqh///4aMGCAateurYkTJyo4OFhTpkzJsf/333+vKlWqaPDgwQoNDdUdd9yhRx99VHFxcX+7eAAAUPDlK4xcuHBBmzdvVtu2bV3a27Ztqw0bNuS4T7NmzXT06FEtW7ZMxhj99ttv+uSTT9ShQ4dcj3P+/Hmlpqa6LAAAoHDKVxhJTk5WRkaGAgMDXdoDAwOVmJiY4z7NmjXTnDlz1L17d3l5eal8+fIqWbKk3n777VyPM2bMGAUEBDiX4ODg/JQJAAAKkKuawOpwOFzWjTHZ2rLs3r1bgwcP1osvvqjNmzfr66+/1sGDBzVo0KBcX3/48OFKSUlxLkeOHLmaMgEAQAHgkZ/OZcuWlbu7e7ZRkKSkpGyjJVnGjBmj5s2ba9iwYZKk+vXrq3jx4oqIiNDo0aNVoUKFbPt4e3vL29s7P6UBAIACKl8jI15eXmrcuLFiY2Nd2mNjY9WsWbMc9/njjz/k5uZ6GHd3d0kXR1QAAEDRlu/LNNHR0Zo2bZqmT5+uPXv2aOjQoTp8+LDzssvw4cPVu3dvZ/+OHTtq0aJFmjJlig4cOKD169dr8ODBatq0qYKCgq7dmQAAgAIpX5dpJKl79+46ceKERo0apYSEBIWFhWnZsmUKCQmRJCUkJLg8c6RPnz46ffq03nnnHT311FMqWbKkWrVqpTfeeOPanQUAACiwHKYAXCtJTU1VQECAUlJS5O/vb7ucXFV5bqntEgqNQ6/nfus3AKBgyOvvbz6bBgAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWHVVYWTy5MkKDQ2Vj4+PGjdurO++++6y/c+fP68RI0YoJCRE3t7eqlatmqZPn35VBQMAgMLFI787zJ8/X0OGDNHkyZPVvHlzvffee4qMjNTu3btVuXLlHPfp1q2bfvvtN8XExKh69epKSkpSenr63y4eAAAUfA5jjMnPDrfddpsaNWqkKVOmONtq166tzp07a8yYMdn6f/311+rRo4cOHDig0qVL5+kY58+f1/nz553rqampCg4OVkpKivz9/fNT7nVV5bmltksoNA693sF2CQCAvyk1NVUBAQFX/P2dr8s0Fy5c0ObNm9W2bVuX9rZt22rDhg057vPFF1+oSZMmGjt2rCpWrKiaNWvq6aef1p9//pnrccaMGaOAgADnEhwcnJ8yAQBAAZKvyzTJycnKyMhQYGCgS3tgYKASExNz3OfAgQNat26dfHx89Nlnnyk5OVmPP/64fv/991znjQwfPlzR0dHO9ayREQAAUPjke86IJDkcDpd1Y0y2tiyZmZlyOByaM2eOAgICJEkTJkxQ165d9e6776pYsWLZ9vH29pa3t/fVlAYAAAqYfF2mKVu2rNzd3bONgiQlJWUbLclSoUIFVaxY0RlEpItzTIwxOnr06FWUDAAACpN8hREvLy81btxYsbGxLu2xsbFq1qxZjvs0b95c8fHxOnPmjLNt7969cnNzU6VKla6iZAAAUJjk+zkj0dHRmjZtmqZPn649e/Zo6NChOnz4sAYNGiTp4nyP3r17O/s/+OCDKlOmjPr27avdu3dr7dq1GjZsmPr165fjJRoAAFC05HvOSPfu3XXixAmNGjVKCQkJCgsL07JlyxQSEiJJSkhI0OHDh539/fz8FBsbqyeffFJNmjRRmTJl1K1bN40ePfranQUAACiw8v2cERvyep+ybTxn5NrhOSMAUPD9I88ZAQAAuNYIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKy6qjAyefJkhYaGysfHR40bN9Z3332Xp/3Wr18vDw8P3XLLLVdzWAAAUAjlO4zMnz9fQ4YM0YgRI7RlyxZFREQoMjJShw8fvux+KSkp6t27t1q3bn3VxQIAgMIn32FkwoQJ6t+/vwYMGKDatWtr4sSJCg4O1pQpUy6736OPPqoHH3xQ4eHhV10sAAAofPIVRi5cuKDNmzerbdu2Lu1t27bVhg0bct1vxowZ+uWXXzRy5Mg8Hef8+fNKTU11WQAAQOGUrzCSnJysjIwMBQYGurQHBgYqMTExx3327dun5557TnPmzJGHh0eejjNmzBgFBAQ4l+Dg4PyUCQAACpCrmsDqcDhc1o0x2dokKSMjQw8++KBefvll1axZM8+vP3z4cKWkpDiXI0eOXE2ZAACgAMjbUMX/lC1bVu7u7tlGQZKSkrKNlkjS6dOnFRcXpy1btuiJJ56QJGVmZsoYIw8PDy1fvlytWrXKtp+3t7e8vb3zUxoAACig8jUy4uXlpcaNGys2NtalPTY2Vs2aNcvW39/fXzt27NDWrVudy6BBg1SrVi1t3bpVt91229+rHgAAFHj5GhmRpOjoaPXq1UtNmjRReHi43n//fR0+fFiDBg2SdPESy7FjxzRr1iy5ubkpLCzMZf+bbrpJPj4+2doBAEDRlO8w0r17d504cUKjRo1SQkKCwsLCtGzZMoWEhEiSEhISrvjMEQAAgCwOY4yxXcSVpKamKiAgQCkpKfL397ddTq6qPLfUdgmFxqHXO9guAQDwN+X19zefTQMAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALDqqsLI5MmTFRoaKh8fHzVu3Fjfffddrn0XLVqkNm3aqFy5cvL391d4eLi++eabqy4YAAAULvkOI/Pnz9eQIUM0YsQIbdmyRREREYqMjNThw4dz7L927Vq1adNGy5Yt0+bNm9WyZUt17NhRW7Zs+dvFAwCAgs9hjDH52eG2225To0aNNGXKFGdb7dq11blzZ40ZMyZPr1G3bl11795dL774Yp76p6amKiAgQCkpKfL3989PuddVleeW2i6h0Dj0egfbJQAA/qa8/v7O18jIhQsXtHnzZrVt29alvW3bttqwYUOeXiMzM1OnT59W6dKlc+1z/vx5paamuiwAAKBwylcYSU5OVkZGhgIDA13aAwMDlZiYmKfXGD9+vM6ePatu3brl2mfMmDEKCAhwLsHBwfkpEwAAFCBXNYHV4XC4rBtjsrXlZN68eXrppZc0f/583XTTTbn2Gz58uFJSUpzLkSNHrqZMAABQAHjkp3PZsmXl7u6ebRQkKSkp22jJpebPn6/+/ftr4cKFuvvuuy/b19vbW97e3vkpDQAAFFD5Ghnx8vJS48aNFRsb69IeGxurZs2a5brfvHnz1KdPH82dO1cdOjAxEQAA/J98jYxIUnR0tHr16qUmTZooPDxc77//vg4fPqxBgwZJuniJ5dixY5o1a5aki0Gkd+/e+s9//qPbb7/dOapSrFgxBQQEXMNTAQAABVG+w0j37t114sQJjRo1SgkJCQoLC9OyZcsUEhIiSUpISHB55sh7772n9PR0RUVFKSoqytn+8MMPa+bMmX//DAAAQIGW7+eM2MBzRooenjMCAAXfP/KcEQAAgGuNMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqj6vZafLkyXrzzTeVkJCgunXrauLEiYqIiMi1/5o1axQdHa1du3YpKChIzzzzjAYNGnTVRQPIuyrPLbVdQqFw6PUOtksACq18j4zMnz9fQ4YM0YgRI7RlyxZFREQoMjJShw8fzrH/wYMH1b59e0VERGjLli16/vnnNXjwYH366ad/u3gAAFDw5TuMTJgwQf3799eAAQNUu3ZtTZw4UcHBwZoyZUqO/adOnarKlStr4sSJql27tgYMGKB+/fpp3Lhxf7t4AABQ8OXrMs2FCxe0efNmPffccy7tbdu21YYNG3LcZ+PGjWrbtq1LW7t27RQTE6O0tDR5enpm2+f8+fM6f/68cz0lJUWSlJqamp9yr7vM83/YLqHQuNG/1gUJ35fXBt+TQP5l/dwYYy7bL19hJDk5WRkZGQoMDHRpDwwMVGJiYo77JCYm5tg/PT1dycnJqlChQrZ9xowZo5dffjlbe3BwcH7KRQEWMNF2BYArvieBq3f69GkFBATkuv2qJrA6HA6XdWNMtrYr9c+pPcvw4cMVHR3tXM/MzNTvv/+uMmXKXPY4uLLU1FQFBwfryJEj8vf3t10OwPckbjh8T147xhidPn1aQUFBl+2XrzBStmxZubu7ZxsFSUpKyjb6kaV8+fI59vfw8FCZMmVy3Mfb21ve3t4ubSVLlsxPqbgCf39/fshwQ+F7EjcavievjcuNiGTJ1wRWLy8vNW7cWLGxsS7tsbGxatasWY77hIeHZ+u/fPlyNWnSJMf5IgAAoGjJ99000dHRmjZtmqZPn649e/Zo6NChOnz4sPO5IcOHD1fv3r2d/QcNGqRff/1V0dHR2rNnj6ZPn66YmBg9/fTT1+4sAABAgZXvOSPdu3fXiRMnNGrUKCUkJCgsLEzLli1TSEiIJCkhIcHlmSOhoaFatmyZhg4dqnfffVdBQUGaNGmSunTpcu3OAnnm7e2tkSNHZrsMBtjC9yRuNHxPXn8Oc6X7bQAAAP5BfDYNAACwijACAACsIowAAACrCCMAAMAqwggAADn47bffNGrUKNtlFAncTQMAQA62bdumRo0aKSMjw3YphR4jIwAAwCrCSCG2f/9+bd682aVtxYoVatmypZo2barXXnvNUmVAzvbs2aOqVavaLgPAdUYYKcSGDRumxYsXO9cPHjyojh07ysvLS+Hh4RozZowmTpxorT7gUhcuXNCvv/5quwwA11m+HwePgiMuLk7PPPOMc33OnDmqWbOmvvnmG0lS/fr19fbbb2vIkCGWKgQAe6Kjoy+7/fjx49epEhBGCrHk5GRVqlTJub5q1Sp17NjRuX7XXXfpqaeeslEaAFi3ZcuWK/a58847r0MlIIwUYqVLl1ZCQoKCg4OVmZmpuLg4DR061Ln9woUL4mYqAEXVqlWrbJeA/yGMFGItWrTQK6+8osmTJ2vhwoXKzMxUy5Ytndt3796tKlWq2CsQRU6pUqXkcDhy3Z6enn4dqwEu75dfftHAgQO1cuVK26UUeoSRQuzVV19VmzZtVKVKFbm5uWnSpEkqXry4c/tHH32kVq1aWawQRQ0TplGQnDlzRmvWrLFdRpHAQ88KubS0NO3evVvlypVTUFCQy7Zt27apUqVKKlOmjKXqAODGxUPPrh9GRgo5T09PNWjQwKUtPT1d586dy9YOAIANPGekEFu2bJk++ugjl7ZXX31Vfn5+KlmypNq2bauTJ09aqg5FUalSpVS6dOkrLgCKFkZGCrFx48apS5cuzvUNGzboxRdf1KhRo1S7dm2NGDFCr7zyiiZMmGCxShQlzBnBjaRhw4aXnVD9xx9/XMdqijbCSCG2c+dOjR8/3rn+ySefqE2bNhoxYoQkycfHR//+978JI7huHn744Sv24Y4aXC+dO3e2XQL+hzBSiJ0+fdplcuq6devUtWtX53rdunUVHx9vozQgm927dysmJkazZ8/Wb7/9ZrscFAEjR460XQL+hzkjhVhQUJD27Nkj6eItatu2bVPz5s2d20+cOCFfX19b5QE6c+aMpk2bpvDwcNWvX18//PCDnnvuOdtloYhISkq67Pb09HT9+OOP16maoo2RkUKsa9euGjJkiJ5//nktW7ZM5cuX1+233+7cHhcXp1q1almsEEXVunXrNG3aNH366acKDQ3V7t27tWbNGpewDPzTKlSooISEBN10002SpNq1a+ubb75R5cqVJV38gy08PJxbe68DRkYKsZEjR6pJkyYaPHiwtm7dqtmzZ8vd3d25fd68eS6fVQP808aOHaubb75ZPXr0ULly5bRu3Tpt375dDodDpUqVsl0eiphLH7N19OjRbHOWeBTX9cHISCHm6+ub7dbev+JzGXC9Pf/883r22Wc1atQol2AM3Kgud7cNrh1GRoqokydP6u2339Ytt9xiuxQUIaNGjdLChQsVGhqqZ599Vjt37rRdEoAbAGGkiPn222/1wAMPKCgoSGPHjlWLFi1sl4Qi5Pnnn9fevXv10UcfKTExUbfffrsaNGggYwwP4MN153A4dPr0aaWmpiolJUUOh0NnzpxRamqqc8H1wWfTFAGHDx/WjBkzNGPGDJ05c0YnT57UggULXB6IBlwPBw4cUGhoqHPo+/Tp05ozZ45mzJihzZs3q2nTpuratauio6MtV4qiwM3NzeUyjDEmx3UmsP7zCCOF2IIFCzRt2jStX79e7du3V8+ePRUZGanixYtr27ZtqlOnju0SUcS4u7u73L3QvXt3TZo0SYGBgdqxY4diYmI0d+7cK95yCVwLef1EXkaQ/3mEkULMw8NDzzzzjIYPH64SJUo42z09PQkjsMLNzU2JiYnOMFKiRAlt27ZNVatWdfZJS0uTp6enrRIBWMCckUKsX79+mjx5su655x5NnTqVa/IoEAgiuF7i4+P19NNP5zg3JCUlRcOGDeNpwNcJYaQQe//995WQkKBHHnlE8+bNU4UKFXTvvffKGKPMzEzb5aEIcjgc2W6V5NZJ2DJhwgSlpqbK398/27aAgACdPn2az+66TrhMU4Ts379f06ZN00cffaQzZ86oQ4cO6tq1q+6//37bpaGIcHNzU2RkpLy9vSVJS5YsUatWrVS8eHGXfosWLbJRHoqYsLAwTZ06VXfccUeO2zds2KCBAwdq165d17myoocwUoj98ccfGjZsmBYvXqy0tDTdfffdmjRpkkqXLq2lS5cqJiZGX331lc6fP2+7VBQRffv2zVO/GTNm/MOVAFLx4sW1Z88e5+PfL3X48GHVrl1bZ8+evc6VFT2EkUJs2LBhmjx5sh566CH5+Pho3rx5uuuuu7Rw4UJnn6SkJOdkQgAoSsqWLatFixbpzjvvzHH72rVrdf/99ys5Ofk6V1b0EEYKsWrVqunVV19Vjx49JEk//vijmjdvrnPnzvEobgBFXocOHRQUFKQPPvggx+0DBgxQfHy8li1bdp0rK3r4bJpC7MiRI4qIiHCuN23aVB4eHoqPj1dwcLDFygDAvqefflpt2rRRQECAhg0bpsDAQEnSb7/9prFjx2rmzJlavny55SqLBkZGCjF3d3clJiaqXLlyzrYSJUpo+/btCg0NtVgZANwY3nvvPf373/9WWlqa/P395XA4lJKSIk9PT7311lt67LHHbJdYJBBGCrFL71yQcr57gTsXABRlx44d04IFC7R//34ZY1SzZk117dpVlSpVsl1akUEYKcS4cwEAUBAQRgAAgFU8gRUAAFhFGAEAAFYRRgAAgFU8ZwQAUKT9+eefio2N1d69e+VwOFSjRg21adNGxYoVs11akUEYAQAUWV988YUGDBiQ7ZHvZcuWVUxMjDp27GipsqKFyzQAgCJpw4YN6tq1q+68806tX79ev//+u37//XetW7dOERER6tq1qzZu3Gi7zCKBW3sBAEVS+/btFRwcrPfeey/H7Y8++qiOHDnCZ9NcB4QRAECRVKpUKa1du1b16tXLcfv27dvVokULnTx58jpXVvRwmQYAUCSdO3dO/v7+uW4PCAjQ+fPnr2NFRRdhBABQJNWsWVMrV67MdfuKFStUvXr161hR0UUYAQAUSX369NHTTz+d45yQpUuX6plnnsnzZ3zh72HOCACgSMrMzFT37t316aefqlatWqpdu7Ykaffu3dq3b586d+6shQsXys2Nv9v/aYQRAECRNn/+fM2bN0979+6VdPHyTY8ePdSjRw/LlRUdhBEAAGAVY08AAMAqHgcPACiS3Nzc5HA4LtvH4XAoPT39OlVUdBFGAABF0meffZbrtg0bNujtt98WMxmuD+aMAADwPz/99JOGDx+uJUuW6KGHHtIrr7yiypUr2y6r0GPOCACgyIuPj9fAgQNVv359paena+vWrfrwww8JItcJYQQAUGSlpKTo2WefVfXq1bVr1y6tWLFCS5YsUVhYmO3SihTmjAAAiqSxY8fqjTfeUPny5TVv3jzde++9tksqspgzAgAoktzc3FSsWDHdfffdcnd3z7XfokWLrmNVRRMjIwCAIql3795XvLUX1wcjIwAAwComsAIAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIcANLTEzUk08+qapVq8rb21vBwcHq2LGjVqxYIUmqUqWKHA6HHA6HfH19FRYWpvfee8+5/8yZM53bHQ6HKlSooG7duungwYMux9myZYu6d++uChUqyNvbWyEhIfp//+//acmSJZf91NIDBw7ogQceUFBQkHx8fFSpUiXde++92rt3b7Zj57SsXr1aknT06FF5eXnp5ptvdr72Sy+9dMX9Dx06pD59+qhz587Zatu6dauzT5b33ntPDRo0UPHixVWyZEk1bNhQb7zxRp6+Fvk5zo4dO9SiRQsVK1ZMFStW1KhRo/j0V+AyCCPADerQoUNq3LixVq5cqbFjx2rHjh36+uuv1bJlS0VFRTn7jRo1SgkJCdq+fbs6d+6sQYMGaf78+c7t/v7+SkhIUHx8vObOnautW7eqU6dOysjIkCR9/vnnuv3223XmzBl9+OGH2r17txYuXKjOnTvrhRdeUEpKSo71XbhwQW3atFFqaqoWLVqkn3/+WfPnz1dYWJhSUlLUvXt3JSQkOJfw8HANHDjQpa1Zs2aSLoambt266Y8//tD69eslSU8//bRL30qVKjnPNWsJDg7O8/sZExOj6OhoDR48WNu2bdP69ev1zDPP6MyZM/n+2lxOamqq2rRpo6CgIG3atElvv/22xo0bpwkTJlzT4wCFCU9gBW5Qjz/+uBwOh3788UcVL17c2V63bl3169fPuV6iRAmVL19ekjR69GgtWLBAixcvVvfu3SVJDofDub1ChQoaOXKkevbsqf3796tSpUrq37+/OnTo4PLI62rVqqlp06YaMGBArn/R7969WwcOHNDKlSsVEhIiSQoJCVHz5s2dfYoVK+b8t5eXl3x9fZ21ZDHGaMaMGZo8ebIqVaqkmJgYNW/eXH5+fvLz83P2c3d3dznX/FqyZIm6deum/v37O9vq1q17Va91OXPmzNG5c+c0c+ZMeXt7KywsTHv37tWECRMUHR3NEz+BHDAyAtyAfv/9d3399deKiopyCSJZSpYsmeu+Pj4+SktLy3V7VkBIS0vT8uXLdeLECT3zzDO59s/tl2e5cuXk5uamTz75xDnKcjVWrVqlP/74Q3fffbd69eqlBQsW6PTp01f9erkpX768vv/+e/3666/X/LX/auPGjWrRooW8vb2dbe3atVN8fLzLpRwA/4cwAtyA9u/fL2OMyxyKK0lPT9fMmTO1Y8cOtW7dOsc+R48e1ZtvvqlKlSqpZs2a2rt3rySpVq1azj6bNm1yjkr4+fnpyy+/zPG1KlasqEmTJunFF19UqVKl1KpVK73yyis6cOBAPs704uWTHj16yN3dXXXr1lX16tVdLjNdKyNHjlTJkiVVpUoV1apVS3369NGCBQuUmZl5TY+TmJiowMBAl7as9cTExGt6LKCwIIwAN6CsSyN5GdJ/9tln5efnp2LFiikqKkrDhg3To48+6tyekpIiPz8/FS9eXMHBwbpw4YIWLVokLy+vHF+vfv362rp1q7Zu3aqzZ88qPT0912NHRUUpMTFRs2fPVnh4uBYuXKi6desqNjY2T+d56tQpLVq0SD179nS29ezZU9OnT8/T/vlRoUIFbdy4UTt27NDgwYOVlpamhx9+WPfcc881DySXft3y8/UEiiLmjAA3oBo1asjhcGjPnj053sHxV8OGDVOfPn3k6+urChUqZPuFV6JECf33v/+Vm5ubAgMDXS771KhRQ5L0888/6/bbb5ckeXt7q3r16nmutUSJEurUqZM6deqk0aNHq127dho9erTatGlzxX3nzp2rc+fO6bbbbnO2GWOUmZmp3bt3q06dOld8DX9//xwvvZw6dUqSFBAQ4NIeFhamsLAwRUVFad26dYqIiNCaNWvUsmXLa3Kc8uXLZxsBSUpKkqRsIyYALmJkBLgBlS5dWu3atdO7776rs2fPZtue9QtQksqWLavq1asrKCgox7+83dzcVL16dVWtWjXb/JO2bduqdOnSeb699UocDoduvvnmHGvOSUxMjJ566innSMzWrVu1bds2tWzZMs+jIzfffLN27typc+fOubRv2rRJ5cqVU6lSpXLdNyvs5KXevB4nPDxca9eu1YULF5x9li9frqCgIFWpUiVP5wQUNYQR4AY1efJkZWRkqGnTpvr000+1b98+7dmzR5MmTVJ4ePg1OYafn5+mTZumpUuXqkOHDvrmm2904MABbd++XWPHjpV08S6WLDfffLM+++wzSRefr3Hvvffqk08+0e7du7V//37FxMRo+vTpuvfee6947K1bt+q///2vBgwY4BytyFoeeOABzZo167ITcbM89NBD8vDwUK9evRQXF6dffvlFs2fP1pgxYzRs2DBnv8cee0yvvPKK1q9fr19//VXff/+9evfurXLlyuXp/czrcR588EF5e3urT58+2rlzpz777DO99tpr3EkDXI4BcMOKj483UVFRJiQkxHh5eZmKFSuaTp06mVWrVhljjAkJCTFvvfVWrvvPmDHDBAQEXPE4mzZtMl27djU33XST8fDwMGXKlDHt2rUzH3/8scnMzHT2k2RmzJhhjDHm+PHjZvDgwSYsLMz4+fmZEiVKmHr16plx48aZjIyMbMdo0aKF+fe//+1cf+KJJ0ydOnVyrCcpKcm4u7ubTz/91Nl2uXPdt2+f6dKli6lYsaIpXry4qVevnnnnnXdc6vjkk09M+/btTYUKFYyXl5cJCgoyXbp0Mdu3b7/i+5Of4xhjzPbt201ERITx9vY25cuXNy+99JLL+wjAlcMYHgsIAADs4TINAACwijACAJLLs1UuXb777jvb5QGFGpdpAEAXHzSXm4oVK7o82h7AtUUYAQAAVnGZBgAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBV/x9AbvHEk7H8DgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAHyCAYAAAA5oM6mAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/fklEQVR4nO3de3zP9f//8ft755PNYZotwxybY0yHkRwKIaHkVKHwS5Qci/QhU61UigqVoYM0Ct9kn1go5zQZakoOhQxZbENmh9fvD9+9v73tPTaHPW27XS+X1+Xi/Xw9X6/X4/XeeN89n6/X622zLMsSAACAIS6mCwAAAKUbYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEl23Hjh169NFHFRYWJi8vL/n5+alJkyaaMmWK/v77b3u/Vq1ayWaz2Rdvb281atRIb731lnJycuz9+vfv79DP09NTderU0cSJE3X27Nk8x1+/fr169+6tKlWqyNPTU76+vqpXr55GjRqlX375pUDn8P3336tbt272fQQFBSkyMlKjRo2SJM2bN8+hpvyWatWqOey3SZMmstlsev311+1t3377bYH2ZbPZJEkvvPCCbDabjh8/7rT2+vXrq1WrVg5tBw8e1JAhQ1S7dm15e3urfPnyatCggQYNGqSDBw8W6D2RpN9//z1P/f/2+uuvy2az6ffff7e3Xfjzc3ZOl9r3/v37NWzYMIWHh8vX11deXl6qVq2aHn74Ya1Zs0b/fmB07s8mISHBaY333nuv/eeS+15earnw/czVrVs3eXt76+TJk/m+Zw899JDc3d119OjRfPtcyGaz6YUXXihw/6tt3759evLJJ+2/Lz4+PqpXr56ef/55/fnnn8bq+re4uDij7xGKhpvpAlA8ffDBBxoyZIjq1KmjMWPGqG7dusrMzFRCQoJmzZqlTZs2acmSJfb+1atX1/z58yVJx44d06xZszRixAglJyfr1Vdftffz9vbW6tWrJUknTpzQggULFBUVpV9++UWxsbH2fs8//7xeeuklRUZG6vnnn1etWrWUlZWlHTt26MMPP9TUqVOVlZUlV1fXfM9h+fLluu+++9SqVStNmTJFwcHBSk5OVkJCgj777DO98cYb6tSpkzZt2uSwXWRkpLp3724PLJLk6elp/3NiYqK2bdsmSYqJidHo0aMlnQ8oF+6rW7duqlGjRr4f+oVx6NAhNWnSRGXLltWoUaNUp04dpaamKikpSQsXLtS+ffsUGhp6xce5mH///Arryy+/VJ8+fRQYGKjBgwerSZMm8vT01J49e/T555+rTZs2+uabb3TXXXcVet8DBw7UPffcY3+dnJys+++/X0899ZT69Oljb/f393e6/YABA7R06VJ9+umnGjJkSJ71qampWrJkie69914FBQUVuj4TvvrqK/Xq1UuBgYF68skn1bhxY9lsNu3cuVNz5szR8uXL7b/HJsXFxendd98lkJR0FlBIGzdutFxdXa177rnHOnv2bJ71GRkZ1v/8z//YX7ds2dKqV6+eQ59z585Z1atXt3x8fKxz585ZlmVZ/fr1s3x9ffPsr0WLFpYk69ChQ5ZlWdann35qSbIGDx5s5eTk5Omfk5NjvfPOO1ZWVtZFz+POO++0atSoYWVmZuZZl52dne92kqyhQ4fmu37o0KGWJKtTp06WJGvDhg359q1atarVqVMnp+smTpxoSbL++usvp+vr1atntWzZ0v56woQJliRr3759Tvtf7JwutH//fkuS9dprrzld/9prr1mSrP3799vb8vv5FWTfe/bssXx8fKxbbrnFSk1NdbrdmjVrrMTERPvruXPnWpKsH374wWn/Tp06WVWrVi1wDReTlZVlhYSEWBEREU7Xz5w505JkLVu2rED7yyXJmjhxYqG2uRr27dtn+fr6Wo0bN7ZOnjyZZ31OTo71xRdfFHldzuT+fULJxjQNCu3ll1+WzWbT+++/7zAikMvDw0P33XffRffh7u6uiIgInTlzRn/99ddF+95+++2SpD/++EOS9OKLLyowMFBvvvmmw/B/LpvNpqFDh150VESSUlJSFBgYKDe3vAOELi6X91fj7Nmz+vTTTxUREaE333xTkjRnzpzL2ldhpaSkyMXFRTfccIPT9Zd7TkVh6tSpOnPmjGbMmJHv6ESrVq3UqFGjIq7sPFdXV/Xr109bt27Vzp0786yfO3eugoOD1aFDB/31118aMmSI6tatKz8/P91www1q06aN1q1bd8nj5E4nXSh3Surf02KSFBsbq8jISPn6+srPz0/t27cv0GjG1KlTdfr0ac2YMUMBAQF51ttsNt1///0ObXPmzFGjRo3k5eWl8uXLq1u3btq1a5dDn1atWjmd6urfv7/DVOa/p+qmTp2qsLAw+fn5KTIyUps3b3bY7t1337XXlLvkvg+LFi3SbbfdpoCAAPn4+Kh69ep67LHHLnn+uP5cv/864bqUnZ2t1atXKyIi4oqH/Pfu3Ss3NzeVK1fuov327NkjSapYsaIOHz6spKQktW3bVl5eXld0/MjISH3//fcaNmyYvv/+e2VmZl7R/iRp8eLFOnHihB577DHVqlVLd9xxh2JjY3Xq1Kkr3velREZGKicnR/fff79WrFihtLS0K95nTk6OsrKy8iz/vtbnQoXtL0nx8fEKDg5W06ZNC11jdna202NaV/kLyR977DHZbLY84TIpKUlbtmxRv3795Orqar9eauLEiVq+fLnmzp2r6tWrq1WrVvr222+vWj0vv/yyevfurbp162rhwoX6+OOPlZ6erhYtWigpKemi265cuVJBQUH2oH8p0dHRGjBggOrVq6fFixdr2rRp2rFjhyIjI/Xbb79d9jm8++67io+P11tvvaX58+fr9OnT6tixo1JTUyVJ//nPf9S9e3dJ0qZNm+xLcHCwNm3apJ49e6p69er67LPPtHz5ck2YMEFZWVmXXQ8MMj00g+LlyJEjliSrV69eBd4md5omMzPTyszMtA4fPmyNHTvWkmQ9+OCD9n65w/y5/f766y9r2rRpls1ms2655RbLsixr8+bNliRr7NixeY6TlZVl3zYzM9PpFM6/HT9+3LrjjjssSZYky93d3WrWrJkVHR1tpaen57udLjJN06ZNG8vLy8s6ceKEZVn/N5UQExPjtP/VnKbJycmxHn/8ccvFxcWSZNlsNis8PNwaMWKEw3RKQeROY1xquXCaJr9+d911V559/3uKxMvLy7r99tvz1JGdne3wM/33VFPue3ux5WpN0+Rq2bKlFRgYaJ9atCzLGjVqlCXJ2r17t9Ntcn8v77rrLqtbt24O63TBNE3uz/xCueea+34fOHDAcnNzs5566imHfunp6ValSpWsHj16XPQ88nu/nTlx4oTl7e1tdezY0aH9wIEDlqenp9WnTx97W8uWLR1+J3P169fP4WeR+/43aNDAYTp1y5YtliRrwYIF9rb8pmlef/11S5LTaSYUP8VqZGTt2rXq3LmzQkJCZLPZtHTp0kLvw7Isvf7666pdu7Y8PT0VGhqql19++eoXCwc///yz3N3d5e7urpCQEL3xxht66KGH9MEHHzj0O336tL1fxYoVNXz4cHXo0MHhYtj8VKhQwb6tu7u7vvjii0v2X7dunX744Qe98sor6tKli3bv3q1x48apQYMG+d7Fkp/9+/drzZo1uv/++1W2bFlJ0oMPPqgyZcoUyVSNzWbTrFmztG/fPs2YMUOPPvqoMjMz9eabb6pevXr67rvvCr3Pp59+Wj/88EOe5emnn3ba39vb22n/GTNmXNY53X///Q4/02HDhuXp89FHHzk95h133HFZx7yYAQMG6Pjx4/ryyy8lnR8F+uSTT9SiRQvVqlXL3m/WrFlq0qSJvLy85ObmJnd3d61atSrPtMblWrFihbKystS3b1+H0SAvLy+1bNnyqo7AbNq0Sf/884/69+/v0B4aGqo2bdpo1apVl73vTp06OUynNmzYUNL/TclezC233CJJ6tGjhxYuXHjd3P2Dy1Os7qY5ffq0GjVqpEcffVQPPPDAZe3j6aef1sqVK/X666+rQYMGSk1NLfSHTmkWGBgoHx8f7d+/v1Db1ahRQ5999plsNpu8vLwUFhYmHx+fPP28vb21du1aSefvUKlatarDNQS5U0PO/rH69ttvlZWVpa1bt2rw4MEFrq1p06b26YHMzEw9++yzevPNNzVlyhRNmTKlwPuZM2eOLMtS9+7dHW4Bve+++zR//nz98ssvuummmwq8v9xrWbKzs52uz8rKkru7e572qlWr6oknnrC/XrhwoXr37q0xY8Zoy5YtBT6+JFWuXNnp1El+H3YuLi6XNdVSpUoVpz/TN954Q88//7yk//vwuVB4eLjTYwYEBBTqduaC6N69u5566inNnTtXDzzwgOLi4nT06FGHO8KmTp2qUaNGafDgwZo8ebICAwPl6uqq//znP1ctjOTePpzfe3Kp64OqVKlS4L/DKSkpkqTg4OA860JCQhQfH1+g/ThToUIFh9e516D9888/l9z2zjvv1NKlSzV9+nT17dtXGRkZqlevnsaPH6/evXtfdk0wo1iFkQ4dOqhDhw75rj937pyef/55zZ8/XydPnlT9+vX16quv2i+o2rVrl2bOnKmffvpJderUKaKqSxZXV1fddddd+u9//6tDhw6pcuXKBdrOy8urQB9Sl/owCwkJUb169RQfH6+zZ886XDdy8803S9IVXZ/h7u6uiRMn6s0339RPP/1U4O1ycnI0b948Scpz4V+uOXPmFCrc5N4i+ueff+a5XdSyLCUnJxfoPe3Ro4eio6MLdT5FrW3btnr33XeVkJDgcE41atQwWFVe3t7e6t27tz744AMlJydrzpw5KlOmjB588EF7n08++UStWrXSzJkzHbZNT0+/5P5zf58zMjIcLg6/8D9MgYGBkqTPP/9cVatWLfR5tG/fXm+//bY2b958yetGcgNDcnJynnWHDx+215Jbf+71Hv92rf7D16VLF3Xp0kUZGRnavHmzoqOj1adPH1WrVk2RkZHX5Ji4NorVNM2lPProo9qwYYM+++wz7dixQw8++KDuuece+wVWy5YtU/Xq1fXVV18pLCxM1apV08CBAx0e0IVLGzdunCzL0qBBg3Tu3Lk86zMzM7Vs2bJrdvzx48fr+PHjGjly5BVdpOjsH1dJ9v+9hoSEFHhfK1as0KFDhzR06FCtWbMmz1KvXj199NFHhbq4rk2bNrLZbA7PV8n19ddfKy0tTXffffclz+fUqVM6ePBgoc6nqI0YMUI+Pj4aOnRogT60TRowYICys7P12muvKS4uTr169XIY5ct9YN+/7dixI88zZpzJveNkx44dDu0X/n1q37693NzctHfvXvvI3oXLxYwYMUK+vr4aMmSI0/BgWZZ9ajQyMlLe3t765JNPHPocOnRIq1evdnjuS7Vq1bR7925lZGTY21JSUrRx48ZLnnt+CjJa4unpqZYtW9pHqK6H56OgcIrVyMjF7N27VwsWLNChQ4fs/+iOHj1aX3/9tebOnauXX35Z+/bt0x9//KFFixbpo48+UnZ2tkaMGKHu3btf9oOaSqPIyEjNnDlTQ4YMUUREhJ544gnVq1dPmZmZ2rZtm95//33Vr19fnTt3vibH7927t37++We99NJL2r59u/r3769atWopJydHBw8e1McffyxJKlOmjH2bqKgoRUVFadWqVWrZsqWk8/+gV65cWZ07d9ZNN92knJwcJSYm6o033pCfn1++10U4ExMTIzc3Nz333HNOP/Qff/xxDRs2TMuXL1eXLl0KtM8aNWroySef1GuvvaaTJ0+qY8eO9msyXnnlFTVt2tThgV0vvfSSNmzYoJ49e+rmm2+Wt7e39u/fr3feeUcpKSl67bXXCnw+lysnJ8fh1sx/a9y4sdNbwaXz57pgwQL17t1bDRo00BNPPGF/6NmxY8e0cuVKSfk/lKwoNW3aVA0bNtRbb70ly7I0YMAAh/X33nuvJk+erIkTJ6ply5b69ddfFRUVpbCwsEuG0Y4dO6p8+fIaMGCAoqKi5Obmpnnz5uWZbqpWrZqioqI0fvx47du3T/fcc4/KlSuno0ePasuWLfL19dWkSZPyPU5YWJg+++wz++9K7kPPpPN3B+VOOXbr1k1ly5bVf/7zHz333HPq27evevfurZSUFE2aNEleXl6aOHGifb+PPPKI3nvvPT388MMaNGiQUlJSNGXKlCv6uTVo0ECS9Oqrr6pDhw5ydXVVw4YN9eKLL+rQoUO66667VLlyZZ08eVLTpk2Tu7u7/e84ihFz185eGUnWkiVL7K8XLlxoSbJ8fX0dFjc3N/uV5YMGDbIkWb/++qt9u61bt1qSrF9++aWoT6HYS0xMtPr162dVqVLF8vDwsD9EacKECdaxY8fs/Zw99MyZgj40K9fatWutnj17WpUrV7bc3d0tHx8fq27dutYTTzxhJSQkOPTNvUthzZo19rbY2FirT58+Vq1atSw/Pz/L3d3dqlKlivXII49YSUlJ+R5XF9xN89dff1keHh5W165d890m946Ezp07O7Rf7G4ayzp/h8zMmTOtpk2bWj4+PpaHh4dVq1Yt69lnn81zx8/mzZutoUOHWo0aNbLKly9vubq6WhUrVrTuueceKy4uLt9jOHO5Dz3TRe5s+e233y65771791pPPfWUVadOHcvb29vy9PS0qlataj344IPWkiVLHO6QKsqHnl1o2rRpliSrbt26edZlZGRYo0ePtm688UbLy8vLatKkibV06dI8d5RYlvOHnm3ZssVq1qyZ5evra914443WxIkTrdmzZ+d5vy3LspYuXWq1bt3a8vf3t79X3bt3t7755psCncfevXutIUOGWDVr1rQ8PT0tb29vq27dutbIkSPzHGv27NlWw4YNLQ8PDysgIMDq0qWL9fPPP+fZ54cffmiFh4dbXl5eVt26da3Y2Nh876Zx9v5f+J5kZGRYAwcOtCpWrGjZbDb7+/DVV19ZHTp0sG688UbLw8PDuuGGG6yOHTta69atK9C54/pis6yrfDN+EbHZbFqyZIm6du0q6fzDfx566CH9/PPPeR525efnp0qVKmnixIl6+eWXHZ4n8c8//8jHx0crV65U27Zti/IUAACAStA0TePGjZWdna1jx46pRYsWTvs0b95cWVlZ2rt3r/3CuN27d0vSZV0EBgAArlyxGhk5deqU/WmcjRs31tSpU9W6dWuVL19eVapU0cMPP6wNGzbojTfeUOPGjXX8+HGtXr1aDRo0UMeOHZWTk6NbbrlFfn5+9m+MHTp0qPz9/e1z0kBJZllWvrcK53J1dXX6SHIAuFaK1d00CQkJaty4sf1Cq5EjR6px48aaMGGCpPPfD9G3b1/7N5bed999+v777+3PpnBxcdGyZcsUGBioO++8U506dVJ4eLg+++wzY+cEFKXvvvvO4SFizpYPP/zQdJkASpliNTIC4Mqkp6fr119/vWifsLCwPA+jAoBriTACAACMKhYXsObk5Ojw4cMqU6YMc9kAABQTlmUpPT1dISEhF/2agmIRRg4fPnzFX1cPAADMOHjw4EW/PqRYhJHcJ2kePHjwungCIwAAuLS0tDSFhoY6PBHbmWIRRnKnZvz9/QkjAAAUM5e6xKJY3doLAABKHsIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCp0GFm7dq06d+6skJAQ2Ww2LV269JLbfPfdd4qIiJCXl5eqV6+uWbNmXU6tAACgBCp0GDl9+rQaNWqkd955p0D99+/fr44dO6pFixbatm2bnnvuOQ0bNkxffPFFoYsFAAAlT6G/tbdDhw7q0KFDgfvPmjVLVapU0VtvvSVJCg8PV0JCgl5//XU98MADhT08AAAoYa75NSObNm1Su3btHNrat2+vhIQEZWZmOt0mIyNDaWlpDgsAACiZCj0yUlhHjhxRUFCQQ1tQUJCysrJ0/PhxBQcH59kmOjpakyZNutalXXXVxi43XUKJ8fsrnUyXAAAoIkVyN43NZnN4bVmW0/Zc48aNU2pqqn05ePDgNa8RAACYcc1HRipVqqQjR444tB07dkxubm6qUKGC0208PT3l6el5rUsDAADXgWs+MhIZGan4+HiHtpUrV6pp06Zyd3e/1ocHAADXuUKHkVOnTikxMVGJiYmSzt+6m5iYqAMHDkg6P8XSt29fe//Bgwfrjz/+0MiRI7Vr1y7NmTNHMTExGj169NU5AwAAUKwVepomISFBrVu3tr8eOXKkJKlfv36aN2+ekpOT7cFEksLCwhQXF6cRI0bo3XffVUhIiKZPn85tvQAAQJJks3KvJr2OpaWlKSAgQKmpqfL39zddTr64m+bq4W4aACj+Cvr5zXfTAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMOqywsiMGTMUFhYmLy8vRUREaN26dRftP3/+fDVq1Eg+Pj4KDg7Wo48+qpSUlMsqGAAAlCyFDiOxsbEaPny4xo8fr23btqlFixbq0KGDDhw44LT/+vXr1bdvXw0YMEA///yzFi1apB9++EEDBw684uIBAEDxV+gwMnXqVA0YMEADBw5UeHi43nrrLYWGhmrmzJlO+2/evFnVqlXTsGHDFBYWpjvuuEOPP/64EhISrrh4AABQ/BUqjJw7d05bt25Vu3btHNrbtWunjRs3Ot2mWbNmOnTokOLi4mRZlo4eParPP/9cnTp1yvc4GRkZSktLc1gAAEDJVKgwcvz4cWVnZysoKMihPSgoSEeOHHG6TbNmzTR//nz17NlTHh4eqlSpksqWLau333473+NER0crICDAvoSGhhamTAAAUIxc1gWsNpvN4bVlWXnaciUlJWnYsGGaMGGCtm7dqq+//lr79+/X4MGD893/uHHjlJqaal8OHjx4OWUCAIBiwK0wnQMDA+Xq6ppnFOTYsWN5RktyRUdHq3nz5hozZowkqWHDhvL19VWLFi304osvKjg4OM82np6e8vT0LExpAACgmCrUyIiHh4ciIiIUHx/v0B4fH69mzZo53ebMmTNycXE8jKurq6TzIyoAAKB0K/Q0zciRIzV79mzNmTNHu3bt0ogRI3TgwAH7tMu4cePUt29fe//OnTtr8eLFmjlzpvbt26cNGzZo2LBhuvXWWxUSEnL1zgQAABRLhZqmkaSePXsqJSVFUVFRSk5OVv369RUXF6eqVatKkpKTkx2eOdK/f3+lp6frnXfe0ahRo1S2bFm1adNGr7766tU7CwAAUGzZrGIwV5KWlqaAgAClpqbK39/fdDn5qjZ2uekSSozfX8n/1m8AQPFQ0M9vvpsGAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABg1GWFkRkzZigsLExeXl6KiIjQunXrLto/IyND48ePV9WqVeXp6akaNWpozpw5l1UwAAAoWdwKu0FsbKyGDx+uGTNmqHnz5nrvvffUoUMHJSUlqUqVKk636dGjh44ePaqYmBjVrFlTx44dU1ZW1hUXDwAAij+bZVlWYTa47bbb1KRJE82cOdPeFh4erq5duyo6OjpP/6+//lq9evXSvn37VL58+csqMi0tTQEBAUpNTZW/v/9l7aMoVBu73HQJJcbvr3QyXQIA4AoV9PO7UNM0586d09atW9WuXTuH9nbt2mnjxo1Ot/nyyy/VtGlTTZkyRTfeeKNq166t0aNH659//sn3OBkZGUpLS3NYAABAyVSoaZrjx48rOztbQUFBDu1BQUE6cuSI02327dun9evXy8vLS0uWLNHx48c1ZMgQ/f333/leNxIdHa1JkyYVpjQAAFBMXdYFrDabzeG1ZVl52nLl5OTIZrNp/vz5uvXWW9WxY0dNnTpV8+bNy3d0ZNy4cUpNTbUvBw8evJwyAQBAMVCokZHAwEC5urrmGQU5duxYntGSXMHBwbrxxhsVEBBgbwsPD5dlWTp06JBq1aqVZxtPT095enoWpjQAAFBMFWpkxMPDQxEREYqPj3doj4+PV7NmzZxu07x5cx0+fFinTp2yt+3evVsuLi6qXLnyZZQMAABKkkJP04wcOVKzZ8/WnDlztGvXLo0YMUIHDhzQ4MGDJZ2fYunbt6+9f58+fVShQgU9+uijSkpK0tq1azVmzBg99thj8vb2vnpnAgAAiqVCP2ekZ8+eSklJUVRUlJKTk1W/fn3FxcWpatWqkqTk5GQdOHDA3t/Pz0/x8fF66qmn1LRpU1WoUEE9evTQiy++ePXOAgAAFFuFfs6ICTxnpPThOSMAUPxdk+eMAAAAXG2EEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABh1WWFkxowZCgsLk5eXlyIiIrRu3boCbbdhwwa5ubnp5ptvvpzDAgCAEqjQYSQ2NlbDhw/X+PHjtW3bNrVo0UIdOnTQgQMHLrpdamqq+vbtq7vuuuuyiwUAACVPocPI1KlTNWDAAA0cOFDh4eF66623FBoaqpkzZ150u8cff1x9+vRRZGTkZRcLAABKnkKFkXPnzmnr1q1q166dQ3u7du20cePGfLebO3eu9u7dq4kTJxboOBkZGUpLS3NYAABAyVSoMHL8+HFlZ2crKCjIoT0oKEhHjhxxus1vv/2msWPHav78+XJzcyvQcaKjoxUQEGBfQkNDC1MmAAAoRi7rAlabzebw2rKsPG2SlJ2drT59+mjSpEmqXbt2gfc/btw4paam2peDBw9eTpkAAKAYKNhQxf8KDAyUq6trnlGQY8eO5RktkaT09HQlJCRo27ZtevLJJyVJOTk5sixLbm5uWrlypdq0aZNnO09PT3l6ehamNAAAUEwVamTEw8NDERERio+Pd2iPj49Xs2bN8vT39/fXzp07lZiYaF8GDx6sOnXqKDExUbfddtuVVQ8AAIq9Qo2MSNLIkSP1yCOPqGnTpoqMjNT777+vAwcOaPDgwZLOT7H8+eef+uijj+Ti4qL69es7bH/DDTfIy8srTzsAACidCh1GevbsqZSUFEVFRSk5OVn169dXXFycqlatKklKTk6+5DNHAAAActksy7JMF3EpaWlpCggIUGpqqvz9/U2Xk69qY5ebLqHE+P2VTqZLAABcoYJ+fvPdNAAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMuK4zMmDFDYWFh8vLyUkREhNatW5dv38WLF6tt27aqWLGi/P39FRkZqRUrVlx2wQAAoGQpdBiJjY3V8OHDNX78eG3btk0tWrRQhw4ddODAAaf9165dq7Zt2youLk5bt25V69at1blzZ23btu2KiwcAAMWfzbIsqzAb3HbbbWrSpIlmzpxpbwsPD1fXrl0VHR1doH3Uq1dPPXv21IQJEwrUPy0tTQEBAUpNTZW/v39hyi1S1cYuN11CifH7K51MlwAAuEIF/fwu1MjIuXPntHXrVrVr186hvV27dtq4cWOB9pGTk6P09HSVL18+3z4ZGRlKS0tzWAAAQMlUqDBy/PhxZWdnKygoyKE9KChIR44cKdA+3njjDZ0+fVo9evTIt090dLQCAgLsS2hoaGHKBAAAxchlXcBqs9kcXluWlafNmQULFuiFF15QbGysbrjhhnz7jRs3Tqmpqfbl4MGDl1MmAAAoBtwK0zkwMFCurq55RkGOHTuWZ7TkQrGxsRowYIAWLVqku++++6J9PT095enpWZjSAABAMVWokREPDw9FREQoPj7eoT0+Pl7NmjXLd7sFCxaof//++vTTT9WpExcmAgCA/1OokRFJGjlypB555BE1bdpUkZGRev/993XgwAENHjxY0vkplj///FMfffSRpPNBpG/fvpo2bZpuv/12+6iKt7e3AgICruKpAACA4qjQYaRnz55KSUlRVFSUkpOTVb9+fcXFxalq1aqSpOTkZIdnjrz33nvKysrS0KFDNXToUHt7v379NG/evCs/AwAAUKwV+jkjJvCckdKH54wAQPF3TZ4zAgAAcLURRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGDUZYWRGTNmKCwsTF5eXoqIiNC6desu2v+7775TRESEvLy8VL16dc2aNeuyigUAACVPocNIbGyshg8frvHjx2vbtm1q0aKFOnTooAMHDjjtv3//fnXs2FEtWrTQtm3b9Nxzz2nYsGH64osvrrh4AABQ/Nksy7IKs8Ftt92mJk2aaObMmfa28PBwde3aVdHR0Xn6P/vss/ryyy+1a9cue9vgwYO1fft2bdq0qUDHTEtLU0BAgFJTU+Xv71+YcotUtbHLTZdQYvz+SifTJQAArlBBP7/dCrPTc+fOaevWrRo7dqxDe7t27bRx40an22zatEnt2rVzaGvfvr1iYmKUmZkpd3f3PNtkZGQoIyPD/jo1NVXS+ZO6nuVknDFdQolxvf+sAQCXlvtv+aXGPQoVRo4fP67s7GwFBQU5tAcFBenIkSNOtzly5IjT/llZWTp+/LiCg4PzbBMdHa1JkyblaQ8NDS1MuSjGAt4yXQEA4GpJT09XQEBAvusLFUZy2Ww2h9eWZeVpu1R/Z+25xo0bp5EjR9pf5+Tk6O+//1aFChUuehxcWlpamkJDQ3Xw4MHresoLpQe/k7je8Dt59ViWpfT0dIWEhFy0X6HCSGBgoFxdXfOMghw7dizP6EeuSpUqOe3v5uamChUqON3G09NTnp6eDm1ly5YtTKm4BH9/f/6S4brC7ySuN/xOXh0XGxHJVai7aTw8PBQREaH4+HiH9vj4eDVr1szpNpGRkXn6r1y5Uk2bNnV6vQgAAChdCn1r78iRIzV79mzNmTNHu3bt0ogRI3TgwAENHjxY0vkplr59+9r7Dx48WH/88YdGjhypXbt2ac6cOYqJidHo0aOv3lkAAIBiq9DXjPTs2VMpKSmKiopScnKy6tevr7i4OFWtWlWSlJyc7PDMkbCwMMXFxWnEiBF69913FRISounTp+uBBx64emeBAvP09NTEiRPzTIMBpvA7iesNv5NFr9DPGQEAALia+G4aAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAJw4evSooqKiTJdRKnA3DQAATmzfvl1NmjRRdna26VJKPEZGAACAUYSREmzPnj3aunWrQ9uqVavUunVr3XrrrXr55ZcNVQY4t2vXLlWvXt10GQCKGGGkBBszZoyWLl1qf71//3517txZHh4eioyMVHR0tN566y1j9QEXOnfunP744w/TZQAoYoV+HDyKj4SEBD3zzDP21/Pnz1ft2rW1YsUKSVLDhg319ttva/jw4YYqBABzRo4cedH1f/31VxFVAsJICXb8+HFVrlzZ/nrNmjXq3Lmz/XWrVq00atQoE6UBgHHbtm27ZJ8777yzCCoBYaQEK1++vJKTkxUaGqqcnBwlJCRoxIgR9vXnzp0TN1MBKK3WrFljugT8L8JICdayZUtNnjxZM2bM0KJFi5STk6PWrVvb1yclJalatWrmCkSpU65cOdlstnzXZ2VlFWE1wMXt3btXgwYN0urVq02XUuIRRkqwl156SW3btlW1atXk4uKi6dOny9fX177+448/Vps2bQxWiNKGC6ZRnJw6dUrfffed6TJKBR56VsJlZmYqKSlJFStWVEhIiMO67du3q3LlyqpQoYKh6gDg+sVDz4oOIyMlnLu7uxo1auTQlpWVpbNnz+ZpBwDABJ4zUoLFxcXp448/dmh76aWX5Ofnp7Jly6pdu3Y6ceKEoepQGpUrV07ly5e/5AKgdGFkpAR7/fXX9cADD9hfb9y4URMmTFBUVJTCw8M1fvx4TZ48WVOnTjVYJUoTrhnB9aRx48YXvaD6zJkzRVhN6UYYKcF++uknvfHGG/bXn3/+udq2bavx48dLkry8vPT0008TRlBk+vXrd8k+3FGDotK1a1fTJeB/EUZKsPT0dIeLU9evX6/u3bvbX9erV0+HDx82URqQR1JSkmJiYvTJJ5/o6NGjpstBKTBx4kTTJeB/cc1ICRYSEqJdu3ZJOn+L2vbt29W8eXP7+pSUFPn4+JgqD9CpU6c0e/ZsRUZGqmHDhvr+++81duxY02WhlDh27NhF12dlZWnLli1FVE3pxshICda9e3cNHz5czz33nOLi4lSpUiXdfvvt9vUJCQmqU6eOwQpRWq1fv16zZ8/WF198obCwMCUlJem7775zCMvAtRYcHKzk5GTdcMMNkqTw8HCtWLFCVapUkXT+P2yRkZHc2lsEGBkpwSZOnKimTZtq2LBhSkxM1CeffCJXV1f7+gULFjh8Vw1wrU2ZMkU33XSTevXqpYoVK2r9+vXasWOHbDabypUrZ7o8lDIXPmbr0KFDea5Z4lFcRYORkRLMx8cnz629/8b3MqCoPffcc3r22WcVFRXlEIyB69XF7rbB1cPISCl14sQJvf3227r55ptNl4JSJCoqSosWLVJYWJieffZZ/fTTT6ZLAnAdIIyUMt9884169+6tkJAQTZkyRS1btjRdEkqR5557Trt379bHH3+sI0eO6Pbbb1ejRo1kWRYP4EORs9lsSk9PV1pamlJTU2Wz2XTq1CmlpaXZFxQNvpumFDhw4IDmzp2ruXPn6tSpUzpx4oQWLlzo8EA0oCjs27dPYWFh9qHv9PR0zZ8/X3PnztXWrVt16623qnv37ho5cqThSlEauLi4OEzDWJbl9DUXsF57hJESbOHChZo9e7Y2bNigjh076uGHH1aHDh3k6+ur7du3q27duqZLRCnj6urqcPdCz549NX36dAUFBWnnzp2KiYnRp59+eslbLoGroaDfyMsI8rVHGCnB3Nzc9Mwzz2jcuHEqU6aMvd3d3Z0wAiNcXFx05MgRexgpU6aMtm/frurVq9v7ZGZmyt3d3VSJAAzgmpES7LHHHtOMGTN0zz33aNasWczJo1ggiKCoHD58WKNHj3Z6bUhqaqrGjBnD04CLCGGkBHv//feVnJys//f//p8WLFig4OBgdenSRZZlKScnx3R5KIVsNlueWyW5dRKmTJ06VWlpafL398+zLiAgQOnp6Xx3VxFhmqYU2bNnj2bPnq2PP/5Yp06dUqdOndS9e3fdf//9pktDKeHi4qIOHTrI09NTkrRs2TK1adNGvr6+Dv0WL15sojyUMvXr19esWbN0xx13OF2/ceNGDRo0SD///HMRV1b6EEZKsDNnzmjMmDFaunSpMjMzdffdd2v69OkqX768li9frpiYGP33v/9VRkaG6VJRSjz66KMF6jd37txrXAkg+fr6ateuXfbHv1/owIEDCg8P1+nTp4u4stKHMFKCjRkzRjNmzNBDDz0kLy8vLViwQK1atdKiRYvsfY4dO2a/mBAASpPAwEAtXrxYd955p9P1a9eu1f3336/jx48XcWWlD2GkBKtRo4Zeeukl9erVS5K0ZcsWNW/eXGfPnuVR3ABKvU6dOikkJEQffPCB0/UDBw7U4cOHFRcXV8SVlT58N00JdvDgQbVo0cL++tZbb5Wbm5sOHz6s0NBQg5UBgHmjR49W27ZtFRAQoDFjxigoKEiSdPToUU2ZMkXz5s3TypUrDVdZOjAyUoK5urrqyJEjqlixor2tTJky2rFjh8LCwgxWBgDXh/fee09PP/20MjMz5e/vL5vNptTUVLm7u+vNN9/UE088YbrEUoEwUoJdeOeC5PzuBe5cAFCa/fnnn1q4cKH27Nkjy7JUu3Ztde/eXZUrVzZdWqlBGCnBuHMBAFAcEEYAAIBRPIEVAAAYRRgBAABGEUYAAIBRPGcEAFCq/fPPP4qPj9fu3btls9lUq1YttW3bVt7e3qZLKzUIIwCAUuvLL7/UwIED8zzyPTAwUDExMercubOhykoXpmkAAKXSxo0b1b17d915553asGGD/v77b/39999av369WrRooe7du2vTpk2myywVuLUXAFAqdezYUaGhoXrvvfecrn/88cd18OBBvpumCBBGAAClUrly5bR27Vo1aNDA6fodO3aoZcuWOnHiRBFXVvowTQMAKJXOnj0rf3//fNcHBAQoIyOjCCsqvQgjAIBSqXbt2lq9enW+61etWqWaNWsWYUWlF2EEAFAq9e/fX6NHj3Z6Tcjy5cv1zDPPFPg7vnBluGYEAFAq5eTkqGfPnvriiy9Up04dhYeHS5KSkpL022+/qWvXrlq0aJFcXPh/+7VGGAEAlGqxsbFasGCBdu/eLen89E2vXr3Uq1cvw5WVHoQRAABgFGNPAADAKB4HDwAolVxcXGSz2S7ax2azKSsrq4gqKr0IIwCAUmnJkiX5rtu4caPefvttcSVD0eCaEQAA/tcvv/yicePGadmyZXrooYc0efJkValSxXRZJR7XjAAASr3Dhw9r0KBBatiwobKyspSYmKgPP/yQIFJECCMAgFIrNTVVzz77rGrWrKmff/5Zq1at0rJly1S/fn3TpZUqXDMCACiVpkyZoldffVWVKlXSggUL1KVLF9MllVpcMwIAKJVcXFzk7e2tu+++W66urvn2W7x4cRFWVToxMgIAKJX69u17yVt7UTQYGQEAAEZxASsAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMANfQkSNH9NRTT6l69ery9PRUaGioOnfurFWrVkmSqlWrJpvNJpvNJh8fH9WvX1/vvfeefft58+bZ19tsNgUHB6tHjx7av3+/w3G2bdumnj17Kjg4WJ6enqpataruvfdeLVu27KLfOrpv3z717t1bISEh8vLyUuXKldWlSxft3r07z7GdLd9++60k6dChQ/Lw8NBNN91k3/cLL7xwye1///139e/fX127ds1TW2Jior1Prvfee0+NGjWSr6+vypYtq8aNG+vVV18t0M/ihRde0M0335yn/ffff5fNZlNiYqLDa2fL5s2b7T+XsmXLOuzn3Llzeu2119SkSRP5+voqICBAjRo10vPPP6/Dhw/b++V3vt9++61sNptOnjyp/v37X/K9A0oSwghwjfz++++KiIjQ6tWrNWXKFO3cuVNff/21WrduraFDh9r7RUVFKTk5WTt27FDXrl01ePBgxcbG2tf7+/srOTlZhw8f1qeffqrExETdd999ys7OliT9z//8j26//XadOnVKH374oZKSkrRo0SJ17dpVzz//vFJTU53Wd+7cObVt21ZpaWlavHixfv31V8XGxqp+/fpKTU1Vz549lZycbF8iIyM1aNAgh7ZmzZpJOv/h3KNHD505c0YbNmyQJI0ePdqhb+XKle3nmruEhoYW+P2MiYnRyJEjNWzYMG3fvl0bNmzQM888o1OnThX6Z1MQ33zzjUOtycnJioiIcNo3IyNDbdu21csvv6z+/ftr7dq12rp1q6ZMmaKUlBS9/fbbhTr2tGnTHI4rSXPnzs3TBpQUPIEVuEaGDBkim82mLVu2yNfX195er149PfbYY/bXZcqUUaVKlSRJL774ohYuXKilS5eqZ8+ekiSbzWZfHxwcrIkTJ+rhhx/Wnj17VLlyZQ0YMECdOnVyeGR1jRo1dOutt2rgwIH5jowkJSVp3759Wr16tapWrSpJqlq1qpo3b27v4+3tbf+zh4eHfHx87LXksixLc+fO1YwZM1S5cmXFxMSoefPm8vPzk5+fn72fq6urw7kW1rJly9SjRw8NGDDA3lavXr3L2ldBVKhQocC1vvnmm1q/fr0SEhLUuHFje3vNmjXVvn37i45OORMQEKCAgACHtrJly172ewdc7xgZAa6Bv//+W19//bWGDh3qEERyXTjE/29eXl7KzMzMd31uQMjMzNTKlSuVkpKiZ555Jt/++Q3pV6xYUS4uLvr888/toyyXY82aNTpz5ozuvvtuPfLII1q4cKHS09Mve3/5qVSpkjZv3qw//vjjqu/7Si1YsEBt27Z1CCL/xrQKcHGEEeAa2LNnjyzLcriG4lKysrI0b9487dy5U3fddZfTPocOHdJrr72mypUrq3bt2tq9e7ckqU6dOvY+P/zwg31Uws/PT1999ZXTfd14442aPn26JkyYoHLlyqlNmzaaPHmy9u3bV4gzPT990qtXL7m6uqpevXqqWbOmwzTT1TJx4kSVLVtW1apVU506ddS/f38tXLhQOTk5Bd7Hzp07Hd4bPz+/fEdXmjVrlqdvfqFt9+7dDj8DSerWrZt9u9zprFxfffVVnn136NChwOcBlDSEEeAayB2WL8j/iJ999ln5+fnJ29tbQ4cO1ZgxY/T444/b16empsrPz0++vr4KDQ3VuXPntHjxYnl4eDjdX8OGDZWYmKjExESdPn1aWVlZ+R576NChOnLkiD755BNFRkZq0aJFqlevnuLj4wt0nidPntTixYv18MMP29sefvhhzZkzp0DbF0ZwcLA2bdqknTt3atiwYcrMzFS/fv10zz33FDiQ1KlTx/7e5C5xcXFO+8bGxubpe7Fvdr3wZz1jxgwlJibqscce05kzZxzWtW7dOs++Z8+eXaBzAEoirhkBroFatWrJZrNp165dTu+c+LcxY8aof//+8vHxUXBwcJ4PtTJlyujHH3+Ui4uLgoKCHKZ9atWqJUn69ddfdfvtt0uSPD09VbNmzQLXWqZMGd13332677779OKLL6p9+/Z68cUX1bZt20tu++mnn+rs2bO67bbb7G2WZSknJ0dJSUmqW7fuJffh7+/vdOrl5MmTkpTn2on69eurfv36Gjp0qNavX68WLVrou+++U+vWrS95LA8PjzzvjZub838GQ0NDC/w+1qpVS7/88otDW3BwsCSpfPnyefr7+vrm2fehQ4cKdCygJGJkBLgGypcvr/bt2+vdd9/V6dOn86zP/aCVpMDAQNWsWVMhISFOR1JcXFxUs2ZNVa9ePc/1J+3atVP58uULfHvrpdhsNt10001Oa3YmJiZGo0aNcvgf/vbt29W6desCj47cdNNN+umnn3T27FmH9h9++EEVK1ZUuXLl8t02N+wUtN5rpXfv3oqPj9e2bduM1gEUV4QR4BqZMWOGsrOzdeutt+qLL77Qb7/9pl27dmn69OmKjIy8Ksfw8/PT7NmztXz5cnXq1EkrVqzQvn37tGPHDk2ZMkWSHKYWbrrpJi1ZskTS+ed4dOnSRZ9//rmSkpK0Z88excTEaM6cOerSpcslj52YmKgff/xRAwcOtI9W5C69e/fWRx99dNELcXM99NBDcnNz0yOPPKKEhATt3btXn3zyiaKjozVmzBh7vyeeeEKTJ0/Whg0b9Mcff2jz5s3q27evKlaseNXez39LSUnRkSNHHJYLA1OuESNGKDIyUm3atNG0adP0448/av/+/VqxYoX++9//XnR6BwBhBLhmwsLC9OOPP6p169YaNWqU6tevr7Zt22rVqlWaOXPmVTtOt27dtHHjRvn4+Khv376qU6eO2rRpo9WrV+uzzz7Tvffea+/766+/2p87UrlyZVWrVk2TJk3SbbfdpiZNmmjatGmaNGmSxo8ff8njxsTEqG7duk4v0u3atav+/vtvLVu27JL7CQgI0Lp162RZlrp27apGjRppypQpmjx5skaNGmXvd/fdd2vz5s168MEHVbt2bT3wwAPy8vLSqlWrVKFChYK8VYVy9913Kzg42GFZunSp0765dYwdO1Zz587VHXfcofDwcA0fPlzNmzfPdzsA59mswt4ADwAAcBUxMgIAAIwijAAoES58bse/l3Xr1pkuD8BFME0DoETYs2dPvutuvPFGh0fbA7i+EEYAAIBRTNMAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMOr/A1fz3XoUwIAlAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for column in cppg_df.columns:\n",
    "    plt.figure()\n",
    "    cppg_df[column].value_counts().plot.bar()\n",
    "    plt.title(f'{column} Value Counts')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: zuerst prüfen ob Classification Binary möglich, dann multi-class (different Failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check here, whether other Fail Values are in DF\n",
    "y = []\n",
    "\n",
    "contains_failure = cppg_df.isin([\"FAIL\"]).any(axis=1)\n",
    "\n",
    "for item in contains_failure:\n",
    "    if item:\n",
    "        y.append(\"FAIL\")\n",
    "    else:\n",
    "        y.append(\"PASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"labels_clustered.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(y, f)\n",
    "\n",
    "# with open(\"labels.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(cppg_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final_df.pickle\", \"rb\") as f:\n",
    "    final_df = pickle.load(f)\n",
    "\n",
    "with open(\"labels.pickle\", \"rb\") as f:\n",
    "    y = pickle.load(f)\n",
    "\n",
    "with open(\"labels_clustered.pickle\", \"rb\") as f:\n",
    "    y_clustered = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS    973464\n",
      "FAIL     26536\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y_clustered)\n",
    "\n",
    "value_distribution = pd.DataFrame([le.classes_[line] for line in y_encoded])\n",
    "print(value_distribution.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_one_hot = to_categorical(y_encoded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersampling values for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=RANDOM_STATE)\n",
    "\n",
    "x_resampled, y_resampled = rus.fit_resample(final_df, y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_resampled = to_categorical(y_resampled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Models Comparison"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_models():\n",
    "    return [\n",
    "        ('Decision Tree', DecisionTreeClassifier(random_state=RANDOM_STATE)),\n",
    "        ('Random Forest', RandomForestClassifier(random_state=RANDOM_STATE)),\n",
    "        ('Extremely Randomized Trees', ExtraTreesClassifier(random_state=RANDOM_STATE)),\n",
    "        ('Ada Boost', AdaBoostClassifier(random_state=RANDOM_STATE)),\n",
    "        ('Gradient Boosting', GradientBoostingClassifier(random_state=RANDOM_STATE)),\n",
    "        ('Support Vector Machine', SVC(random_state=RANDOM_STATE)),\n",
    "        # ('Multilayer Perceptron', MLPClassifier(random_state=RANDOM_STATE)),\n",
    "        ('Naive Bayes', GaussianNB()),\n",
    "    ]\n",
    "\n",
    "scorings = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'balanced_accuracy': make_scorer(balanced_accuracy_score),\n",
    "    'f1_score': make_scorer(f1_score),\n",
    "    'mcc': make_scorer(matthews_corrcoef),\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [44:10<24:12, 726.27s/it]c:\\Users\\Dominik Hahn\\anaconda3\\envs\\check_gpu_neu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dominik Hahn\\anaconda3\\envs\\check_gpu_neu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "100%|██████████| 8/8 [47:52<00:00, 359.12s/it]\n"
     ]
    }
   ],
   "source": [
    "all_cv_scores = []\n",
    "\n",
    "for classifier_name, classifier in tqdm(all_models()):\n",
    "    formatted_result = {}\n",
    "    formatted_result['Classifier'] = classifier_name\n",
    "\n",
    "    cv_score = cross_validate(estimator=classifier, X=x_resampled, y=y_resampled, scoring=scorings)\n",
    "\n",
    "    for score_name, scores in cv_score.items():\n",
    "        formatted_result[f'{score_name}_mean'] = np.mean(scores)\n",
    "        formatted_result[f'{score_name}_std'] = np.std(scores)\n",
    "\n",
    "    all_cv_scores.append(formatted_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>fit_time_mean</th>\n",
       "      <th>fit_time_std</th>\n",
       "      <th>score_time_mean</th>\n",
       "      <th>score_time_std</th>\n",
       "      <th>test_accuracy_mean</th>\n",
       "      <th>test_accuracy_std</th>\n",
       "      <th>test_balanced_accuracy_mean</th>\n",
       "      <th>test_balanced_accuracy_std</th>\n",
       "      <th>test_f1_score_mean</th>\n",
       "      <th>test_f1_score_std</th>\n",
       "      <th>test_mcc_mean</th>\n",
       "      <th>test_mcc_std</th>\n",
       "      <th>test_precision_mean</th>\n",
       "      <th>test_precision_std</th>\n",
       "      <th>test_recall_mean</th>\n",
       "      <th>test_recall_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>8.326296</td>\n",
       "      <td>0.164635</td>\n",
       "      <td>0.026807</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0.643371</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.643372</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.642428</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>0.286777</td>\n",
       "      <td>0.004537</td>\n",
       "      <td>0.644180</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.640752</td>\n",
       "      <td>0.005731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>37.364999</td>\n",
       "      <td>0.088898</td>\n",
       "      <td>0.201645</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.731478</td>\n",
       "      <td>0.004971</td>\n",
       "      <td>0.731478</td>\n",
       "      <td>0.004971</td>\n",
       "      <td>0.727898</td>\n",
       "      <td>0.004593</td>\n",
       "      <td>0.463144</td>\n",
       "      <td>0.009978</td>\n",
       "      <td>0.737793</td>\n",
       "      <td>0.006616</td>\n",
       "      <td>0.718307</td>\n",
       "      <td>0.005435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Extremely Randomized Trees</td>\n",
       "      <td>13.962944</td>\n",
       "      <td>0.094414</td>\n",
       "      <td>0.276862</td>\n",
       "      <td>0.006243</td>\n",
       "      <td>0.727936</td>\n",
       "      <td>0.005188</td>\n",
       "      <td>0.727936</td>\n",
       "      <td>0.005188</td>\n",
       "      <td>0.727555</td>\n",
       "      <td>0.004881</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.010377</td>\n",
       "      <td>0.728605</td>\n",
       "      <td>0.005983</td>\n",
       "      <td>0.726523</td>\n",
       "      <td>0.004680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ada Boost</td>\n",
       "      <td>28.227951</td>\n",
       "      <td>0.252147</td>\n",
       "      <td>0.387487</td>\n",
       "      <td>0.009954</td>\n",
       "      <td>0.715537</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0.715537</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0.713480</td>\n",
       "      <td>0.004214</td>\n",
       "      <td>0.431120</td>\n",
       "      <td>0.008271</td>\n",
       "      <td>0.718677</td>\n",
       "      <td>0.004190</td>\n",
       "      <td>0.708358</td>\n",
       "      <td>0.004332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>144.577302</td>\n",
       "      <td>1.635258</td>\n",
       "      <td>0.042810</td>\n",
       "      <td>0.004119</td>\n",
       "      <td>0.723809</td>\n",
       "      <td>0.003254</td>\n",
       "      <td>0.723809</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.716517</td>\n",
       "      <td>0.003677</td>\n",
       "      <td>0.448223</td>\n",
       "      <td>0.006486</td>\n",
       "      <td>0.735947</td>\n",
       "      <td>0.003632</td>\n",
       "      <td>0.698108</td>\n",
       "      <td>0.005284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>245.712112</td>\n",
       "      <td>0.995928</td>\n",
       "      <td>50.914860</td>\n",
       "      <td>0.071796</td>\n",
       "      <td>0.693643</td>\n",
       "      <td>0.004563</td>\n",
       "      <td>0.693643</td>\n",
       "      <td>0.004562</td>\n",
       "      <td>0.679901</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>0.388759</td>\n",
       "      <td>0.009139</td>\n",
       "      <td>0.711849</td>\n",
       "      <td>0.005853</td>\n",
       "      <td>0.650777</td>\n",
       "      <td>0.008879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Multilayer Perceptron</td>\n",
       "      <td>44.139722</td>\n",
       "      <td>11.553525</td>\n",
       "      <td>0.037208</td>\n",
       "      <td>0.003488</td>\n",
       "      <td>0.698071</td>\n",
       "      <td>0.006902</td>\n",
       "      <td>0.698071</td>\n",
       "      <td>0.006902</td>\n",
       "      <td>0.711023</td>\n",
       "      <td>0.006081</td>\n",
       "      <td>0.398341</td>\n",
       "      <td>0.012110</td>\n",
       "      <td>0.682315</td>\n",
       "      <td>0.013476</td>\n",
       "      <td>0.743218</td>\n",
       "      <td>0.023813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.267060</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>0.063414</td>\n",
       "      <td>0.003323</td>\n",
       "      <td>0.521801</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.521801</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.675157</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.132380</td>\n",
       "      <td>0.005102</td>\n",
       "      <td>0.511214</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.993895</td>\n",
       "      <td>0.001029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Classifier  fit_time_mean  fit_time_std  score_time_mean  \\\n",
       "0               Decision Tree       8.326296      0.164635         0.026807   \n",
       "1               Random Forest      37.364999      0.088898         0.201645   \n",
       "2  Extremely Randomized Trees      13.962944      0.094414         0.276862   \n",
       "3                   Ada Boost      28.227951      0.252147         0.387487   \n",
       "4           Gradient Boosting     144.577302      1.635258         0.042810   \n",
       "5      Support Vector Machine     245.712112      0.995928        50.914860   \n",
       "6       Multilayer Perceptron      44.139722     11.553525         0.037208   \n",
       "7                 Naive Bayes       0.267060      0.014659         0.063414   \n",
       "\n",
       "   score_time_std  test_accuracy_mean  test_accuracy_std  \\\n",
       "0        0.000980            0.643371           0.002257   \n",
       "1        0.000800            0.731478           0.004971   \n",
       "2        0.006243            0.727936           0.005188   \n",
       "3        0.009954            0.715537           0.004136   \n",
       "4        0.004119            0.723809           0.003254   \n",
       "5        0.071796            0.693643           0.004563   \n",
       "6        0.003488            0.698071           0.006902   \n",
       "7        0.003323            0.521801           0.000985   \n",
       "\n",
       "   test_balanced_accuracy_mean  test_balanced_accuracy_std  \\\n",
       "0                     0.643372                    0.002257   \n",
       "1                     0.731478                    0.004971   \n",
       "2                     0.727936                    0.005188   \n",
       "3                     0.715537                    0.004136   \n",
       "4                     0.723809                    0.003255   \n",
       "5                     0.693643                    0.004562   \n",
       "6                     0.698071                    0.006902   \n",
       "7                     0.521801                    0.001001   \n",
       "\n",
       "   test_f1_score_mean  test_f1_score_std  test_mcc_mean  test_mcc_std  \\\n",
       "0            0.642428           0.001805       0.286777      0.004537   \n",
       "1            0.727898           0.004593       0.463144      0.009978   \n",
       "2            0.727555           0.004881       0.455882      0.010377   \n",
       "3            0.713480           0.004214       0.431120      0.008271   \n",
       "4            0.716517           0.003677       0.448223      0.006486   \n",
       "5            0.679901           0.005506       0.388759      0.009139   \n",
       "6            0.711023           0.006081       0.398341      0.012110   \n",
       "7            0.675157           0.000559       0.132380      0.005102   \n",
       "\n",
       "   test_precision_mean  test_precision_std  test_recall_mean  test_recall_std  \n",
       "0             0.644180            0.004076          0.640752         0.005731  \n",
       "1             0.737793            0.006616          0.718307         0.005435  \n",
       "2             0.728605            0.005983          0.726523         0.004680  \n",
       "3             0.718677            0.004190          0.708358         0.004332  \n",
       "4             0.735947            0.003632          0.698108         0.005284  \n",
       "5             0.711849            0.005853          0.650777         0.008879  \n",
       "6             0.682315            0.013476          0.743218         0.023813  \n",
       "7             0.511214            0.000506          0.993895         0.001029  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_cv_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Accuracy Mean: Random Forest 73,15%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot Checking with previously removed imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:03<00:00,  2.51it/s]\n"
     ]
    }
   ],
   "source": [
    "all_cv_scores_without_imbalance = []\n",
    "\n",
    "oversampler = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "x_over, y_over = oversampler.fit_resample(X=df_no_na, y=le.fit_transform(y))\n",
    "\n",
    "for classifier_name, classifier in tqdm(all_models()):\n",
    "    formatted_result_over = {}\n",
    "    formatted_result_over['Classifier'] = classifier_name\n",
    "\n",
    "    cv_score = cross_validate(estimator=classifier, X=x_over, y=y_over, scoring=scorings)\n",
    "\n",
    "    for score_name, scores in cv_score.items():\n",
    "        formatted_result_over[f'{score_name}_mean'] = np.mean(scores)\n",
    "        formatted_result_over[f'{score_name}_std'] = np.std(scores)\n",
    "\n",
    "    all_cv_scores_without_imbalance.append(formatted_result_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>fit_time_mean</th>\n",
       "      <th>fit_time_std</th>\n",
       "      <th>score_time_mean</th>\n",
       "      <th>score_time_std</th>\n",
       "      <th>test_accuracy_mean</th>\n",
       "      <th>test_accuracy_std</th>\n",
       "      <th>test_balanced_accuracy_mean</th>\n",
       "      <th>test_balanced_accuracy_std</th>\n",
       "      <th>test_f1_score_mean</th>\n",
       "      <th>test_f1_score_std</th>\n",
       "      <th>test_mcc_mean</th>\n",
       "      <th>test_mcc_std</th>\n",
       "      <th>test_precision_mean</th>\n",
       "      <th>test_precision_std</th>\n",
       "      <th>test_recall_mean</th>\n",
       "      <th>test_recall_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.003702</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.995888</td>\n",
       "      <td>0.003085</td>\n",
       "      <td>0.995892</td>\n",
       "      <td>0.003074</td>\n",
       "      <td>0.995866</td>\n",
       "      <td>0.003114</td>\n",
       "      <td>0.991829</td>\n",
       "      <td>0.006093</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.991784</td>\n",
       "      <td>0.006149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.112172</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>0.009962</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.998972</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.998974</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.998972</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.997949</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.997949</td>\n",
       "      <td>0.002512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Extremely Randomized Trees</td>\n",
       "      <td>0.080817</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.011203</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ada Boost</td>\n",
       "      <td>0.079149</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.007608</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.995375</td>\n",
       "      <td>0.002997</td>\n",
       "      <td>0.995374</td>\n",
       "      <td>0.002997</td>\n",
       "      <td>0.995343</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.990811</td>\n",
       "      <td>0.005947</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.990748</td>\n",
       "      <td>0.005993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.223380</td>\n",
       "      <td>0.001430</td>\n",
       "      <td>0.002939</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.995374</td>\n",
       "      <td>0.003779</td>\n",
       "      <td>0.995371</td>\n",
       "      <td>0.003783</td>\n",
       "      <td>0.995335</td>\n",
       "      <td>0.003823</td>\n",
       "      <td>0.990818</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.990743</td>\n",
       "      <td>0.007566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.052081</td>\n",
       "      <td>0.001980</td>\n",
       "      <td>0.037913</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.560641</td>\n",
       "      <td>0.007405</td>\n",
       "      <td>0.560653</td>\n",
       "      <td>0.006770</td>\n",
       "      <td>0.691683</td>\n",
       "      <td>0.004281</td>\n",
       "      <td>0.230188</td>\n",
       "      <td>0.019414</td>\n",
       "      <td>0.532810</td>\n",
       "      <td>0.004697</td>\n",
       "      <td>0.985615</td>\n",
       "      <td>0.005041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Multilayer Perceptron</td>\n",
       "      <td>0.062324</td>\n",
       "      <td>0.004194</td>\n",
       "      <td>0.003198</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.608409</td>\n",
       "      <td>0.044981</td>\n",
       "      <td>0.608388</td>\n",
       "      <td>0.044990</td>\n",
       "      <td>0.583019</td>\n",
       "      <td>0.182454</td>\n",
       "      <td>0.279745</td>\n",
       "      <td>0.118470</td>\n",
       "      <td>0.607535</td>\n",
       "      <td>0.024857</td>\n",
       "      <td>0.680185</td>\n",
       "      <td>0.348492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.504112</td>\n",
       "      <td>0.003848</td>\n",
       "      <td>0.504116</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>0.016255</td>\n",
       "      <td>0.012075</td>\n",
       "      <td>0.061013</td>\n",
       "      <td>0.020590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008232</td>\n",
       "      <td>0.006194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.014380</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.944498</td>\n",
       "      <td>0.020993</td>\n",
       "      <td>0.944526</td>\n",
       "      <td>0.020995</td>\n",
       "      <td>0.943279</td>\n",
       "      <td>0.019389</td>\n",
       "      <td>0.892480</td>\n",
       "      <td>0.042413</td>\n",
       "      <td>0.975845</td>\n",
       "      <td>0.048309</td>\n",
       "      <td>0.914692</td>\n",
       "      <td>0.018328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Classifier  fit_time_mean  fit_time_std  score_time_mean  \\\n",
       "0               Decision Tree       0.003702      0.000748         0.003200   \n",
       "1               Random Forest       0.112172      0.001289         0.009962   \n",
       "2  Extremely Randomized Trees       0.080817      0.002773         0.011203   \n",
       "3                   Ada Boost       0.079149      0.000696         0.007608   \n",
       "4           Gradient Boosting       0.223380      0.001430         0.002939   \n",
       "5      Support Vector Machine       0.052081      0.001980         0.037913   \n",
       "6       Multilayer Perceptron       0.062324      0.004194         0.003198   \n",
       "7                 Naive Bayes       0.001600      0.000490         0.002800   \n",
       "8                         KNN       0.001800      0.000400         0.014380   \n",
       "\n",
       "   score_time_std  test_accuracy_mean  test_accuracy_std  \\\n",
       "0        0.000401            0.995888           0.003085   \n",
       "1        0.000083            0.998972           0.001259   \n",
       "2        0.000980            1.000000           0.000000   \n",
       "3        0.000487            0.995375           0.002997   \n",
       "4        0.000123            0.995374           0.003779   \n",
       "5        0.002800            0.560641           0.007405   \n",
       "6        0.000402            0.608409           0.044981   \n",
       "7        0.000400            0.504112           0.003848   \n",
       "8        0.000512            0.944498           0.020993   \n",
       "\n",
       "   test_balanced_accuracy_mean  test_balanced_accuracy_std  \\\n",
       "0                     0.995892                    0.003074   \n",
       "1                     0.998974                    0.001256   \n",
       "2                     1.000000                    0.000000   \n",
       "3                     0.995374                    0.002997   \n",
       "4                     0.995371                    0.003783   \n",
       "5                     0.560653                    0.006770   \n",
       "6                     0.608388                    0.044990   \n",
       "7                     0.504116                    0.003097   \n",
       "8                     0.944526                    0.020995   \n",
       "\n",
       "   test_f1_score_mean  test_f1_score_std  test_mcc_mean  test_mcc_std  \\\n",
       "0            0.995866           0.003114       0.991829      0.006093   \n",
       "1            0.998972           0.001259       0.997949      0.002512   \n",
       "2            1.000000           0.000000       1.000000      0.000000   \n",
       "3            0.995343           0.003021       0.990811      0.005947   \n",
       "4            0.995335           0.003823       0.990818      0.007480   \n",
       "5            0.691683           0.004281       0.230188      0.019414   \n",
       "6            0.583019           0.182454       0.279745      0.118470   \n",
       "7            0.016255           0.012075       0.061013      0.020590   \n",
       "8            0.943279           0.019389       0.892480      0.042413   \n",
       "\n",
       "   test_precision_mean  test_precision_std  test_recall_mean  test_recall_std  \n",
       "0             1.000000            0.000000          0.991784         0.006149  \n",
       "1             1.000000            0.000000          0.997949         0.002512  \n",
       "2             1.000000            0.000000          1.000000         0.000000  \n",
       "3             1.000000            0.000000          0.990748         0.005993  \n",
       "4             1.000000            0.000000          0.990743         0.007566  \n",
       "5             0.532810            0.004697          0.985615         0.005041  \n",
       "6             0.607535            0.024857          0.680185         0.348492  \n",
       "7             1.000000            0.000000          0.008232         0.006194  \n",
       "8             0.975845            0.048309          0.914692         0.018328  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_cv_scores_without_imbalance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything except Naive Bayes/KNN Performing quite Well\n",
    "\n",
    "Best: Extremely Randomized Trees: Balanced Acc: 100%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "Here for the Dataset all features will be taken into consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "number_of_features = 120\n",
    "\n",
    "x_train_feat, x_test_feat, y_train_feat, y_test_feat = train_test_split(x_resampled, y_resampled, random_state=RANDOM_STATE)\n",
    "\n",
    "# oversampler = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "# x_train_feat_over, y_train_feat_over = oversampler.fit_resample(x_train_feat, y_train_feat)\n",
    "\n",
    "random_forest = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "random_forest.fit(x_train_feat, y_train_feat)\n",
    "feature_importances = random_forest.feature_importances_[:number_of_features]\n",
    "standard_deviation_importances = np.std([tree.feature_importances_ for tree in random_forest.estimators_], axis=0)[:number_of_features]\n",
    "\n",
    "forest_importances = pd.Series(feature_importances, index=x_resampled.columns[:number_of_features])\n",
    "forest_importances = forest_importances.sort_values()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figheight(8)\n",
    "fig.set_figwidth(12)\n",
    "forest_importances.plot.bar(yerr=standard_deviation_importances, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dominik Hahn\\anaconda3\\envs\\check_gpu_neu\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CPW.Name</th>\n",
       "      <th>CPW.ANODE_FZLENGTH</th>\n",
       "      <th>CPW.ANODE_LENGTH</th>\n",
       "      <th>CPW.CATHODE_FZLENGTH_UPPER</th>\n",
       "      <th>CPW.CATHODE_LENGTH</th>\n",
       "      <th>CPW.DIAMTER_MAX</th>\n",
       "      <th>CPW.DIAMTER_MIN</th>\n",
       "      <th>CPW.DIAMTER_AVERAGE</th>\n",
       "      <th>CPW.ANODETABPOSITION</th>\n",
       "      <th>CPW.CATHODETABPOSITION</th>\n",
       "      <th>...</th>\n",
       "      <th>CPX.AI_ACOHs030</th>\n",
       "      <th>CPX.AI_ACOHs031</th>\n",
       "      <th>CPX.AI_ACOHs032</th>\n",
       "      <th>CPX.AI_ACOHs033</th>\n",
       "      <th>CPX.AI_ACOHs034</th>\n",
       "      <th>CPX.AI_ACOHs035</th>\n",
       "      <th>CPX.AI_ACOHs036</th>\n",
       "      <th>CPX.AI_ACOHs037</th>\n",
       "      <th>CPX.AI_ACOHs038</th>\n",
       "      <th>CPX.AI_ACOHs039</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.288943</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>0.763647</td>\n",
       "      <td>0.006635</td>\n",
       "      <td>0.974757</td>\n",
       "      <td>0.982987</td>\n",
       "      <td>0.984151</td>\n",
       "      <td>0.110626</td>\n",
       "      <td>0.952026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.939973</td>\n",
       "      <td>0.475328</td>\n",
       "      <td>0.423790</td>\n",
       "      <td>0.466112</td>\n",
       "      <td>0.924119</td>\n",
       "      <td>0.449965</td>\n",
       "      <td>0.927866</td>\n",
       "      <td>0.450437</td>\n",
       "      <td>0.466692</td>\n",
       "      <td>0.812415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.286520</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.793200</td>\n",
       "      <td>0.006651</td>\n",
       "      <td>0.979029</td>\n",
       "      <td>0.982788</td>\n",
       "      <td>0.984250</td>\n",
       "      <td>0.719249</td>\n",
       "      <td>0.555135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.932120</td>\n",
       "      <td>0.485863</td>\n",
       "      <td>0.472508</td>\n",
       "      <td>0.428385</td>\n",
       "      <td>0.925593</td>\n",
       "      <td>0.422582</td>\n",
       "      <td>0.924251</td>\n",
       "      <td>0.409766</td>\n",
       "      <td>0.462595</td>\n",
       "      <td>0.810901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.287636</td>\n",
       "      <td>0.003203</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.006657</td>\n",
       "      <td>0.976893</td>\n",
       "      <td>0.981196</td>\n",
       "      <td>0.984844</td>\n",
       "      <td>0.274082</td>\n",
       "      <td>0.046617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.944243</td>\n",
       "      <td>0.537294</td>\n",
       "      <td>0.476261</td>\n",
       "      <td>0.492599</td>\n",
       "      <td>0.927413</td>\n",
       "      <td>0.445451</td>\n",
       "      <td>0.931586</td>\n",
       "      <td>0.419254</td>\n",
       "      <td>0.421558</td>\n",
       "      <td>0.808114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.287253</td>\n",
       "      <td>0.003256</td>\n",
       "      <td>0.916610</td>\n",
       "      <td>0.006683</td>\n",
       "      <td>0.978738</td>\n",
       "      <td>0.984579</td>\n",
       "      <td>0.987023</td>\n",
       "      <td>0.992854</td>\n",
       "      <td>0.810332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.958635</td>\n",
       "      <td>0.635140</td>\n",
       "      <td>0.580432</td>\n",
       "      <td>0.519463</td>\n",
       "      <td>0.927002</td>\n",
       "      <td>0.358454</td>\n",
       "      <td>0.903423</td>\n",
       "      <td>0.246302</td>\n",
       "      <td>0.269907</td>\n",
       "      <td>0.790576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.291270</td>\n",
       "      <td>0.003234</td>\n",
       "      <td>0.709362</td>\n",
       "      <td>0.006690</td>\n",
       "      <td>0.984369</td>\n",
       "      <td>0.982589</td>\n",
       "      <td>0.986330</td>\n",
       "      <td>0.756578</td>\n",
       "      <td>0.632159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.937469</td>\n",
       "      <td>0.448738</td>\n",
       "      <td>0.482024</td>\n",
       "      <td>0.467893</td>\n",
       "      <td>0.923222</td>\n",
       "      <td>0.464308</td>\n",
       "      <td>0.920379</td>\n",
       "      <td>0.465195</td>\n",
       "      <td>0.476710</td>\n",
       "      <td>0.811268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CPW.Name  CPW.ANODE_FZLENGTH  CPW.ANODE_LENGTH  CPW.CATHODE_FZLENGTH_UPPER  \\\n",
       "0  0.192308            0.288943          0.003238                    0.763647   \n",
       "1  0.807692            0.286520          0.003200                    0.793200   \n",
       "2  0.076923            0.287636          0.003203                    0.781832   \n",
       "3  0.230769            0.287253          0.003256                    0.916610   \n",
       "4  0.038462            0.291270          0.003234                    0.709362   \n",
       "\n",
       "   CPW.CATHODE_LENGTH  CPW.DIAMTER_MAX  CPW.DIAMTER_MIN  CPW.DIAMTER_AVERAGE  \\\n",
       "0            0.006635         0.974757         0.982987             0.984151   \n",
       "1            0.006651         0.979029         0.982788             0.984250   \n",
       "2            0.006657         0.976893         0.981196             0.984844   \n",
       "3            0.006683         0.978738         0.984579             0.987023   \n",
       "4            0.006690         0.984369         0.982589             0.986330   \n",
       "\n",
       "   CPW.ANODETABPOSITION  CPW.CATHODETABPOSITION  ...  CPX.AI_ACOHs030  \\\n",
       "0              0.110626                0.952026  ...         0.939973   \n",
       "1              0.719249                0.555135  ...         0.932120   \n",
       "2              0.274082                0.046617  ...         0.944243   \n",
       "3              0.992854                0.810332  ...         0.958635   \n",
       "4              0.756578                0.632159  ...         0.937469   \n",
       "\n",
       "   CPX.AI_ACOHs031  CPX.AI_ACOHs032  CPX.AI_ACOHs033  CPX.AI_ACOHs034  \\\n",
       "0         0.475328         0.423790         0.466112         0.924119   \n",
       "1         0.485863         0.472508         0.428385         0.925593   \n",
       "2         0.537294         0.476261         0.492599         0.927413   \n",
       "3         0.635140         0.580432         0.519463         0.927002   \n",
       "4         0.448738         0.482024         0.467893         0.923222   \n",
       "\n",
       "   CPX.AI_ACOHs035  CPX.AI_ACOHs036  CPX.AI_ACOHs037  CPX.AI_ACOHs038  \\\n",
       "0         0.449965         0.927866         0.450437         0.466692   \n",
       "1         0.422582         0.924251         0.409766         0.462595   \n",
       "2         0.445451         0.931586         0.419254         0.421558   \n",
       "3         0.358454         0.903423         0.246302         0.269907   \n",
       "4         0.464308         0.920379         0.465195         0.476710   \n",
       "\n",
       "   CPX.AI_ACOHs039  \n",
       "0         0.812415  \n",
       "1         0.810901  \n",
       "2         0.808114  \n",
       "3         0.790576  \n",
       "4         0.811268  \n",
       "\n",
       "[5 rows x 110 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = SelectFromModel(estimator=random_forest, prefit=True)\n",
    "x_data_selected = selector.transform(x_resampled)\n",
    "best_features = selector.get_feature_names_out(x_resampled.columns)\n",
    "x_data_selected = pd.DataFrame(x_data_selected[:,:number_of_features], columns=best_features[:number_of_features])\n",
    "x_data_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ---- Validation for Decision Tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:32<03:13, 32.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ---- Validation for Random Forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [04:19<12:15, 147.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ---- Validation for Extremely Randomized Trees\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [05:02<06:36, 99.23s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ---- Validation for Ada Boost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [06:54<05:13, 104.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ---- Validation for Gradient Boosting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [16:50<09:23, 281.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ---- Validation for Support Vector Machine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6/7 [28:59<07:13, 433.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ---- Validation for Naive Bayes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [28:59<00:00, 248.56s/it]\n"
     ]
    }
   ],
   "source": [
    "all_cv_scores_selected = []\n",
    "\n",
    "# oversampler = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "# Important to use the selected features in here to see if the feature selection has a benefit onto the different classifiers\n",
    "# x_over_selected, y_over_selected = oversampler.fit_resample(X=x_data_selected, y=le.fit_transform(y))\n",
    "\n",
    "for classifier_name, classifier in tqdm(all_models()):\n",
    "    print(f\"[INFO] ---- Validation for {classifier_name}\")\n",
    "    formatted_result_selected = {}\n",
    "    formatted_result_selected['Classifier'] = classifier_name\n",
    "\n",
    "    cv_score = cross_validate(estimator=classifier, X=x_data_selected, y=y_resampled, scoring=scorings)\n",
    "\n",
    "    for score_name, scores in cv_score.items():\n",
    "        formatted_result_selected[f'{score_name}_mean'] = np.mean(scores)\n",
    "        formatted_result_selected[f'{score_name}_std'] = np.std(scores)\n",
    "\n",
    "    all_cv_scores_selected.append(formatted_result_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>fit_time_mean</th>\n",
       "      <th>fit_time_std</th>\n",
       "      <th>score_time_mean</th>\n",
       "      <th>score_time_std</th>\n",
       "      <th>test_accuracy_mean</th>\n",
       "      <th>test_accuracy_std</th>\n",
       "      <th>test_balanced_accuracy_mean</th>\n",
       "      <th>test_balanced_accuracy_std</th>\n",
       "      <th>test_f1_score_mean</th>\n",
       "      <th>test_f1_score_std</th>\n",
       "      <th>test_mcc_mean</th>\n",
       "      <th>test_mcc_std</th>\n",
       "      <th>test_precision_mean</th>\n",
       "      <th>test_precision_std</th>\n",
       "      <th>test_recall_mean</th>\n",
       "      <th>test_recall_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>6.434544</td>\n",
       "      <td>0.122072</td>\n",
       "      <td>0.016204</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>0.641826</td>\n",
       "      <td>0.005097</td>\n",
       "      <td>0.641826</td>\n",
       "      <td>0.005097</td>\n",
       "      <td>0.640808</td>\n",
       "      <td>0.004694</td>\n",
       "      <td>0.283664</td>\n",
       "      <td>0.010206</td>\n",
       "      <td>0.642660</td>\n",
       "      <td>0.005668</td>\n",
       "      <td>0.638981</td>\n",
       "      <td>0.004634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>45.320775</td>\n",
       "      <td>0.466139</td>\n",
       "      <td>0.185041</td>\n",
       "      <td>0.001789</td>\n",
       "      <td>0.734342</td>\n",
       "      <td>0.004012</td>\n",
       "      <td>0.734342</td>\n",
       "      <td>0.004013</td>\n",
       "      <td>0.727189</td>\n",
       "      <td>0.003203</td>\n",
       "      <td>0.469368</td>\n",
       "      <td>0.008214</td>\n",
       "      <td>0.747399</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.708095</td>\n",
       "      <td>0.004208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Extremely Randomized Trees</td>\n",
       "      <td>8.185634</td>\n",
       "      <td>0.145609</td>\n",
       "      <td>0.249656</td>\n",
       "      <td>0.009481</td>\n",
       "      <td>0.727371</td>\n",
       "      <td>0.006788</td>\n",
       "      <td>0.727371</td>\n",
       "      <td>0.006788</td>\n",
       "      <td>0.722970</td>\n",
       "      <td>0.006874</td>\n",
       "      <td>0.454974</td>\n",
       "      <td>0.013585</td>\n",
       "      <td>0.734836</td>\n",
       "      <td>0.007136</td>\n",
       "      <td>0.711486</td>\n",
       "      <td>0.006880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ada Boost</td>\n",
       "      <td>22.329003</td>\n",
       "      <td>0.061123</td>\n",
       "      <td>0.122028</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.708189</td>\n",
       "      <td>0.003721</td>\n",
       "      <td>0.708189</td>\n",
       "      <td>0.003721</td>\n",
       "      <td>0.701609</td>\n",
       "      <td>0.004055</td>\n",
       "      <td>0.416805</td>\n",
       "      <td>0.007443</td>\n",
       "      <td>0.717811</td>\n",
       "      <td>0.004671</td>\n",
       "      <td>0.686162</td>\n",
       "      <td>0.006226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>119.265573</td>\n",
       "      <td>0.419691</td>\n",
       "      <td>0.027406</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.720531</td>\n",
       "      <td>0.004768</td>\n",
       "      <td>0.720531</td>\n",
       "      <td>0.004768</td>\n",
       "      <td>0.708322</td>\n",
       "      <td>0.005867</td>\n",
       "      <td>0.442622</td>\n",
       "      <td>0.009372</td>\n",
       "      <td>0.740636</td>\n",
       "      <td>0.004232</td>\n",
       "      <td>0.678738</td>\n",
       "      <td>0.008086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>117.463291</td>\n",
       "      <td>0.526711</td>\n",
       "      <td>28.259419</td>\n",
       "      <td>0.256585</td>\n",
       "      <td>0.688367</td>\n",
       "      <td>0.005343</td>\n",
       "      <td>0.688367</td>\n",
       "      <td>0.005342</td>\n",
       "      <td>0.665363</td>\n",
       "      <td>0.005926</td>\n",
       "      <td>0.380369</td>\n",
       "      <td>0.010860</td>\n",
       "      <td>0.718421</td>\n",
       "      <td>0.007025</td>\n",
       "      <td>0.619649</td>\n",
       "      <td>0.007309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.077418</td>\n",
       "      <td>0.005608</td>\n",
       "      <td>0.025605</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.635231</td>\n",
       "      <td>0.016385</td>\n",
       "      <td>0.635229</td>\n",
       "      <td>0.016394</td>\n",
       "      <td>0.549603</td>\n",
       "      <td>0.041976</td>\n",
       "      <td>0.291906</td>\n",
       "      <td>0.025577</td>\n",
       "      <td>0.715879</td>\n",
       "      <td>0.005786</td>\n",
       "      <td>0.448859</td>\n",
       "      <td>0.056537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Classifier  fit_time_mean  fit_time_std  score_time_mean  \\\n",
       "0               Decision Tree       6.434544      0.122072         0.016204   \n",
       "1               Random Forest      45.320775      0.466139         0.185041   \n",
       "2  Extremely Randomized Trees       8.185634      0.145609         0.249656   \n",
       "3                   Ada Boost      22.329003      0.061123         0.122028   \n",
       "4           Gradient Boosting     119.265573      0.419691         0.027406   \n",
       "5      Support Vector Machine     117.463291      0.526711        28.259419   \n",
       "6                 Naive Bayes       0.077418      0.005608         0.025605   \n",
       "\n",
       "   score_time_std  test_accuracy_mean  test_accuracy_std  \\\n",
       "0        0.001470            0.641826           0.005097   \n",
       "1        0.001789            0.734342           0.004012   \n",
       "2        0.009481            0.727371           0.006788   \n",
       "3        0.002281            0.708189           0.003721   \n",
       "4        0.000800            0.720531           0.004768   \n",
       "5        0.256585            0.688367           0.005343   \n",
       "6        0.000490            0.635231           0.016385   \n",
       "\n",
       "   test_balanced_accuracy_mean  test_balanced_accuracy_std  \\\n",
       "0                     0.641826                    0.005097   \n",
       "1                     0.734342                    0.004013   \n",
       "2                     0.727371                    0.006788   \n",
       "3                     0.708189                    0.003721   \n",
       "4                     0.720531                    0.004768   \n",
       "5                     0.688367                    0.005342   \n",
       "6                     0.635229                    0.016394   \n",
       "\n",
       "   test_f1_score_mean  test_f1_score_std  test_mcc_mean  test_mcc_std  \\\n",
       "0            0.640808           0.004694       0.283664      0.010206   \n",
       "1            0.727189           0.003203       0.469368      0.008214   \n",
       "2            0.722970           0.006874       0.454974      0.013585   \n",
       "3            0.701609           0.004055       0.416805      0.007443   \n",
       "4            0.708322           0.005867       0.442622      0.009372   \n",
       "5            0.665363           0.005926       0.380369      0.010860   \n",
       "6            0.549603           0.041976       0.291906      0.025577   \n",
       "\n",
       "   test_precision_mean  test_precision_std  test_recall_mean  test_recall_std  \n",
       "0             0.642660            0.005668          0.638981         0.004634  \n",
       "1             0.747399            0.006696          0.708095         0.004208  \n",
       "2             0.734836            0.007136          0.711486         0.006880  \n",
       "3             0.717811            0.004671          0.686162         0.006226  \n",
       "4             0.740636            0.004232          0.678738         0.008086  \n",
       "5             0.718421            0.007025          0.619649         0.007309  \n",
       "6             0.715879            0.005786          0.448859         0.056537  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_cv_scores_selected)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Accuracy Mean: Random Forest 73,62%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparable Results as in the previous check"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 19938, number of negative: 19866\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008596 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26223\n",
      "[LightGBM] [Info] Number of data points in the train set: 39804, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500904 -> initscore=0.003618\n",
      "[LightGBM] [Info] Start training from score 0.003618\n",
      "Accuracy: 0.7255049743744347\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data_selected, y_resampled, random_state=RANDOM_STATE)\n",
    "train_data = lgb.Dataset(x_train, label=y_train)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.215,\n",
    "    'feature_fraction': 0.9\n",
    "}\n",
    "\n",
    "model = lgb.train(params, train_data, num_boost_round=100)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_binary = [round(p) for p in y_pred]\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.7222391702257474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1c470099160>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_binary)\n",
    "cm_display = ConfusionMatrixDisplay(cm, display_labels=le.classes_)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Calculating precision, recall, and F1-score\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"F1-score:\", f1_score)\n",
    "cm_display.plot(cmap=\"Reds\", xticks_rotation=90)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6729842\ttotal: 41.2ms\tremaining: 41.2s\n",
      "1:\tlearn: 0.6571437\ttotal: 68.1ms\tremaining: 34s\n",
      "2:\tlearn: 0.6438313\ttotal: 93.2ms\tremaining: 31s\n",
      "3:\tlearn: 0.6337558\ttotal: 120ms\tremaining: 30s\n",
      "4:\tlearn: 0.6249511\ttotal: 146ms\tremaining: 29s\n",
      "5:\tlearn: 0.6178185\ttotal: 171ms\tremaining: 28.4s\n",
      "6:\tlearn: 0.6121943\ttotal: 196ms\tremaining: 27.9s\n",
      "7:\tlearn: 0.6072110\ttotal: 222ms\tremaining: 27.5s\n",
      "8:\tlearn: 0.6028912\ttotal: 246ms\tremaining: 27.1s\n",
      "9:\tlearn: 0.5984926\ttotal: 271ms\tremaining: 26.9s\n",
      "10:\tlearn: 0.5947815\ttotal: 296ms\tremaining: 26.6s\n",
      "11:\tlearn: 0.5914638\ttotal: 323ms\tremaining: 26.6s\n",
      "12:\tlearn: 0.5890773\ttotal: 347ms\tremaining: 26.4s\n",
      "13:\tlearn: 0.5868530\ttotal: 371ms\tremaining: 26.1s\n",
      "14:\tlearn: 0.5851552\ttotal: 394ms\tremaining: 25.9s\n",
      "15:\tlearn: 0.5833957\ttotal: 419ms\tremaining: 25.7s\n",
      "16:\tlearn: 0.5813380\ttotal: 442ms\tremaining: 25.6s\n",
      "17:\tlearn: 0.5800429\ttotal: 466ms\tremaining: 25.4s\n",
      "18:\tlearn: 0.5785552\ttotal: 490ms\tremaining: 25.3s\n",
      "19:\tlearn: 0.5768100\ttotal: 515ms\tremaining: 25.2s\n",
      "20:\tlearn: 0.5756284\ttotal: 539ms\tremaining: 25.1s\n",
      "21:\tlearn: 0.5740461\ttotal: 563ms\tremaining: 25s\n",
      "22:\tlearn: 0.5728065\ttotal: 586ms\tremaining: 24.9s\n",
      "23:\tlearn: 0.5715965\ttotal: 610ms\tremaining: 24.8s\n",
      "24:\tlearn: 0.5705136\ttotal: 636ms\tremaining: 24.8s\n",
      "25:\tlearn: 0.5696104\ttotal: 660ms\tremaining: 24.7s\n",
      "26:\tlearn: 0.5687276\ttotal: 684ms\tremaining: 24.7s\n",
      "27:\tlearn: 0.5676611\ttotal: 710ms\tremaining: 24.6s\n",
      "28:\tlearn: 0.5666896\ttotal: 737ms\tremaining: 24.7s\n",
      "29:\tlearn: 0.5659780\ttotal: 762ms\tremaining: 24.7s\n",
      "30:\tlearn: 0.5652647\ttotal: 789ms\tremaining: 24.7s\n",
      "31:\tlearn: 0.5642512\ttotal: 815ms\tremaining: 24.7s\n",
      "32:\tlearn: 0.5635255\ttotal: 841ms\tremaining: 24.6s\n",
      "33:\tlearn: 0.5629072\ttotal: 866ms\tremaining: 24.6s\n",
      "34:\tlearn: 0.5622413\ttotal: 892ms\tremaining: 24.6s\n",
      "35:\tlearn: 0.5612045\ttotal: 920ms\tremaining: 24.6s\n",
      "36:\tlearn: 0.5604882\ttotal: 949ms\tremaining: 24.7s\n",
      "37:\tlearn: 0.5600007\ttotal: 975ms\tremaining: 24.7s\n",
      "38:\tlearn: 0.5594501\ttotal: 1s\tremaining: 24.8s\n",
      "39:\tlearn: 0.5589336\ttotal: 1.03s\tremaining: 24.7s\n",
      "40:\tlearn: 0.5583860\ttotal: 1.05s\tremaining: 24.7s\n",
      "41:\tlearn: 0.5577989\ttotal: 1.08s\tremaining: 24.6s\n",
      "42:\tlearn: 0.5573629\ttotal: 1.1s\tremaining: 24.6s\n",
      "43:\tlearn: 0.5565781\ttotal: 1.13s\tremaining: 24.5s\n",
      "44:\tlearn: 0.5561443\ttotal: 1.15s\tremaining: 24.5s\n",
      "45:\tlearn: 0.5557743\ttotal: 1.18s\tremaining: 24.4s\n",
      "46:\tlearn: 0.5551389\ttotal: 1.2s\tremaining: 24.3s\n",
      "47:\tlearn: 0.5548153\ttotal: 1.22s\tremaining: 24.3s\n",
      "48:\tlearn: 0.5543197\ttotal: 1.25s\tremaining: 24.2s\n",
      "49:\tlearn: 0.5538497\ttotal: 1.27s\tremaining: 24.2s\n",
      "50:\tlearn: 0.5533700\ttotal: 1.3s\tremaining: 24.1s\n",
      "51:\tlearn: 0.5528605\ttotal: 1.32s\tremaining: 24.1s\n",
      "52:\tlearn: 0.5524862\ttotal: 1.34s\tremaining: 24s\n",
      "53:\tlearn: 0.5520976\ttotal: 1.37s\tremaining: 24s\n",
      "54:\tlearn: 0.5517585\ttotal: 1.39s\tremaining: 24s\n",
      "55:\tlearn: 0.5514173\ttotal: 1.42s\tremaining: 23.9s\n",
      "56:\tlearn: 0.5509974\ttotal: 1.44s\tremaining: 23.9s\n",
      "57:\tlearn: 0.5503884\ttotal: 1.47s\tremaining: 23.8s\n",
      "58:\tlearn: 0.5498598\ttotal: 1.49s\tremaining: 23.8s\n",
      "59:\tlearn: 0.5494485\ttotal: 1.52s\tremaining: 23.8s\n",
      "60:\tlearn: 0.5491284\ttotal: 1.54s\tremaining: 23.7s\n",
      "61:\tlearn: 0.5485360\ttotal: 1.56s\tremaining: 23.7s\n",
      "62:\tlearn: 0.5480704\ttotal: 1.59s\tremaining: 23.6s\n",
      "63:\tlearn: 0.5476976\ttotal: 1.61s\tremaining: 23.6s\n",
      "64:\tlearn: 0.5472475\ttotal: 1.64s\tremaining: 23.6s\n",
      "65:\tlearn: 0.5468611\ttotal: 1.66s\tremaining: 23.6s\n",
      "66:\tlearn: 0.5465564\ttotal: 1.69s\tremaining: 23.6s\n",
      "67:\tlearn: 0.5462025\ttotal: 1.72s\tremaining: 23.6s\n",
      "68:\tlearn: 0.5458889\ttotal: 1.77s\tremaining: 23.8s\n",
      "69:\tlearn: 0.5455282\ttotal: 1.79s\tremaining: 23.8s\n",
      "70:\tlearn: 0.5451072\ttotal: 1.82s\tremaining: 23.8s\n",
      "71:\tlearn: 0.5448663\ttotal: 1.84s\tremaining: 23.7s\n",
      "72:\tlearn: 0.5443358\ttotal: 1.86s\tremaining: 23.7s\n",
      "73:\tlearn: 0.5439486\ttotal: 1.89s\tremaining: 23.6s\n",
      "74:\tlearn: 0.5434710\ttotal: 1.91s\tremaining: 23.6s\n",
      "75:\tlearn: 0.5430291\ttotal: 1.94s\tremaining: 23.6s\n",
      "76:\tlearn: 0.5427374\ttotal: 1.96s\tremaining: 23.5s\n",
      "77:\tlearn: 0.5422275\ttotal: 1.99s\tremaining: 23.5s\n",
      "78:\tlearn: 0.5417468\ttotal: 2.01s\tremaining: 23.5s\n",
      "79:\tlearn: 0.5413634\ttotal: 2.04s\tremaining: 23.4s\n",
      "80:\tlearn: 0.5410216\ttotal: 2.06s\tremaining: 23.4s\n",
      "81:\tlearn: 0.5407388\ttotal: 2.08s\tremaining: 23.3s\n",
      "82:\tlearn: 0.5403875\ttotal: 2.11s\tremaining: 23.3s\n",
      "83:\tlearn: 0.5400468\ttotal: 2.13s\tremaining: 23.3s\n",
      "84:\tlearn: 0.5396043\ttotal: 2.16s\tremaining: 23.2s\n",
      "85:\tlearn: 0.5392049\ttotal: 2.18s\tremaining: 23.2s\n",
      "86:\tlearn: 0.5387180\ttotal: 2.2s\tremaining: 23.1s\n",
      "87:\tlearn: 0.5382883\ttotal: 2.23s\tremaining: 23.1s\n",
      "88:\tlearn: 0.5378901\ttotal: 2.25s\tremaining: 23.1s\n",
      "89:\tlearn: 0.5374892\ttotal: 2.28s\tremaining: 23s\n",
      "90:\tlearn: 0.5371722\ttotal: 2.3s\tremaining: 23s\n",
      "91:\tlearn: 0.5367595\ttotal: 2.33s\tremaining: 23s\n",
      "92:\tlearn: 0.5364145\ttotal: 2.35s\tremaining: 22.9s\n",
      "93:\tlearn: 0.5361118\ttotal: 2.37s\tremaining: 22.9s\n",
      "94:\tlearn: 0.5358153\ttotal: 2.4s\tremaining: 22.8s\n",
      "95:\tlearn: 0.5354410\ttotal: 2.42s\tremaining: 22.8s\n",
      "96:\tlearn: 0.5351046\ttotal: 2.44s\tremaining: 22.8s\n",
      "97:\tlearn: 0.5347705\ttotal: 2.47s\tremaining: 22.7s\n",
      "98:\tlearn: 0.5345494\ttotal: 2.49s\tremaining: 22.7s\n",
      "99:\tlearn: 0.5341307\ttotal: 2.51s\tremaining: 22.6s\n",
      "100:\tlearn: 0.5337746\ttotal: 2.54s\tremaining: 22.6s\n",
      "101:\tlearn: 0.5333335\ttotal: 2.56s\tremaining: 22.6s\n",
      "102:\tlearn: 0.5329871\ttotal: 2.59s\tremaining: 22.5s\n",
      "103:\tlearn: 0.5326452\ttotal: 2.61s\tremaining: 22.5s\n",
      "104:\tlearn: 0.5321794\ttotal: 2.63s\tremaining: 22.4s\n",
      "105:\tlearn: 0.5317845\ttotal: 2.66s\tremaining: 22.4s\n",
      "106:\tlearn: 0.5314146\ttotal: 2.68s\tremaining: 22.4s\n",
      "107:\tlearn: 0.5309600\ttotal: 2.71s\tremaining: 22.4s\n",
      "108:\tlearn: 0.5305366\ttotal: 2.73s\tremaining: 22.3s\n",
      "109:\tlearn: 0.5301354\ttotal: 2.76s\tremaining: 22.3s\n",
      "110:\tlearn: 0.5297746\ttotal: 2.78s\tremaining: 22.3s\n",
      "111:\tlearn: 0.5295965\ttotal: 2.81s\tremaining: 22.3s\n",
      "112:\tlearn: 0.5293056\ttotal: 2.83s\tremaining: 22.3s\n",
      "113:\tlearn: 0.5289135\ttotal: 2.86s\tremaining: 22.2s\n",
      "114:\tlearn: 0.5284643\ttotal: 2.89s\tremaining: 22.2s\n",
      "115:\tlearn: 0.5280938\ttotal: 2.92s\tremaining: 22.2s\n",
      "116:\tlearn: 0.5276524\ttotal: 2.94s\tremaining: 22.2s\n",
      "117:\tlearn: 0.5270686\ttotal: 2.97s\tremaining: 22.2s\n",
      "118:\tlearn: 0.5265472\ttotal: 2.99s\tremaining: 22.2s\n",
      "119:\tlearn: 0.5260129\ttotal: 3.03s\tremaining: 22.2s\n",
      "120:\tlearn: 0.5256567\ttotal: 3.05s\tremaining: 22.2s\n",
      "121:\tlearn: 0.5252709\ttotal: 3.08s\tremaining: 22.1s\n",
      "122:\tlearn: 0.5248553\ttotal: 3.1s\tremaining: 22.1s\n",
      "123:\tlearn: 0.5245597\ttotal: 3.13s\tremaining: 22.1s\n",
      "124:\tlearn: 0.5240668\ttotal: 3.15s\tremaining: 22s\n",
      "125:\tlearn: 0.5237000\ttotal: 3.17s\tremaining: 22s\n",
      "126:\tlearn: 0.5233902\ttotal: 3.2s\tremaining: 22s\n",
      "127:\tlearn: 0.5228149\ttotal: 3.22s\tremaining: 21.9s\n",
      "128:\tlearn: 0.5222179\ttotal: 3.25s\tremaining: 21.9s\n",
      "129:\tlearn: 0.5217915\ttotal: 3.27s\tremaining: 21.9s\n",
      "130:\tlearn: 0.5213263\ttotal: 3.29s\tremaining: 21.9s\n",
      "131:\tlearn: 0.5208135\ttotal: 3.32s\tremaining: 21.8s\n",
      "132:\tlearn: 0.5205562\ttotal: 3.34s\tremaining: 21.8s\n",
      "133:\tlearn: 0.5202093\ttotal: 3.37s\tremaining: 21.8s\n",
      "134:\tlearn: 0.5197754\ttotal: 3.39s\tremaining: 21.7s\n",
      "135:\tlearn: 0.5193249\ttotal: 3.42s\tremaining: 21.7s\n",
      "136:\tlearn: 0.5190477\ttotal: 3.44s\tremaining: 21.7s\n",
      "137:\tlearn: 0.5185080\ttotal: 3.46s\tremaining: 21.6s\n",
      "138:\tlearn: 0.5182195\ttotal: 3.49s\tremaining: 21.6s\n",
      "139:\tlearn: 0.5177793\ttotal: 3.51s\tremaining: 21.6s\n",
      "140:\tlearn: 0.5173100\ttotal: 3.53s\tremaining: 21.5s\n",
      "141:\tlearn: 0.5167730\ttotal: 3.56s\tremaining: 21.5s\n",
      "142:\tlearn: 0.5163508\ttotal: 3.58s\tremaining: 21.5s\n",
      "143:\tlearn: 0.5160613\ttotal: 3.6s\tremaining: 21.4s\n",
      "144:\tlearn: 0.5156057\ttotal: 3.62s\tremaining: 21.4s\n",
      "145:\tlearn: 0.5151951\ttotal: 3.65s\tremaining: 21.3s\n",
      "146:\tlearn: 0.5147487\ttotal: 3.67s\tremaining: 21.3s\n",
      "147:\tlearn: 0.5144652\ttotal: 3.69s\tremaining: 21.3s\n",
      "148:\tlearn: 0.5141003\ttotal: 3.71s\tremaining: 21.2s\n",
      "149:\tlearn: 0.5137126\ttotal: 3.74s\tremaining: 21.2s\n",
      "150:\tlearn: 0.5133299\ttotal: 3.76s\tremaining: 21.1s\n",
      "151:\tlearn: 0.5129865\ttotal: 3.78s\tremaining: 21.1s\n",
      "152:\tlearn: 0.5125268\ttotal: 3.81s\tremaining: 21.1s\n",
      "153:\tlearn: 0.5122148\ttotal: 3.83s\tremaining: 21s\n",
      "154:\tlearn: 0.5116969\ttotal: 3.85s\tremaining: 21s\n",
      "155:\tlearn: 0.5113202\ttotal: 3.88s\tremaining: 21s\n",
      "156:\tlearn: 0.5109837\ttotal: 3.9s\tremaining: 20.9s\n",
      "157:\tlearn: 0.5106071\ttotal: 3.93s\tremaining: 20.9s\n",
      "158:\tlearn: 0.5102786\ttotal: 3.95s\tremaining: 20.9s\n",
      "159:\tlearn: 0.5100381\ttotal: 3.97s\tremaining: 20.9s\n",
      "160:\tlearn: 0.5096630\ttotal: 4s\tremaining: 20.8s\n",
      "161:\tlearn: 0.5094095\ttotal: 4.02s\tremaining: 20.8s\n",
      "162:\tlearn: 0.5091184\ttotal: 4.04s\tremaining: 20.8s\n",
      "163:\tlearn: 0.5087762\ttotal: 4.07s\tremaining: 20.7s\n",
      "164:\tlearn: 0.5084191\ttotal: 4.09s\tremaining: 20.7s\n",
      "165:\tlearn: 0.5079757\ttotal: 4.11s\tremaining: 20.7s\n",
      "166:\tlearn: 0.5075723\ttotal: 4.13s\tremaining: 20.6s\n",
      "167:\tlearn: 0.5072894\ttotal: 4.16s\tremaining: 20.6s\n",
      "168:\tlearn: 0.5069579\ttotal: 4.18s\tremaining: 20.5s\n",
      "169:\tlearn: 0.5066715\ttotal: 4.2s\tremaining: 20.5s\n",
      "170:\tlearn: 0.5062964\ttotal: 4.22s\tremaining: 20.5s\n",
      "171:\tlearn: 0.5059875\ttotal: 4.25s\tremaining: 20.4s\n",
      "172:\tlearn: 0.5056329\ttotal: 4.27s\tremaining: 20.4s\n",
      "173:\tlearn: 0.5052943\ttotal: 4.29s\tremaining: 20.4s\n",
      "174:\tlearn: 0.5049497\ttotal: 4.32s\tremaining: 20.3s\n",
      "175:\tlearn: 0.5047104\ttotal: 4.34s\tremaining: 20.3s\n",
      "176:\tlearn: 0.5042733\ttotal: 4.36s\tremaining: 20.3s\n",
      "177:\tlearn: 0.5039080\ttotal: 4.38s\tremaining: 20.2s\n",
      "178:\tlearn: 0.5035504\ttotal: 4.41s\tremaining: 20.2s\n",
      "179:\tlearn: 0.5032980\ttotal: 4.43s\tremaining: 20.2s\n",
      "180:\tlearn: 0.5029455\ttotal: 4.45s\tremaining: 20.1s\n",
      "181:\tlearn: 0.5026541\ttotal: 4.47s\tremaining: 20.1s\n",
      "182:\tlearn: 0.5021997\ttotal: 4.5s\tremaining: 20.1s\n",
      "183:\tlearn: 0.5017956\ttotal: 4.52s\tremaining: 20s\n",
      "184:\tlearn: 0.5015210\ttotal: 4.54s\tremaining: 20s\n",
      "185:\tlearn: 0.5011070\ttotal: 4.56s\tremaining: 20s\n",
      "186:\tlearn: 0.5007386\ttotal: 4.58s\tremaining: 19.9s\n",
      "187:\tlearn: 0.5003736\ttotal: 4.61s\tremaining: 19.9s\n",
      "188:\tlearn: 0.4999941\ttotal: 4.63s\tremaining: 19.9s\n",
      "189:\tlearn: 0.4995995\ttotal: 4.66s\tremaining: 19.8s\n",
      "190:\tlearn: 0.4992662\ttotal: 4.68s\tremaining: 19.8s\n",
      "191:\tlearn: 0.4987925\ttotal: 4.7s\tremaining: 19.8s\n",
      "192:\tlearn: 0.4984039\ttotal: 4.72s\tremaining: 19.8s\n",
      "193:\tlearn: 0.4980209\ttotal: 4.75s\tremaining: 19.7s\n",
      "194:\tlearn: 0.4976705\ttotal: 4.77s\tremaining: 19.7s\n",
      "195:\tlearn: 0.4973545\ttotal: 4.79s\tremaining: 19.7s\n",
      "196:\tlearn: 0.4970634\ttotal: 4.81s\tremaining: 19.6s\n",
      "197:\tlearn: 0.4966773\ttotal: 4.84s\tremaining: 19.6s\n",
      "198:\tlearn: 0.4964385\ttotal: 4.86s\tremaining: 19.6s\n",
      "199:\tlearn: 0.4960619\ttotal: 4.88s\tremaining: 19.5s\n",
      "200:\tlearn: 0.4956682\ttotal: 4.91s\tremaining: 19.5s\n",
      "201:\tlearn: 0.4953809\ttotal: 4.94s\tremaining: 19.5s\n",
      "202:\tlearn: 0.4950850\ttotal: 4.96s\tremaining: 19.5s\n",
      "203:\tlearn: 0.4948194\ttotal: 4.98s\tremaining: 19.4s\n",
      "204:\tlearn: 0.4945079\ttotal: 5.01s\tremaining: 19.4s\n",
      "205:\tlearn: 0.4941560\ttotal: 5.03s\tremaining: 19.4s\n",
      "206:\tlearn: 0.4938889\ttotal: 5.05s\tremaining: 19.4s\n",
      "207:\tlearn: 0.4935440\ttotal: 5.08s\tremaining: 19.3s\n",
      "208:\tlearn: 0.4932149\ttotal: 5.1s\tremaining: 19.3s\n",
      "209:\tlearn: 0.4927802\ttotal: 5.12s\tremaining: 19.3s\n",
      "210:\tlearn: 0.4924587\ttotal: 5.15s\tremaining: 19.2s\n",
      "211:\tlearn: 0.4921012\ttotal: 5.17s\tremaining: 19.2s\n",
      "212:\tlearn: 0.4918777\ttotal: 5.19s\tremaining: 19.2s\n",
      "213:\tlearn: 0.4916177\ttotal: 5.22s\tremaining: 19.2s\n",
      "214:\tlearn: 0.4912874\ttotal: 5.24s\tremaining: 19.1s\n",
      "215:\tlearn: 0.4910800\ttotal: 5.26s\tremaining: 19.1s\n",
      "216:\tlearn: 0.4908110\ttotal: 5.29s\tremaining: 19.1s\n",
      "217:\tlearn: 0.4903716\ttotal: 5.31s\tremaining: 19.1s\n",
      "218:\tlearn: 0.4900232\ttotal: 5.33s\tremaining: 19s\n",
      "219:\tlearn: 0.4897730\ttotal: 5.36s\tremaining: 19s\n",
      "220:\tlearn: 0.4894705\ttotal: 5.38s\tremaining: 19s\n",
      "221:\tlearn: 0.4891185\ttotal: 5.41s\tremaining: 18.9s\n",
      "222:\tlearn: 0.4887977\ttotal: 5.43s\tremaining: 18.9s\n",
      "223:\tlearn: 0.4884548\ttotal: 5.45s\tremaining: 18.9s\n",
      "224:\tlearn: 0.4881554\ttotal: 5.47s\tremaining: 18.9s\n",
      "225:\tlearn: 0.4878092\ttotal: 5.5s\tremaining: 18.8s\n",
      "226:\tlearn: 0.4875548\ttotal: 5.52s\tremaining: 18.8s\n",
      "227:\tlearn: 0.4873220\ttotal: 5.54s\tremaining: 18.8s\n",
      "228:\tlearn: 0.4870034\ttotal: 5.57s\tremaining: 18.7s\n",
      "229:\tlearn: 0.4866658\ttotal: 5.59s\tremaining: 18.7s\n",
      "230:\tlearn: 0.4864012\ttotal: 5.61s\tremaining: 18.7s\n",
      "231:\tlearn: 0.4860980\ttotal: 5.64s\tremaining: 18.7s\n",
      "232:\tlearn: 0.4857959\ttotal: 5.66s\tremaining: 18.6s\n",
      "233:\tlearn: 0.4854988\ttotal: 5.68s\tremaining: 18.6s\n",
      "234:\tlearn: 0.4851876\ttotal: 5.71s\tremaining: 18.6s\n",
      "235:\tlearn: 0.4848816\ttotal: 5.73s\tremaining: 18.6s\n",
      "236:\tlearn: 0.4845189\ttotal: 5.75s\tremaining: 18.5s\n",
      "237:\tlearn: 0.4842222\ttotal: 5.78s\tremaining: 18.5s\n",
      "238:\tlearn: 0.4838841\ttotal: 5.8s\tremaining: 18.5s\n",
      "239:\tlearn: 0.4836174\ttotal: 5.83s\tremaining: 18.4s\n",
      "240:\tlearn: 0.4833665\ttotal: 5.85s\tremaining: 18.4s\n",
      "241:\tlearn: 0.4830691\ttotal: 5.87s\tremaining: 18.4s\n",
      "242:\tlearn: 0.4827108\ttotal: 5.89s\tremaining: 18.4s\n",
      "243:\tlearn: 0.4824272\ttotal: 5.92s\tremaining: 18.3s\n",
      "244:\tlearn: 0.4820587\ttotal: 5.94s\tremaining: 18.3s\n",
      "245:\tlearn: 0.4817716\ttotal: 5.97s\tremaining: 18.3s\n",
      "246:\tlearn: 0.4814555\ttotal: 5.99s\tremaining: 18.3s\n",
      "247:\tlearn: 0.4811536\ttotal: 6.01s\tremaining: 18.2s\n",
      "248:\tlearn: 0.4809506\ttotal: 6.04s\tremaining: 18.2s\n",
      "249:\tlearn: 0.4806424\ttotal: 6.06s\tremaining: 18.2s\n",
      "250:\tlearn: 0.4803392\ttotal: 6.08s\tremaining: 18.1s\n",
      "251:\tlearn: 0.4800386\ttotal: 6.11s\tremaining: 18.1s\n",
      "252:\tlearn: 0.4797585\ttotal: 6.13s\tremaining: 18.1s\n",
      "253:\tlearn: 0.4794515\ttotal: 6.15s\tremaining: 18.1s\n",
      "254:\tlearn: 0.4791977\ttotal: 6.18s\tremaining: 18s\n",
      "255:\tlearn: 0.4789412\ttotal: 6.2s\tremaining: 18s\n",
      "256:\tlearn: 0.4786946\ttotal: 6.22s\tremaining: 18s\n",
      "257:\tlearn: 0.4784570\ttotal: 6.25s\tremaining: 18s\n",
      "258:\tlearn: 0.4781424\ttotal: 6.27s\tremaining: 17.9s\n",
      "259:\tlearn: 0.4778586\ttotal: 6.29s\tremaining: 17.9s\n",
      "260:\tlearn: 0.4775368\ttotal: 6.31s\tremaining: 17.9s\n",
      "261:\tlearn: 0.4772083\ttotal: 6.34s\tremaining: 17.8s\n",
      "262:\tlearn: 0.4768668\ttotal: 6.36s\tremaining: 17.8s\n",
      "263:\tlearn: 0.4765653\ttotal: 6.38s\tremaining: 17.8s\n",
      "264:\tlearn: 0.4763489\ttotal: 6.41s\tremaining: 17.8s\n",
      "265:\tlearn: 0.4760499\ttotal: 6.43s\tremaining: 17.7s\n",
      "266:\tlearn: 0.4757253\ttotal: 6.46s\tremaining: 17.7s\n",
      "267:\tlearn: 0.4754137\ttotal: 6.48s\tremaining: 17.7s\n",
      "268:\tlearn: 0.4751952\ttotal: 6.5s\tremaining: 17.7s\n",
      "269:\tlearn: 0.4749967\ttotal: 6.53s\tremaining: 17.7s\n",
      "270:\tlearn: 0.4747133\ttotal: 6.55s\tremaining: 17.6s\n",
      "271:\tlearn: 0.4744566\ttotal: 6.58s\tremaining: 17.6s\n",
      "272:\tlearn: 0.4742009\ttotal: 6.6s\tremaining: 17.6s\n",
      "273:\tlearn: 0.4739421\ttotal: 6.62s\tremaining: 17.6s\n",
      "274:\tlearn: 0.4736797\ttotal: 6.65s\tremaining: 17.5s\n",
      "275:\tlearn: 0.4734393\ttotal: 6.67s\tremaining: 17.5s\n",
      "276:\tlearn: 0.4731793\ttotal: 6.7s\tremaining: 17.5s\n",
      "277:\tlearn: 0.4728673\ttotal: 6.72s\tremaining: 17.5s\n",
      "278:\tlearn: 0.4725901\ttotal: 6.75s\tremaining: 17.4s\n",
      "279:\tlearn: 0.4722895\ttotal: 6.77s\tremaining: 17.4s\n",
      "280:\tlearn: 0.4720553\ttotal: 6.79s\tremaining: 17.4s\n",
      "281:\tlearn: 0.4717441\ttotal: 6.82s\tremaining: 17.4s\n",
      "282:\tlearn: 0.4714481\ttotal: 6.84s\tremaining: 17.3s\n",
      "283:\tlearn: 0.4711789\ttotal: 6.87s\tremaining: 17.3s\n",
      "284:\tlearn: 0.4709272\ttotal: 6.89s\tremaining: 17.3s\n",
      "285:\tlearn: 0.4706791\ttotal: 6.92s\tremaining: 17.3s\n",
      "286:\tlearn: 0.4703731\ttotal: 6.94s\tremaining: 17.2s\n",
      "287:\tlearn: 0.4700413\ttotal: 6.97s\tremaining: 17.2s\n",
      "288:\tlearn: 0.4698649\ttotal: 6.99s\tremaining: 17.2s\n",
      "289:\tlearn: 0.4695806\ttotal: 7.02s\tremaining: 17.2s\n",
      "290:\tlearn: 0.4692505\ttotal: 7.04s\tremaining: 17.2s\n",
      "291:\tlearn: 0.4689427\ttotal: 7.06s\tremaining: 17.1s\n",
      "292:\tlearn: 0.4685458\ttotal: 7.09s\tremaining: 17.1s\n",
      "293:\tlearn: 0.4682501\ttotal: 7.11s\tremaining: 17.1s\n",
      "294:\tlearn: 0.4679739\ttotal: 7.13s\tremaining: 17s\n",
      "295:\tlearn: 0.4677309\ttotal: 7.16s\tremaining: 17s\n",
      "296:\tlearn: 0.4674248\ttotal: 7.18s\tremaining: 17s\n",
      "297:\tlearn: 0.4671908\ttotal: 7.2s\tremaining: 17s\n",
      "298:\tlearn: 0.4669081\ttotal: 7.23s\tremaining: 16.9s\n",
      "299:\tlearn: 0.4666588\ttotal: 7.25s\tremaining: 16.9s\n",
      "300:\tlearn: 0.4664295\ttotal: 7.27s\tremaining: 16.9s\n",
      "301:\tlearn: 0.4661523\ttotal: 7.29s\tremaining: 16.9s\n",
      "302:\tlearn: 0.4659488\ttotal: 7.32s\tremaining: 16.8s\n",
      "303:\tlearn: 0.4656199\ttotal: 7.34s\tremaining: 16.8s\n",
      "304:\tlearn: 0.4653766\ttotal: 7.36s\tremaining: 16.8s\n",
      "305:\tlearn: 0.4651665\ttotal: 7.38s\tremaining: 16.7s\n",
      "306:\tlearn: 0.4649063\ttotal: 7.41s\tremaining: 16.7s\n",
      "307:\tlearn: 0.4646204\ttotal: 7.43s\tremaining: 16.7s\n",
      "308:\tlearn: 0.4642767\ttotal: 7.45s\tremaining: 16.7s\n",
      "309:\tlearn: 0.4640142\ttotal: 7.47s\tremaining: 16.6s\n",
      "310:\tlearn: 0.4636745\ttotal: 7.5s\tremaining: 16.6s\n",
      "311:\tlearn: 0.4634099\ttotal: 7.52s\tremaining: 16.6s\n",
      "312:\tlearn: 0.4631440\ttotal: 7.54s\tremaining: 16.6s\n",
      "313:\tlearn: 0.4628486\ttotal: 7.57s\tremaining: 16.5s\n",
      "314:\tlearn: 0.4626024\ttotal: 7.59s\tremaining: 16.5s\n",
      "315:\tlearn: 0.4623621\ttotal: 7.61s\tremaining: 16.5s\n",
      "316:\tlearn: 0.4621147\ttotal: 7.63s\tremaining: 16.4s\n",
      "317:\tlearn: 0.4618618\ttotal: 7.66s\tremaining: 16.4s\n",
      "318:\tlearn: 0.4616291\ttotal: 7.68s\tremaining: 16.4s\n",
      "319:\tlearn: 0.4613815\ttotal: 7.7s\tremaining: 16.4s\n",
      "320:\tlearn: 0.4611087\ttotal: 7.73s\tremaining: 16.3s\n",
      "321:\tlearn: 0.4608496\ttotal: 7.75s\tremaining: 16.3s\n",
      "322:\tlearn: 0.4605463\ttotal: 7.77s\tremaining: 16.3s\n",
      "323:\tlearn: 0.4602834\ttotal: 7.8s\tremaining: 16.3s\n",
      "324:\tlearn: 0.4600377\ttotal: 7.83s\tremaining: 16.3s\n",
      "325:\tlearn: 0.4597710\ttotal: 7.85s\tremaining: 16.2s\n",
      "326:\tlearn: 0.4595713\ttotal: 7.88s\tremaining: 16.2s\n",
      "327:\tlearn: 0.4593030\ttotal: 7.91s\tremaining: 16.2s\n",
      "328:\tlearn: 0.4589774\ttotal: 7.93s\tremaining: 16.2s\n",
      "329:\tlearn: 0.4586949\ttotal: 7.95s\tremaining: 16.1s\n",
      "330:\tlearn: 0.4583944\ttotal: 7.98s\tremaining: 16.1s\n",
      "331:\tlearn: 0.4581693\ttotal: 8s\tremaining: 16.1s\n",
      "332:\tlearn: 0.4579487\ttotal: 8.02s\tremaining: 16.1s\n",
      "333:\tlearn: 0.4576768\ttotal: 8.05s\tremaining: 16s\n",
      "334:\tlearn: 0.4573726\ttotal: 8.07s\tremaining: 16s\n",
      "335:\tlearn: 0.4571433\ttotal: 8.1s\tremaining: 16s\n",
      "336:\tlearn: 0.4568746\ttotal: 8.12s\tremaining: 16s\n",
      "337:\tlearn: 0.4566564\ttotal: 8.14s\tremaining: 15.9s\n",
      "338:\tlearn: 0.4564237\ttotal: 8.16s\tremaining: 15.9s\n",
      "339:\tlearn: 0.4561135\ttotal: 8.19s\tremaining: 15.9s\n",
      "340:\tlearn: 0.4558820\ttotal: 8.21s\tremaining: 15.9s\n",
      "341:\tlearn: 0.4556718\ttotal: 8.23s\tremaining: 15.8s\n",
      "342:\tlearn: 0.4554154\ttotal: 8.26s\tremaining: 15.8s\n",
      "343:\tlearn: 0.4551920\ttotal: 8.28s\tremaining: 15.8s\n",
      "344:\tlearn: 0.4549554\ttotal: 8.3s\tremaining: 15.8s\n",
      "345:\tlearn: 0.4547117\ttotal: 8.33s\tremaining: 15.7s\n",
      "346:\tlearn: 0.4544172\ttotal: 8.35s\tremaining: 15.7s\n",
      "347:\tlearn: 0.4541411\ttotal: 8.38s\tremaining: 15.7s\n",
      "348:\tlearn: 0.4538911\ttotal: 8.4s\tremaining: 15.7s\n",
      "349:\tlearn: 0.4536203\ttotal: 8.42s\tremaining: 15.6s\n",
      "350:\tlearn: 0.4533075\ttotal: 8.44s\tremaining: 15.6s\n",
      "351:\tlearn: 0.4529995\ttotal: 8.47s\tremaining: 15.6s\n",
      "352:\tlearn: 0.4527386\ttotal: 8.49s\tremaining: 15.6s\n",
      "353:\tlearn: 0.4525021\ttotal: 8.51s\tremaining: 15.5s\n",
      "354:\tlearn: 0.4522267\ttotal: 8.54s\tremaining: 15.5s\n",
      "355:\tlearn: 0.4520038\ttotal: 8.56s\tremaining: 15.5s\n",
      "356:\tlearn: 0.4517484\ttotal: 8.58s\tremaining: 15.5s\n",
      "357:\tlearn: 0.4514688\ttotal: 8.61s\tremaining: 15.4s\n",
      "358:\tlearn: 0.4512437\ttotal: 8.63s\tremaining: 15.4s\n",
      "359:\tlearn: 0.4510392\ttotal: 8.65s\tremaining: 15.4s\n",
      "360:\tlearn: 0.4507956\ttotal: 8.68s\tremaining: 15.4s\n",
      "361:\tlearn: 0.4505547\ttotal: 8.7s\tremaining: 15.3s\n",
      "362:\tlearn: 0.4503394\ttotal: 8.72s\tremaining: 15.3s\n",
      "363:\tlearn: 0.4500283\ttotal: 8.75s\tremaining: 15.3s\n",
      "364:\tlearn: 0.4497955\ttotal: 8.77s\tremaining: 15.3s\n",
      "365:\tlearn: 0.4496015\ttotal: 8.79s\tremaining: 15.2s\n",
      "366:\tlearn: 0.4494060\ttotal: 8.81s\tremaining: 15.2s\n",
      "367:\tlearn: 0.4491086\ttotal: 8.84s\tremaining: 15.2s\n",
      "368:\tlearn: 0.4488304\ttotal: 8.86s\tremaining: 15.2s\n",
      "369:\tlearn: 0.4486631\ttotal: 8.89s\tremaining: 15.1s\n",
      "370:\tlearn: 0.4484213\ttotal: 8.91s\tremaining: 15.1s\n",
      "371:\tlearn: 0.4482625\ttotal: 8.94s\tremaining: 15.1s\n",
      "372:\tlearn: 0.4480561\ttotal: 8.96s\tremaining: 15.1s\n",
      "373:\tlearn: 0.4477869\ttotal: 8.98s\tremaining: 15s\n",
      "374:\tlearn: 0.4475815\ttotal: 9.01s\tremaining: 15s\n",
      "375:\tlearn: 0.4473470\ttotal: 9.03s\tremaining: 15s\n",
      "376:\tlearn: 0.4470603\ttotal: 9.05s\tremaining: 15s\n",
      "377:\tlearn: 0.4468485\ttotal: 9.07s\tremaining: 14.9s\n",
      "378:\tlearn: 0.4465560\ttotal: 9.1s\tremaining: 14.9s\n",
      "379:\tlearn: 0.4463377\ttotal: 9.12s\tremaining: 14.9s\n",
      "380:\tlearn: 0.4460563\ttotal: 9.14s\tremaining: 14.9s\n",
      "381:\tlearn: 0.4457638\ttotal: 9.17s\tremaining: 14.8s\n",
      "382:\tlearn: 0.4455269\ttotal: 9.19s\tremaining: 14.8s\n",
      "383:\tlearn: 0.4453024\ttotal: 9.21s\tremaining: 14.8s\n",
      "384:\tlearn: 0.4450157\ttotal: 9.24s\tremaining: 14.8s\n",
      "385:\tlearn: 0.4447388\ttotal: 9.26s\tremaining: 14.7s\n",
      "386:\tlearn: 0.4444650\ttotal: 9.28s\tremaining: 14.7s\n",
      "387:\tlearn: 0.4442304\ttotal: 9.3s\tremaining: 14.7s\n",
      "388:\tlearn: 0.4440589\ttotal: 9.33s\tremaining: 14.7s\n",
      "389:\tlearn: 0.4438176\ttotal: 9.35s\tremaining: 14.6s\n",
      "390:\tlearn: 0.4436028\ttotal: 9.37s\tremaining: 14.6s\n",
      "391:\tlearn: 0.4433973\ttotal: 9.39s\tremaining: 14.6s\n",
      "392:\tlearn: 0.4431654\ttotal: 9.42s\tremaining: 14.5s\n",
      "393:\tlearn: 0.4428910\ttotal: 9.44s\tremaining: 14.5s\n",
      "394:\tlearn: 0.4426055\ttotal: 9.46s\tremaining: 14.5s\n",
      "395:\tlearn: 0.4422822\ttotal: 9.49s\tremaining: 14.5s\n",
      "396:\tlearn: 0.4420769\ttotal: 9.51s\tremaining: 14.4s\n",
      "397:\tlearn: 0.4417963\ttotal: 9.53s\tremaining: 14.4s\n",
      "398:\tlearn: 0.4416047\ttotal: 9.56s\tremaining: 14.4s\n",
      "399:\tlearn: 0.4413259\ttotal: 9.58s\tremaining: 14.4s\n",
      "400:\tlearn: 0.4410609\ttotal: 9.61s\tremaining: 14.4s\n",
      "401:\tlearn: 0.4407909\ttotal: 9.64s\tremaining: 14.3s\n",
      "402:\tlearn: 0.4405125\ttotal: 9.66s\tremaining: 14.3s\n",
      "403:\tlearn: 0.4402175\ttotal: 9.69s\tremaining: 14.3s\n",
      "404:\tlearn: 0.4400078\ttotal: 9.71s\tremaining: 14.3s\n",
      "405:\tlearn: 0.4397511\ttotal: 9.74s\tremaining: 14.2s\n",
      "406:\tlearn: 0.4395476\ttotal: 9.76s\tremaining: 14.2s\n",
      "407:\tlearn: 0.4393190\ttotal: 9.79s\tremaining: 14.2s\n",
      "408:\tlearn: 0.4390786\ttotal: 9.81s\tremaining: 14.2s\n",
      "409:\tlearn: 0.4388522\ttotal: 9.84s\tremaining: 14.2s\n",
      "410:\tlearn: 0.4386080\ttotal: 9.87s\tremaining: 14.1s\n",
      "411:\tlearn: 0.4384069\ttotal: 9.9s\tremaining: 14.1s\n",
      "412:\tlearn: 0.4381365\ttotal: 9.93s\tremaining: 14.1s\n",
      "413:\tlearn: 0.4378912\ttotal: 9.96s\tremaining: 14.1s\n",
      "414:\tlearn: 0.4376980\ttotal: 9.98s\tremaining: 14.1s\n",
      "415:\tlearn: 0.4374513\ttotal: 10s\tremaining: 14.1s\n",
      "416:\tlearn: 0.4371323\ttotal: 10s\tremaining: 14s\n",
      "417:\tlearn: 0.4369134\ttotal: 10.1s\tremaining: 14s\n",
      "418:\tlearn: 0.4367228\ttotal: 10.1s\tremaining: 14s\n",
      "419:\tlearn: 0.4364414\ttotal: 10.1s\tremaining: 14s\n",
      "420:\tlearn: 0.4362304\ttotal: 10.1s\tremaining: 13.9s\n",
      "421:\tlearn: 0.4359306\ttotal: 10.2s\tremaining: 13.9s\n",
      "422:\tlearn: 0.4357722\ttotal: 10.2s\tremaining: 13.9s\n",
      "423:\tlearn: 0.4354977\ttotal: 10.2s\tremaining: 13.9s\n",
      "424:\tlearn: 0.4352814\ttotal: 10.2s\tremaining: 13.9s\n",
      "425:\tlearn: 0.4349890\ttotal: 10.3s\tremaining: 13.8s\n",
      "426:\tlearn: 0.4347902\ttotal: 10.3s\tremaining: 13.8s\n",
      "427:\tlearn: 0.4345695\ttotal: 10.3s\tremaining: 13.8s\n",
      "428:\tlearn: 0.4343094\ttotal: 10.3s\tremaining: 13.8s\n",
      "429:\tlearn: 0.4340129\ttotal: 10.4s\tremaining: 13.7s\n",
      "430:\tlearn: 0.4337482\ttotal: 10.4s\tremaining: 13.7s\n",
      "431:\tlearn: 0.4335008\ttotal: 10.4s\tremaining: 13.7s\n",
      "432:\tlearn: 0.4332166\ttotal: 10.4s\tremaining: 13.7s\n",
      "433:\tlearn: 0.4329276\ttotal: 10.5s\tremaining: 13.6s\n",
      "434:\tlearn: 0.4327000\ttotal: 10.5s\tremaining: 13.6s\n",
      "435:\tlearn: 0.4324254\ttotal: 10.5s\tremaining: 13.6s\n",
      "436:\tlearn: 0.4321493\ttotal: 10.5s\tremaining: 13.6s\n",
      "437:\tlearn: 0.4319268\ttotal: 10.6s\tremaining: 13.5s\n",
      "438:\tlearn: 0.4316656\ttotal: 10.6s\tremaining: 13.5s\n",
      "439:\tlearn: 0.4314782\ttotal: 10.6s\tremaining: 13.5s\n",
      "440:\tlearn: 0.4312121\ttotal: 10.6s\tremaining: 13.5s\n",
      "441:\tlearn: 0.4309353\ttotal: 10.7s\tremaining: 13.5s\n",
      "442:\tlearn: 0.4306820\ttotal: 10.7s\tremaining: 13.4s\n",
      "443:\tlearn: 0.4304458\ttotal: 10.7s\tremaining: 13.4s\n",
      "444:\tlearn: 0.4302365\ttotal: 10.7s\tremaining: 13.4s\n",
      "445:\tlearn: 0.4299616\ttotal: 10.8s\tremaining: 13.4s\n",
      "446:\tlearn: 0.4297170\ttotal: 10.8s\tremaining: 13.3s\n",
      "447:\tlearn: 0.4294166\ttotal: 10.8s\tremaining: 13.3s\n",
      "448:\tlearn: 0.4291813\ttotal: 10.8s\tremaining: 13.3s\n",
      "449:\tlearn: 0.4288782\ttotal: 10.9s\tremaining: 13.3s\n",
      "450:\tlearn: 0.4287015\ttotal: 10.9s\tremaining: 13.2s\n",
      "451:\tlearn: 0.4285004\ttotal: 10.9s\tremaining: 13.2s\n",
      "452:\tlearn: 0.4282716\ttotal: 10.9s\tremaining: 13.2s\n",
      "453:\tlearn: 0.4280913\ttotal: 11s\tremaining: 13.2s\n",
      "454:\tlearn: 0.4279179\ttotal: 11s\tremaining: 13.2s\n",
      "455:\tlearn: 0.4276364\ttotal: 11s\tremaining: 13.1s\n",
      "456:\tlearn: 0.4274301\ttotal: 11s\tremaining: 13.1s\n",
      "457:\tlearn: 0.4271070\ttotal: 11.1s\tremaining: 13.1s\n",
      "458:\tlearn: 0.4268108\ttotal: 11.1s\tremaining: 13.1s\n",
      "459:\tlearn: 0.4265717\ttotal: 11.1s\tremaining: 13s\n",
      "460:\tlearn: 0.4263355\ttotal: 11.1s\tremaining: 13s\n",
      "461:\tlearn: 0.4260530\ttotal: 11.1s\tremaining: 13s\n",
      "462:\tlearn: 0.4257754\ttotal: 11.2s\tremaining: 13s\n",
      "463:\tlearn: 0.4256059\ttotal: 11.2s\tremaining: 12.9s\n",
      "464:\tlearn: 0.4253452\ttotal: 11.2s\tremaining: 12.9s\n",
      "465:\tlearn: 0.4251508\ttotal: 11.2s\tremaining: 12.9s\n",
      "466:\tlearn: 0.4248969\ttotal: 11.3s\tremaining: 12.9s\n",
      "467:\tlearn: 0.4246521\ttotal: 11.3s\tremaining: 12.8s\n",
      "468:\tlearn: 0.4244073\ttotal: 11.3s\tremaining: 12.8s\n",
      "469:\tlearn: 0.4241835\ttotal: 11.3s\tremaining: 12.8s\n",
      "470:\tlearn: 0.4239038\ttotal: 11.4s\tremaining: 12.8s\n",
      "471:\tlearn: 0.4236561\ttotal: 11.4s\tremaining: 12.7s\n",
      "472:\tlearn: 0.4234720\ttotal: 11.4s\tremaining: 12.7s\n",
      "473:\tlearn: 0.4232002\ttotal: 11.4s\tremaining: 12.7s\n",
      "474:\tlearn: 0.4229678\ttotal: 11.5s\tremaining: 12.7s\n",
      "475:\tlearn: 0.4227511\ttotal: 11.5s\tremaining: 12.6s\n",
      "476:\tlearn: 0.4226065\ttotal: 11.5s\tremaining: 12.6s\n",
      "477:\tlearn: 0.4223636\ttotal: 11.6s\tremaining: 12.6s\n",
      "478:\tlearn: 0.4220831\ttotal: 11.6s\tremaining: 12.6s\n",
      "479:\tlearn: 0.4218553\ttotal: 11.6s\tremaining: 12.6s\n",
      "480:\tlearn: 0.4216440\ttotal: 11.6s\tremaining: 12.6s\n",
      "481:\tlearn: 0.4214323\ttotal: 11.7s\tremaining: 12.6s\n",
      "482:\tlearn: 0.4212678\ttotal: 11.7s\tremaining: 12.5s\n",
      "483:\tlearn: 0.4210582\ttotal: 11.7s\tremaining: 12.5s\n",
      "484:\tlearn: 0.4208189\ttotal: 11.8s\tremaining: 12.5s\n",
      "485:\tlearn: 0.4206807\ttotal: 11.8s\tremaining: 12.5s\n",
      "486:\tlearn: 0.4205192\ttotal: 11.8s\tremaining: 12.5s\n",
      "487:\tlearn: 0.4202987\ttotal: 11.9s\tremaining: 12.4s\n",
      "488:\tlearn: 0.4201398\ttotal: 11.9s\tremaining: 12.4s\n",
      "489:\tlearn: 0.4199210\ttotal: 11.9s\tremaining: 12.4s\n",
      "490:\tlearn: 0.4197074\ttotal: 12s\tremaining: 12.4s\n",
      "491:\tlearn: 0.4194222\ttotal: 12s\tremaining: 12.4s\n",
      "492:\tlearn: 0.4191653\ttotal: 12s\tremaining: 12.4s\n",
      "493:\tlearn: 0.4189254\ttotal: 12s\tremaining: 12.3s\n",
      "494:\tlearn: 0.4187058\ttotal: 12.1s\tremaining: 12.3s\n",
      "495:\tlearn: 0.4184957\ttotal: 12.1s\tremaining: 12.3s\n",
      "496:\tlearn: 0.4182216\ttotal: 12.1s\tremaining: 12.3s\n",
      "497:\tlearn: 0.4179432\ttotal: 12.1s\tremaining: 12.2s\n",
      "498:\tlearn: 0.4176563\ttotal: 12.2s\tremaining: 12.2s\n",
      "499:\tlearn: 0.4175058\ttotal: 12.2s\tremaining: 12.2s\n",
      "500:\tlearn: 0.4172949\ttotal: 12.2s\tremaining: 12.2s\n",
      "501:\tlearn: 0.4170812\ttotal: 12.3s\tremaining: 12.2s\n",
      "502:\tlearn: 0.4168262\ttotal: 12.3s\tremaining: 12.1s\n",
      "503:\tlearn: 0.4166012\ttotal: 12.3s\tremaining: 12.1s\n",
      "504:\tlearn: 0.4164071\ttotal: 12.3s\tremaining: 12.1s\n",
      "505:\tlearn: 0.4161480\ttotal: 12.4s\tremaining: 12.1s\n",
      "506:\tlearn: 0.4158978\ttotal: 12.4s\tremaining: 12.1s\n",
      "507:\tlearn: 0.4156665\ttotal: 12.4s\tremaining: 12s\n",
      "508:\tlearn: 0.4153981\ttotal: 12.5s\tremaining: 12s\n",
      "509:\tlearn: 0.4151904\ttotal: 12.5s\tremaining: 12s\n",
      "510:\tlearn: 0.4149586\ttotal: 12.5s\tremaining: 12s\n",
      "511:\tlearn: 0.4146964\ttotal: 12.5s\tremaining: 11.9s\n",
      "512:\tlearn: 0.4143809\ttotal: 12.6s\tremaining: 11.9s\n",
      "513:\tlearn: 0.4142174\ttotal: 12.6s\tremaining: 11.9s\n",
      "514:\tlearn: 0.4140485\ttotal: 12.6s\tremaining: 11.9s\n",
      "515:\tlearn: 0.4137886\ttotal: 12.7s\tremaining: 11.9s\n",
      "516:\tlearn: 0.4135417\ttotal: 12.7s\tremaining: 11.9s\n",
      "517:\tlearn: 0.4132843\ttotal: 12.7s\tremaining: 11.8s\n",
      "518:\tlearn: 0.4130217\ttotal: 12.7s\tremaining: 11.8s\n",
      "519:\tlearn: 0.4127901\ttotal: 12.8s\tremaining: 11.8s\n",
      "520:\tlearn: 0.4125831\ttotal: 12.8s\tremaining: 11.8s\n",
      "521:\tlearn: 0.4122632\ttotal: 12.8s\tremaining: 11.7s\n",
      "522:\tlearn: 0.4120468\ttotal: 12.8s\tremaining: 11.7s\n",
      "523:\tlearn: 0.4118211\ttotal: 12.9s\tremaining: 11.7s\n",
      "524:\tlearn: 0.4115154\ttotal: 12.9s\tremaining: 11.7s\n",
      "525:\tlearn: 0.4112417\ttotal: 12.9s\tremaining: 11.7s\n",
      "526:\tlearn: 0.4109703\ttotal: 13s\tremaining: 11.6s\n",
      "527:\tlearn: 0.4107363\ttotal: 13s\tremaining: 11.6s\n",
      "528:\tlearn: 0.4104906\ttotal: 13s\tremaining: 11.6s\n",
      "529:\tlearn: 0.4102519\ttotal: 13.1s\tremaining: 11.6s\n",
      "530:\tlearn: 0.4100532\ttotal: 13.1s\tremaining: 11.6s\n",
      "531:\tlearn: 0.4097974\ttotal: 13.1s\tremaining: 11.5s\n",
      "532:\tlearn: 0.4095472\ttotal: 13.2s\tremaining: 11.5s\n",
      "533:\tlearn: 0.4093844\ttotal: 13.2s\tremaining: 11.5s\n",
      "534:\tlearn: 0.4091780\ttotal: 13.2s\tremaining: 11.5s\n",
      "535:\tlearn: 0.4089882\ttotal: 13.2s\tremaining: 11.5s\n",
      "536:\tlearn: 0.4088621\ttotal: 13.3s\tremaining: 11.4s\n",
      "537:\tlearn: 0.4086783\ttotal: 13.3s\tremaining: 11.4s\n",
      "538:\tlearn: 0.4084384\ttotal: 13.3s\tremaining: 11.4s\n",
      "539:\tlearn: 0.4081957\ttotal: 13.3s\tremaining: 11.4s\n",
      "540:\tlearn: 0.4080553\ttotal: 13.4s\tremaining: 11.3s\n",
      "541:\tlearn: 0.4078011\ttotal: 13.4s\tremaining: 11.3s\n",
      "542:\tlearn: 0.4075625\ttotal: 13.4s\tremaining: 11.3s\n",
      "543:\tlearn: 0.4073047\ttotal: 13.4s\tremaining: 11.3s\n",
      "544:\tlearn: 0.4070494\ttotal: 13.5s\tremaining: 11.2s\n",
      "545:\tlearn: 0.4068136\ttotal: 13.5s\tremaining: 11.2s\n",
      "546:\tlearn: 0.4066459\ttotal: 13.5s\tremaining: 11.2s\n",
      "547:\tlearn: 0.4064153\ttotal: 13.5s\tremaining: 11.2s\n",
      "548:\tlearn: 0.4061880\ttotal: 13.6s\tremaining: 11.1s\n",
      "549:\tlearn: 0.4060303\ttotal: 13.6s\tremaining: 11.1s\n",
      "550:\tlearn: 0.4058008\ttotal: 13.6s\tremaining: 11.1s\n",
      "551:\tlearn: 0.4056439\ttotal: 13.6s\tremaining: 11.1s\n",
      "552:\tlearn: 0.4053912\ttotal: 13.7s\tremaining: 11s\n",
      "553:\tlearn: 0.4051640\ttotal: 13.7s\tremaining: 11s\n",
      "554:\tlearn: 0.4049068\ttotal: 13.7s\tremaining: 11s\n",
      "555:\tlearn: 0.4047501\ttotal: 13.7s\tremaining: 11s\n",
      "556:\tlearn: 0.4044813\ttotal: 13.8s\tremaining: 10.9s\n",
      "557:\tlearn: 0.4043052\ttotal: 13.8s\tremaining: 10.9s\n",
      "558:\tlearn: 0.4040620\ttotal: 13.8s\tremaining: 10.9s\n",
      "559:\tlearn: 0.4039036\ttotal: 13.8s\tremaining: 10.9s\n",
      "560:\tlearn: 0.4036614\ttotal: 13.9s\tremaining: 10.8s\n",
      "561:\tlearn: 0.4034290\ttotal: 13.9s\tremaining: 10.8s\n",
      "562:\tlearn: 0.4031127\ttotal: 13.9s\tremaining: 10.8s\n",
      "563:\tlearn: 0.4029520\ttotal: 13.9s\tremaining: 10.8s\n",
      "564:\tlearn: 0.4027957\ttotal: 14s\tremaining: 10.7s\n",
      "565:\tlearn: 0.4025741\ttotal: 14s\tremaining: 10.7s\n",
      "566:\tlearn: 0.4023566\ttotal: 14s\tremaining: 10.7s\n",
      "567:\tlearn: 0.4021891\ttotal: 14s\tremaining: 10.7s\n",
      "568:\tlearn: 0.4019597\ttotal: 14.1s\tremaining: 10.7s\n",
      "569:\tlearn: 0.4017264\ttotal: 14.1s\tremaining: 10.6s\n",
      "570:\tlearn: 0.4015007\ttotal: 14.1s\tremaining: 10.6s\n",
      "571:\tlearn: 0.4012855\ttotal: 14.1s\tremaining: 10.6s\n",
      "572:\tlearn: 0.4010683\ttotal: 14.2s\tremaining: 10.6s\n",
      "573:\tlearn: 0.4009318\ttotal: 14.2s\tremaining: 10.5s\n",
      "574:\tlearn: 0.4007487\ttotal: 14.2s\tremaining: 10.5s\n",
      "575:\tlearn: 0.4005044\ttotal: 14.2s\tremaining: 10.5s\n",
      "576:\tlearn: 0.4003150\ttotal: 14.3s\tremaining: 10.5s\n",
      "577:\tlearn: 0.4001033\ttotal: 14.3s\tremaining: 10.4s\n",
      "578:\tlearn: 0.3999014\ttotal: 14.3s\tremaining: 10.4s\n",
      "579:\tlearn: 0.3997221\ttotal: 14.3s\tremaining: 10.4s\n",
      "580:\tlearn: 0.3994753\ttotal: 14.4s\tremaining: 10.4s\n",
      "581:\tlearn: 0.3992427\ttotal: 14.4s\tremaining: 10.3s\n",
      "582:\tlearn: 0.3990614\ttotal: 14.4s\tremaining: 10.3s\n",
      "583:\tlearn: 0.3988223\ttotal: 14.5s\tremaining: 10.3s\n",
      "584:\tlearn: 0.3986268\ttotal: 14.5s\tremaining: 10.3s\n",
      "585:\tlearn: 0.3983745\ttotal: 14.5s\tremaining: 10.3s\n",
      "586:\tlearn: 0.3981504\ttotal: 14.5s\tremaining: 10.2s\n",
      "587:\tlearn: 0.3979175\ttotal: 14.6s\tremaining: 10.2s\n",
      "588:\tlearn: 0.3977185\ttotal: 14.6s\tremaining: 10.2s\n",
      "589:\tlearn: 0.3975224\ttotal: 14.6s\tremaining: 10.2s\n",
      "590:\tlearn: 0.3973357\ttotal: 14.7s\tremaining: 10.1s\n",
      "591:\tlearn: 0.3970824\ttotal: 14.7s\tremaining: 10.1s\n",
      "592:\tlearn: 0.3968912\ttotal: 14.7s\tremaining: 10.1s\n",
      "593:\tlearn: 0.3966532\ttotal: 14.7s\tremaining: 10.1s\n",
      "594:\tlearn: 0.3964350\ttotal: 14.8s\tremaining: 10s\n",
      "595:\tlearn: 0.3962891\ttotal: 14.8s\tremaining: 10s\n",
      "596:\tlearn: 0.3961034\ttotal: 14.8s\tremaining: 9.99s\n",
      "597:\tlearn: 0.3958209\ttotal: 14.8s\tremaining: 9.97s\n",
      "598:\tlearn: 0.3955624\ttotal: 14.9s\tremaining: 9.95s\n",
      "599:\tlearn: 0.3953501\ttotal: 14.9s\tremaining: 9.93s\n",
      "600:\tlearn: 0.3951886\ttotal: 14.9s\tremaining: 9.91s\n",
      "601:\tlearn: 0.3949691\ttotal: 15s\tremaining: 9.89s\n",
      "602:\tlearn: 0.3947640\ttotal: 15s\tremaining: 9.87s\n",
      "603:\tlearn: 0.3946018\ttotal: 15s\tremaining: 9.84s\n",
      "604:\tlearn: 0.3943123\ttotal: 15s\tremaining: 9.82s\n",
      "605:\tlearn: 0.3941136\ttotal: 15.1s\tremaining: 9.79s\n",
      "606:\tlearn: 0.3938466\ttotal: 15.1s\tremaining: 9.77s\n",
      "607:\tlearn: 0.3936461\ttotal: 15.1s\tremaining: 9.75s\n",
      "608:\tlearn: 0.3934483\ttotal: 15.1s\tremaining: 9.72s\n",
      "609:\tlearn: 0.3932753\ttotal: 15.2s\tremaining: 9.7s\n",
      "610:\tlearn: 0.3930568\ttotal: 15.2s\tremaining: 9.67s\n",
      "611:\tlearn: 0.3928933\ttotal: 15.2s\tremaining: 9.64s\n",
      "612:\tlearn: 0.3926855\ttotal: 15.2s\tremaining: 9.62s\n",
      "613:\tlearn: 0.3924195\ttotal: 15.3s\tremaining: 9.6s\n",
      "614:\tlearn: 0.3922500\ttotal: 15.3s\tremaining: 9.57s\n",
      "615:\tlearn: 0.3920949\ttotal: 15.3s\tremaining: 9.54s\n",
      "616:\tlearn: 0.3918495\ttotal: 15.3s\tremaining: 9.52s\n",
      "617:\tlearn: 0.3916787\ttotal: 15.4s\tremaining: 9.49s\n",
      "618:\tlearn: 0.3914644\ttotal: 15.4s\tremaining: 9.47s\n",
      "619:\tlearn: 0.3912618\ttotal: 15.4s\tremaining: 9.44s\n",
      "620:\tlearn: 0.3910877\ttotal: 15.4s\tremaining: 9.42s\n",
      "621:\tlearn: 0.3908840\ttotal: 15.5s\tremaining: 9.39s\n",
      "622:\tlearn: 0.3906183\ttotal: 15.5s\tremaining: 9.37s\n",
      "623:\tlearn: 0.3904029\ttotal: 15.5s\tremaining: 9.34s\n",
      "624:\tlearn: 0.3902007\ttotal: 15.5s\tremaining: 9.32s\n",
      "625:\tlearn: 0.3899385\ttotal: 15.6s\tremaining: 9.29s\n",
      "626:\tlearn: 0.3897308\ttotal: 15.6s\tremaining: 9.27s\n",
      "627:\tlearn: 0.3895422\ttotal: 15.6s\tremaining: 9.24s\n",
      "628:\tlearn: 0.3893824\ttotal: 15.6s\tremaining: 9.21s\n",
      "629:\tlearn: 0.3892645\ttotal: 15.6s\tremaining: 9.19s\n",
      "630:\tlearn: 0.3890438\ttotal: 15.7s\tremaining: 9.16s\n",
      "631:\tlearn: 0.3888211\ttotal: 15.7s\tremaining: 9.14s\n",
      "632:\tlearn: 0.3886327\ttotal: 15.7s\tremaining: 9.11s\n",
      "633:\tlearn: 0.3884039\ttotal: 15.7s\tremaining: 9.09s\n",
      "634:\tlearn: 0.3881939\ttotal: 15.8s\tremaining: 9.07s\n",
      "635:\tlearn: 0.3879620\ttotal: 15.8s\tremaining: 9.04s\n",
      "636:\tlearn: 0.3878117\ttotal: 15.8s\tremaining: 9.02s\n",
      "637:\tlearn: 0.3876180\ttotal: 15.9s\tremaining: 9s\n",
      "638:\tlearn: 0.3873901\ttotal: 15.9s\tremaining: 8.98s\n",
      "639:\tlearn: 0.3872121\ttotal: 15.9s\tremaining: 8.95s\n",
      "640:\tlearn: 0.3870438\ttotal: 15.9s\tremaining: 8.93s\n",
      "641:\tlearn: 0.3867768\ttotal: 16s\tremaining: 8.91s\n",
      "642:\tlearn: 0.3865588\ttotal: 16s\tremaining: 8.88s\n",
      "643:\tlearn: 0.3863723\ttotal: 16s\tremaining: 8.86s\n",
      "644:\tlearn: 0.3861459\ttotal: 16.1s\tremaining: 8.84s\n",
      "645:\tlearn: 0.3859796\ttotal: 16.1s\tremaining: 8.82s\n",
      "646:\tlearn: 0.3858580\ttotal: 16.1s\tremaining: 8.79s\n",
      "647:\tlearn: 0.3857091\ttotal: 16.1s\tremaining: 8.77s\n",
      "648:\tlearn: 0.3855060\ttotal: 16.2s\tremaining: 8.74s\n",
      "649:\tlearn: 0.3852525\ttotal: 16.2s\tremaining: 8.72s\n",
      "650:\tlearn: 0.3850976\ttotal: 16.2s\tremaining: 8.7s\n",
      "651:\tlearn: 0.3848760\ttotal: 16.2s\tremaining: 8.67s\n",
      "652:\tlearn: 0.3846576\ttotal: 16.3s\tremaining: 8.65s\n",
      "653:\tlearn: 0.3844605\ttotal: 16.3s\tremaining: 8.62s\n",
      "654:\tlearn: 0.3842186\ttotal: 16.3s\tremaining: 8.6s\n",
      "655:\tlearn: 0.3840811\ttotal: 16.4s\tremaining: 8.57s\n",
      "656:\tlearn: 0.3839220\ttotal: 16.4s\tremaining: 8.55s\n",
      "657:\tlearn: 0.3837516\ttotal: 16.4s\tremaining: 8.53s\n",
      "658:\tlearn: 0.3835503\ttotal: 16.4s\tremaining: 8.5s\n",
      "659:\tlearn: 0.3833601\ttotal: 16.5s\tremaining: 8.48s\n",
      "660:\tlearn: 0.3831986\ttotal: 16.5s\tremaining: 8.45s\n",
      "661:\tlearn: 0.3829366\ttotal: 16.5s\tremaining: 8.43s\n",
      "662:\tlearn: 0.3827302\ttotal: 16.5s\tremaining: 8.41s\n",
      "663:\tlearn: 0.3826177\ttotal: 16.6s\tremaining: 8.38s\n",
      "664:\tlearn: 0.3823235\ttotal: 16.6s\tremaining: 8.36s\n",
      "665:\tlearn: 0.3821439\ttotal: 16.6s\tremaining: 8.34s\n",
      "666:\tlearn: 0.3819188\ttotal: 16.7s\tremaining: 8.31s\n",
      "667:\tlearn: 0.3817599\ttotal: 16.7s\tremaining: 8.29s\n",
      "668:\tlearn: 0.3815674\ttotal: 16.7s\tremaining: 8.26s\n",
      "669:\tlearn: 0.3813841\ttotal: 16.7s\tremaining: 8.24s\n",
      "670:\tlearn: 0.3812242\ttotal: 16.8s\tremaining: 8.22s\n",
      "671:\tlearn: 0.3810588\ttotal: 16.8s\tremaining: 8.19s\n",
      "672:\tlearn: 0.3808648\ttotal: 16.8s\tremaining: 8.17s\n",
      "673:\tlearn: 0.3806051\ttotal: 16.8s\tremaining: 8.14s\n",
      "674:\tlearn: 0.3803906\ttotal: 16.9s\tremaining: 8.12s\n",
      "675:\tlearn: 0.3801934\ttotal: 16.9s\tremaining: 8.09s\n",
      "676:\tlearn: 0.3799770\ttotal: 16.9s\tremaining: 8.07s\n",
      "677:\tlearn: 0.3797234\ttotal: 16.9s\tremaining: 8.04s\n",
      "678:\tlearn: 0.3795423\ttotal: 17s\tremaining: 8.02s\n",
      "679:\tlearn: 0.3793538\ttotal: 17s\tremaining: 8s\n",
      "680:\tlearn: 0.3791194\ttotal: 17s\tremaining: 7.97s\n",
      "681:\tlearn: 0.3789274\ttotal: 17s\tremaining: 7.95s\n",
      "682:\tlearn: 0.3786764\ttotal: 17.1s\tremaining: 7.92s\n",
      "683:\tlearn: 0.3784276\ttotal: 17.1s\tremaining: 7.9s\n",
      "684:\tlearn: 0.3782479\ttotal: 17.1s\tremaining: 7.87s\n",
      "685:\tlearn: 0.3780020\ttotal: 17.2s\tremaining: 7.85s\n",
      "686:\tlearn: 0.3777805\ttotal: 17.2s\tremaining: 7.83s\n",
      "687:\tlearn: 0.3775938\ttotal: 17.2s\tremaining: 7.8s\n",
      "688:\tlearn: 0.3774107\ttotal: 17.2s\tremaining: 7.78s\n",
      "689:\tlearn: 0.3771877\ttotal: 17.3s\tremaining: 7.75s\n",
      "690:\tlearn: 0.3769991\ttotal: 17.3s\tremaining: 7.73s\n",
      "691:\tlearn: 0.3767884\ttotal: 17.3s\tremaining: 7.7s\n",
      "692:\tlearn: 0.3766231\ttotal: 17.3s\tremaining: 7.68s\n",
      "693:\tlearn: 0.3764861\ttotal: 17.4s\tremaining: 7.65s\n",
      "694:\tlearn: 0.3762653\ttotal: 17.4s\tremaining: 7.63s\n",
      "695:\tlearn: 0.3760698\ttotal: 17.4s\tremaining: 7.6s\n",
      "696:\tlearn: 0.3758550\ttotal: 17.4s\tremaining: 7.58s\n",
      "697:\tlearn: 0.3756627\ttotal: 17.5s\tremaining: 7.56s\n",
      "698:\tlearn: 0.3754824\ttotal: 17.5s\tremaining: 7.53s\n",
      "699:\tlearn: 0.3753078\ttotal: 17.5s\tremaining: 7.51s\n",
      "700:\tlearn: 0.3751427\ttotal: 17.5s\tremaining: 7.48s\n",
      "701:\tlearn: 0.3749203\ttotal: 17.6s\tremaining: 7.46s\n",
      "702:\tlearn: 0.3746670\ttotal: 17.6s\tremaining: 7.43s\n",
      "703:\tlearn: 0.3744360\ttotal: 17.6s\tremaining: 7.41s\n",
      "704:\tlearn: 0.3742827\ttotal: 17.7s\tremaining: 7.39s\n",
      "705:\tlearn: 0.3741309\ttotal: 17.7s\tremaining: 7.36s\n",
      "706:\tlearn: 0.3739649\ttotal: 17.7s\tremaining: 7.34s\n",
      "707:\tlearn: 0.3737983\ttotal: 17.7s\tremaining: 7.31s\n",
      "708:\tlearn: 0.3736199\ttotal: 17.8s\tremaining: 7.29s\n",
      "709:\tlearn: 0.3733589\ttotal: 17.8s\tremaining: 7.26s\n",
      "710:\tlearn: 0.3732029\ttotal: 17.8s\tremaining: 7.24s\n",
      "711:\tlearn: 0.3730449\ttotal: 17.8s\tremaining: 7.22s\n",
      "712:\tlearn: 0.3729314\ttotal: 17.9s\tremaining: 7.19s\n",
      "713:\tlearn: 0.3727851\ttotal: 17.9s\tremaining: 7.17s\n",
      "714:\tlearn: 0.3725957\ttotal: 17.9s\tremaining: 7.14s\n",
      "715:\tlearn: 0.3723548\ttotal: 17.9s\tremaining: 7.12s\n",
      "716:\tlearn: 0.3721353\ttotal: 18s\tremaining: 7.09s\n",
      "717:\tlearn: 0.3719040\ttotal: 18s\tremaining: 7.07s\n",
      "718:\tlearn: 0.3716831\ttotal: 18s\tremaining: 7.04s\n",
      "719:\tlearn: 0.3715083\ttotal: 18s\tremaining: 7.02s\n",
      "720:\tlearn: 0.3713788\ttotal: 18.1s\tremaining: 6.99s\n",
      "721:\tlearn: 0.3711502\ttotal: 18.1s\tremaining: 6.97s\n",
      "722:\tlearn: 0.3709777\ttotal: 18.1s\tremaining: 6.94s\n",
      "723:\tlearn: 0.3707816\ttotal: 18.1s\tremaining: 6.92s\n",
      "724:\tlearn: 0.3705837\ttotal: 18.2s\tremaining: 6.89s\n",
      "725:\tlearn: 0.3703911\ttotal: 18.2s\tremaining: 6.87s\n",
      "726:\tlearn: 0.3702375\ttotal: 18.2s\tremaining: 6.85s\n",
      "727:\tlearn: 0.3700103\ttotal: 18.3s\tremaining: 6.82s\n",
      "728:\tlearn: 0.3698526\ttotal: 18.3s\tremaining: 6.8s\n",
      "729:\tlearn: 0.3696355\ttotal: 18.3s\tremaining: 6.77s\n",
      "730:\tlearn: 0.3695156\ttotal: 18.3s\tremaining: 6.75s\n",
      "731:\tlearn: 0.3693688\ttotal: 18.4s\tremaining: 6.72s\n",
      "732:\tlearn: 0.3691391\ttotal: 18.4s\tremaining: 6.7s\n",
      "733:\tlearn: 0.3689371\ttotal: 18.4s\tremaining: 6.68s\n",
      "734:\tlearn: 0.3687603\ttotal: 18.4s\tremaining: 6.65s\n",
      "735:\tlearn: 0.3684977\ttotal: 18.5s\tremaining: 6.63s\n",
      "736:\tlearn: 0.3683287\ttotal: 18.5s\tremaining: 6.6s\n",
      "737:\tlearn: 0.3681294\ttotal: 18.5s\tremaining: 6.58s\n",
      "738:\tlearn: 0.3679898\ttotal: 18.6s\tremaining: 6.55s\n",
      "739:\tlearn: 0.3677994\ttotal: 18.6s\tremaining: 6.53s\n",
      "740:\tlearn: 0.3675308\ttotal: 18.6s\tremaining: 6.5s\n",
      "741:\tlearn: 0.3673732\ttotal: 18.6s\tremaining: 6.48s\n",
      "742:\tlearn: 0.3672494\ttotal: 18.7s\tremaining: 6.46s\n",
      "743:\tlearn: 0.3670943\ttotal: 18.7s\tremaining: 6.43s\n",
      "744:\tlearn: 0.3669298\ttotal: 18.7s\tremaining: 6.41s\n",
      "745:\tlearn: 0.3667224\ttotal: 18.7s\tremaining: 6.38s\n",
      "746:\tlearn: 0.3664961\ttotal: 18.8s\tremaining: 6.36s\n",
      "747:\tlearn: 0.3662808\ttotal: 18.8s\tremaining: 6.33s\n",
      "748:\tlearn: 0.3661155\ttotal: 18.8s\tremaining: 6.31s\n",
      "749:\tlearn: 0.3659073\ttotal: 18.9s\tremaining: 6.28s\n",
      "750:\tlearn: 0.3657267\ttotal: 18.9s\tremaining: 6.26s\n",
      "751:\tlearn: 0.3655840\ttotal: 18.9s\tremaining: 6.23s\n",
      "752:\tlearn: 0.3653868\ttotal: 18.9s\tremaining: 6.21s\n",
      "753:\tlearn: 0.3652188\ttotal: 19s\tremaining: 6.19s\n",
      "754:\tlearn: 0.3650320\ttotal: 19s\tremaining: 6.16s\n",
      "755:\tlearn: 0.3648027\ttotal: 19s\tremaining: 6.15s\n",
      "756:\tlearn: 0.3646695\ttotal: 19.1s\tremaining: 6.12s\n",
      "757:\tlearn: 0.3645212\ttotal: 19.1s\tremaining: 6.1s\n",
      "758:\tlearn: 0.3643042\ttotal: 19.1s\tremaining: 6.07s\n",
      "759:\tlearn: 0.3640849\ttotal: 19.2s\tremaining: 6.05s\n",
      "760:\tlearn: 0.3639521\ttotal: 19.2s\tremaining: 6.03s\n",
      "761:\tlearn: 0.3637416\ttotal: 19.2s\tremaining: 6s\n",
      "762:\tlearn: 0.3635914\ttotal: 19.2s\tremaining: 5.98s\n",
      "763:\tlearn: 0.3634436\ttotal: 19.3s\tremaining: 5.95s\n",
      "764:\tlearn: 0.3631772\ttotal: 19.3s\tremaining: 5.93s\n",
      "765:\tlearn: 0.3629773\ttotal: 19.3s\tremaining: 5.9s\n",
      "766:\tlearn: 0.3628127\ttotal: 19.4s\tremaining: 5.88s\n",
      "767:\tlearn: 0.3626189\ttotal: 19.4s\tremaining: 5.85s\n",
      "768:\tlearn: 0.3624589\ttotal: 19.4s\tremaining: 5.83s\n",
      "769:\tlearn: 0.3622449\ttotal: 19.4s\tremaining: 5.81s\n",
      "770:\tlearn: 0.3620124\ttotal: 19.5s\tremaining: 5.78s\n",
      "771:\tlearn: 0.3618720\ttotal: 19.5s\tremaining: 5.76s\n",
      "772:\tlearn: 0.3616604\ttotal: 19.5s\tremaining: 5.74s\n",
      "773:\tlearn: 0.3613994\ttotal: 19.6s\tremaining: 5.71s\n",
      "774:\tlearn: 0.3612298\ttotal: 19.6s\tremaining: 5.69s\n",
      "775:\tlearn: 0.3610724\ttotal: 19.6s\tremaining: 5.66s\n",
      "776:\tlearn: 0.3608978\ttotal: 19.6s\tremaining: 5.64s\n",
      "777:\tlearn: 0.3607831\ttotal: 19.7s\tremaining: 5.61s\n",
      "778:\tlearn: 0.3605558\ttotal: 19.7s\tremaining: 5.59s\n",
      "779:\tlearn: 0.3603836\ttotal: 19.7s\tremaining: 5.56s\n",
      "780:\tlearn: 0.3602256\ttotal: 19.7s\tremaining: 5.54s\n",
      "781:\tlearn: 0.3600778\ttotal: 19.8s\tremaining: 5.51s\n",
      "782:\tlearn: 0.3599251\ttotal: 19.8s\tremaining: 5.49s\n",
      "783:\tlearn: 0.3597270\ttotal: 19.8s\tremaining: 5.46s\n",
      "784:\tlearn: 0.3595571\ttotal: 19.9s\tremaining: 5.44s\n",
      "785:\tlearn: 0.3594066\ttotal: 19.9s\tremaining: 5.42s\n",
      "786:\tlearn: 0.3592723\ttotal: 19.9s\tremaining: 5.39s\n",
      "787:\tlearn: 0.3590241\ttotal: 20s\tremaining: 5.37s\n",
      "788:\tlearn: 0.3588168\ttotal: 20s\tremaining: 5.34s\n",
      "789:\tlearn: 0.3586100\ttotal: 20s\tremaining: 5.32s\n",
      "790:\tlearn: 0.3584192\ttotal: 20s\tremaining: 5.29s\n",
      "791:\tlearn: 0.3582407\ttotal: 20.1s\tremaining: 5.27s\n",
      "792:\tlearn: 0.3580370\ttotal: 20.1s\tremaining: 5.24s\n",
      "793:\tlearn: 0.3578753\ttotal: 20.1s\tremaining: 5.22s\n",
      "794:\tlearn: 0.3576808\ttotal: 20.1s\tremaining: 5.19s\n",
      "795:\tlearn: 0.3575319\ttotal: 20.2s\tremaining: 5.17s\n",
      "796:\tlearn: 0.3573513\ttotal: 20.2s\tremaining: 5.14s\n",
      "797:\tlearn: 0.3571392\ttotal: 20.2s\tremaining: 5.12s\n",
      "798:\tlearn: 0.3569527\ttotal: 20.2s\tremaining: 5.09s\n",
      "799:\tlearn: 0.3567146\ttotal: 20.3s\tremaining: 5.07s\n",
      "800:\tlearn: 0.3565613\ttotal: 20.3s\tremaining: 5.04s\n",
      "801:\tlearn: 0.3563862\ttotal: 20.3s\tremaining: 5.02s\n",
      "802:\tlearn: 0.3562310\ttotal: 20.3s\tremaining: 4.99s\n",
      "803:\tlearn: 0.3560004\ttotal: 20.4s\tremaining: 4.97s\n",
      "804:\tlearn: 0.3557866\ttotal: 20.4s\tremaining: 4.94s\n",
      "805:\tlearn: 0.3556693\ttotal: 20.4s\tremaining: 4.92s\n",
      "806:\tlearn: 0.3555014\ttotal: 20.5s\tremaining: 4.89s\n",
      "807:\tlearn: 0.3553224\ttotal: 20.5s\tremaining: 4.87s\n",
      "808:\tlearn: 0.3551043\ttotal: 20.5s\tremaining: 4.84s\n",
      "809:\tlearn: 0.3548585\ttotal: 20.5s\tremaining: 4.82s\n",
      "810:\tlearn: 0.3547061\ttotal: 20.6s\tremaining: 4.79s\n",
      "811:\tlearn: 0.3545605\ttotal: 20.6s\tremaining: 4.77s\n",
      "812:\tlearn: 0.3543598\ttotal: 20.6s\tremaining: 4.74s\n",
      "813:\tlearn: 0.3541565\ttotal: 20.6s\tremaining: 4.72s\n",
      "814:\tlearn: 0.3539165\ttotal: 20.7s\tremaining: 4.69s\n",
      "815:\tlearn: 0.3537059\ttotal: 20.7s\tremaining: 4.67s\n",
      "816:\tlearn: 0.3535802\ttotal: 20.7s\tremaining: 4.64s\n",
      "817:\tlearn: 0.3534512\ttotal: 20.7s\tremaining: 4.61s\n",
      "818:\tlearn: 0.3533298\ttotal: 20.8s\tremaining: 4.59s\n",
      "819:\tlearn: 0.3531556\ttotal: 20.8s\tremaining: 4.56s\n",
      "820:\tlearn: 0.3529428\ttotal: 20.8s\tremaining: 4.54s\n",
      "821:\tlearn: 0.3527394\ttotal: 20.8s\tremaining: 4.51s\n",
      "822:\tlearn: 0.3525388\ttotal: 20.9s\tremaining: 4.49s\n",
      "823:\tlearn: 0.3524235\ttotal: 20.9s\tremaining: 4.46s\n",
      "824:\tlearn: 0.3521978\ttotal: 20.9s\tremaining: 4.44s\n",
      "825:\tlearn: 0.3520057\ttotal: 21s\tremaining: 4.42s\n",
      "826:\tlearn: 0.3518247\ttotal: 21s\tremaining: 4.39s\n",
      "827:\tlearn: 0.3516452\ttotal: 21s\tremaining: 4.37s\n",
      "828:\tlearn: 0.3515025\ttotal: 21s\tremaining: 4.34s\n",
      "829:\tlearn: 0.3512852\ttotal: 21.1s\tremaining: 4.32s\n",
      "830:\tlearn: 0.3510852\ttotal: 21.1s\tremaining: 4.29s\n",
      "831:\tlearn: 0.3509073\ttotal: 21.1s\tremaining: 4.27s\n",
      "832:\tlearn: 0.3506819\ttotal: 21.2s\tremaining: 4.24s\n",
      "833:\tlearn: 0.3504730\ttotal: 21.2s\tremaining: 4.22s\n",
      "834:\tlearn: 0.3503304\ttotal: 21.2s\tremaining: 4.19s\n",
      "835:\tlearn: 0.3501374\ttotal: 21.2s\tremaining: 4.17s\n",
      "836:\tlearn: 0.3499718\ttotal: 21.3s\tremaining: 4.14s\n",
      "837:\tlearn: 0.3497940\ttotal: 21.3s\tremaining: 4.12s\n",
      "838:\tlearn: 0.3496105\ttotal: 21.3s\tremaining: 4.09s\n",
      "839:\tlearn: 0.3494087\ttotal: 21.3s\tremaining: 4.07s\n",
      "840:\tlearn: 0.3492094\ttotal: 21.4s\tremaining: 4.04s\n",
      "841:\tlearn: 0.3490511\ttotal: 21.4s\tremaining: 4.01s\n",
      "842:\tlearn: 0.3488402\ttotal: 21.4s\tremaining: 3.99s\n",
      "843:\tlearn: 0.3486604\ttotal: 21.4s\tremaining: 3.96s\n",
      "844:\tlearn: 0.3484997\ttotal: 21.5s\tremaining: 3.94s\n",
      "845:\tlearn: 0.3482945\ttotal: 21.5s\tremaining: 3.91s\n",
      "846:\tlearn: 0.3481447\ttotal: 21.5s\tremaining: 3.89s\n",
      "847:\tlearn: 0.3479316\ttotal: 21.6s\tremaining: 3.86s\n",
      "848:\tlearn: 0.3477474\ttotal: 21.6s\tremaining: 3.84s\n",
      "849:\tlearn: 0.3475135\ttotal: 21.6s\tremaining: 3.81s\n",
      "850:\tlearn: 0.3473656\ttotal: 21.6s\tremaining: 3.79s\n",
      "851:\tlearn: 0.3471768\ttotal: 21.7s\tremaining: 3.76s\n",
      "852:\tlearn: 0.3470009\ttotal: 21.7s\tremaining: 3.74s\n",
      "853:\tlearn: 0.3468257\ttotal: 21.7s\tremaining: 3.71s\n",
      "854:\tlearn: 0.3466755\ttotal: 21.7s\tremaining: 3.69s\n",
      "855:\tlearn: 0.3465634\ttotal: 21.8s\tremaining: 3.66s\n",
      "856:\tlearn: 0.3463966\ttotal: 21.8s\tremaining: 3.64s\n",
      "857:\tlearn: 0.3462521\ttotal: 21.8s\tremaining: 3.61s\n",
      "858:\tlearn: 0.3461151\ttotal: 21.9s\tremaining: 3.59s\n",
      "859:\tlearn: 0.3459422\ttotal: 21.9s\tremaining: 3.56s\n",
      "860:\tlearn: 0.3457360\ttotal: 21.9s\tremaining: 3.54s\n",
      "861:\tlearn: 0.3456343\ttotal: 21.9s\tremaining: 3.51s\n",
      "862:\tlearn: 0.3454776\ttotal: 22s\tremaining: 3.49s\n",
      "863:\tlearn: 0.3452796\ttotal: 22s\tremaining: 3.46s\n",
      "864:\tlearn: 0.3451030\ttotal: 22s\tremaining: 3.44s\n",
      "865:\tlearn: 0.3448873\ttotal: 22s\tremaining: 3.41s\n",
      "866:\tlearn: 0.3447264\ttotal: 22.1s\tremaining: 3.38s\n",
      "867:\tlearn: 0.3445290\ttotal: 22.1s\tremaining: 3.36s\n",
      "868:\tlearn: 0.3443568\ttotal: 22.1s\tremaining: 3.33s\n",
      "869:\tlearn: 0.3441830\ttotal: 22.1s\tremaining: 3.31s\n",
      "870:\tlearn: 0.3439556\ttotal: 22.2s\tremaining: 3.28s\n",
      "871:\tlearn: 0.3437438\ttotal: 22.2s\tremaining: 3.26s\n",
      "872:\tlearn: 0.3436386\ttotal: 22.2s\tremaining: 3.23s\n",
      "873:\tlearn: 0.3434726\ttotal: 22.2s\tremaining: 3.21s\n",
      "874:\tlearn: 0.3433567\ttotal: 22.3s\tremaining: 3.18s\n",
      "875:\tlearn: 0.3431350\ttotal: 22.3s\tremaining: 3.16s\n",
      "876:\tlearn: 0.3430143\ttotal: 22.3s\tremaining: 3.13s\n",
      "877:\tlearn: 0.3428472\ttotal: 22.3s\tremaining: 3.1s\n",
      "878:\tlearn: 0.3426906\ttotal: 22.4s\tremaining: 3.08s\n",
      "879:\tlearn: 0.3425207\ttotal: 22.4s\tremaining: 3.06s\n",
      "880:\tlearn: 0.3423232\ttotal: 22.4s\tremaining: 3.03s\n",
      "881:\tlearn: 0.3421683\ttotal: 22.5s\tremaining: 3s\n",
      "882:\tlearn: 0.3420327\ttotal: 22.5s\tremaining: 2.98s\n",
      "883:\tlearn: 0.3418622\ttotal: 22.5s\tremaining: 2.95s\n",
      "884:\tlearn: 0.3417283\ttotal: 22.5s\tremaining: 2.93s\n",
      "885:\tlearn: 0.3415956\ttotal: 22.6s\tremaining: 2.9s\n",
      "886:\tlearn: 0.3414906\ttotal: 22.6s\tremaining: 2.88s\n",
      "887:\tlearn: 0.3413372\ttotal: 22.6s\tremaining: 2.85s\n",
      "888:\tlearn: 0.3412183\ttotal: 22.6s\tremaining: 2.83s\n",
      "889:\tlearn: 0.3410185\ttotal: 22.7s\tremaining: 2.8s\n",
      "890:\tlearn: 0.3408796\ttotal: 22.7s\tremaining: 2.77s\n",
      "891:\tlearn: 0.3406846\ttotal: 22.7s\tremaining: 2.75s\n",
      "892:\tlearn: 0.3404570\ttotal: 22.7s\tremaining: 2.72s\n",
      "893:\tlearn: 0.3402776\ttotal: 22.8s\tremaining: 2.7s\n",
      "894:\tlearn: 0.3401174\ttotal: 22.8s\tremaining: 2.67s\n",
      "895:\tlearn: 0.3399475\ttotal: 22.8s\tremaining: 2.65s\n",
      "896:\tlearn: 0.3397578\ttotal: 22.8s\tremaining: 2.62s\n",
      "897:\tlearn: 0.3395705\ttotal: 22.9s\tremaining: 2.6s\n",
      "898:\tlearn: 0.3394064\ttotal: 22.9s\tremaining: 2.57s\n",
      "899:\tlearn: 0.3392765\ttotal: 22.9s\tremaining: 2.54s\n",
      "900:\tlearn: 0.3391097\ttotal: 22.9s\tremaining: 2.52s\n",
      "901:\tlearn: 0.3389613\ttotal: 23s\tremaining: 2.49s\n",
      "902:\tlearn: 0.3387803\ttotal: 23s\tremaining: 2.47s\n",
      "903:\tlearn: 0.3386053\ttotal: 23s\tremaining: 2.44s\n",
      "904:\tlearn: 0.3384074\ttotal: 23s\tremaining: 2.42s\n",
      "905:\tlearn: 0.3382331\ttotal: 23.1s\tremaining: 2.39s\n",
      "906:\tlearn: 0.3380658\ttotal: 23.1s\tremaining: 2.37s\n",
      "907:\tlearn: 0.3378834\ttotal: 23.1s\tremaining: 2.34s\n",
      "908:\tlearn: 0.3377190\ttotal: 23.1s\tremaining: 2.31s\n",
      "909:\tlearn: 0.3375917\ttotal: 23.1s\tremaining: 2.29s\n",
      "910:\tlearn: 0.3373594\ttotal: 23.2s\tremaining: 2.26s\n",
      "911:\tlearn: 0.3371872\ttotal: 23.2s\tremaining: 2.24s\n",
      "912:\tlearn: 0.3370412\ttotal: 23.2s\tremaining: 2.21s\n",
      "913:\tlearn: 0.3369010\ttotal: 23.2s\tremaining: 2.19s\n",
      "914:\tlearn: 0.3366802\ttotal: 23.3s\tremaining: 2.16s\n",
      "915:\tlearn: 0.3365299\ttotal: 23.3s\tremaining: 2.14s\n",
      "916:\tlearn: 0.3363595\ttotal: 23.3s\tremaining: 2.11s\n",
      "917:\tlearn: 0.3361975\ttotal: 23.3s\tremaining: 2.08s\n",
      "918:\tlearn: 0.3360051\ttotal: 23.4s\tremaining: 2.06s\n",
      "919:\tlearn: 0.3358199\ttotal: 23.4s\tremaining: 2.03s\n",
      "920:\tlearn: 0.3356935\ttotal: 23.4s\tremaining: 2.01s\n",
      "921:\tlearn: 0.3355754\ttotal: 23.4s\tremaining: 1.98s\n",
      "922:\tlearn: 0.3353890\ttotal: 23.5s\tremaining: 1.96s\n",
      "923:\tlearn: 0.3352562\ttotal: 23.5s\tremaining: 1.93s\n",
      "924:\tlearn: 0.3351010\ttotal: 23.5s\tremaining: 1.91s\n",
      "925:\tlearn: 0.3349024\ttotal: 23.5s\tremaining: 1.88s\n",
      "926:\tlearn: 0.3347356\ttotal: 23.6s\tremaining: 1.85s\n",
      "927:\tlearn: 0.3345310\ttotal: 23.6s\tremaining: 1.83s\n",
      "928:\tlearn: 0.3342854\ttotal: 23.6s\tremaining: 1.8s\n",
      "929:\tlearn: 0.3341473\ttotal: 23.6s\tremaining: 1.78s\n",
      "930:\tlearn: 0.3339168\ttotal: 23.6s\tremaining: 1.75s\n",
      "931:\tlearn: 0.3337392\ttotal: 23.7s\tremaining: 1.73s\n",
      "932:\tlearn: 0.3335824\ttotal: 23.7s\tremaining: 1.7s\n",
      "933:\tlearn: 0.3334158\ttotal: 23.7s\tremaining: 1.68s\n",
      "934:\tlearn: 0.3332909\ttotal: 23.7s\tremaining: 1.65s\n",
      "935:\tlearn: 0.3330732\ttotal: 23.8s\tremaining: 1.63s\n",
      "936:\tlearn: 0.3328506\ttotal: 23.8s\tremaining: 1.6s\n",
      "937:\tlearn: 0.3327184\ttotal: 23.8s\tremaining: 1.57s\n",
      "938:\tlearn: 0.3325230\ttotal: 23.9s\tremaining: 1.55s\n",
      "939:\tlearn: 0.3323670\ttotal: 23.9s\tremaining: 1.52s\n",
      "940:\tlearn: 0.3322495\ttotal: 23.9s\tremaining: 1.5s\n",
      "941:\tlearn: 0.3320539\ttotal: 23.9s\tremaining: 1.47s\n",
      "942:\tlearn: 0.3318724\ttotal: 24s\tremaining: 1.45s\n",
      "943:\tlearn: 0.3316869\ttotal: 24s\tremaining: 1.42s\n",
      "944:\tlearn: 0.3315244\ttotal: 24s\tremaining: 1.4s\n",
      "945:\tlearn: 0.3313691\ttotal: 24s\tremaining: 1.37s\n",
      "946:\tlearn: 0.3311814\ttotal: 24s\tremaining: 1.34s\n",
      "947:\tlearn: 0.3310381\ttotal: 24.1s\tremaining: 1.32s\n",
      "948:\tlearn: 0.3309152\ttotal: 24.1s\tremaining: 1.29s\n",
      "949:\tlearn: 0.3308042\ttotal: 24.1s\tremaining: 1.27s\n",
      "950:\tlearn: 0.3306582\ttotal: 24.1s\tremaining: 1.24s\n",
      "951:\tlearn: 0.3305101\ttotal: 24.2s\tremaining: 1.22s\n",
      "952:\tlearn: 0.3303608\ttotal: 24.2s\tremaining: 1.19s\n",
      "953:\tlearn: 0.3302309\ttotal: 24.2s\tremaining: 1.17s\n",
      "954:\tlearn: 0.3300943\ttotal: 24.2s\tremaining: 1.14s\n",
      "955:\tlearn: 0.3299377\ttotal: 24.3s\tremaining: 1.12s\n",
      "956:\tlearn: 0.3297285\ttotal: 24.3s\tremaining: 1.09s\n",
      "957:\tlearn: 0.3295762\ttotal: 24.3s\tremaining: 1.07s\n",
      "958:\tlearn: 0.3294094\ttotal: 24.3s\tremaining: 1.04s\n",
      "959:\tlearn: 0.3292438\ttotal: 24.4s\tremaining: 1.01s\n",
      "960:\tlearn: 0.3290456\ttotal: 24.4s\tremaining: 990ms\n",
      "961:\tlearn: 0.3288955\ttotal: 24.4s\tremaining: 965ms\n",
      "962:\tlearn: 0.3286928\ttotal: 24.4s\tremaining: 939ms\n",
      "963:\tlearn: 0.3284840\ttotal: 24.5s\tremaining: 914ms\n",
      "964:\tlearn: 0.3283449\ttotal: 24.5s\tremaining: 888ms\n",
      "965:\tlearn: 0.3281823\ttotal: 24.5s\tremaining: 863ms\n",
      "966:\tlearn: 0.3280154\ttotal: 24.5s\tremaining: 838ms\n",
      "967:\tlearn: 0.3278613\ttotal: 24.6s\tremaining: 812ms\n",
      "968:\tlearn: 0.3277299\ttotal: 24.6s\tremaining: 787ms\n",
      "969:\tlearn: 0.3275823\ttotal: 24.6s\tremaining: 762ms\n",
      "970:\tlearn: 0.3274265\ttotal: 24.6s\tremaining: 736ms\n",
      "971:\tlearn: 0.3272876\ttotal: 24.7s\tremaining: 711ms\n",
      "972:\tlearn: 0.3271336\ttotal: 24.7s\tremaining: 685ms\n",
      "973:\tlearn: 0.3269243\ttotal: 24.7s\tremaining: 660ms\n",
      "974:\tlearn: 0.3267358\ttotal: 24.7s\tremaining: 635ms\n",
      "975:\tlearn: 0.3265435\ttotal: 24.8s\tremaining: 609ms\n",
      "976:\tlearn: 0.3263344\ttotal: 24.8s\tremaining: 584ms\n",
      "977:\tlearn: 0.3261413\ttotal: 24.8s\tremaining: 558ms\n",
      "978:\tlearn: 0.3259401\ttotal: 24.8s\tremaining: 533ms\n",
      "979:\tlearn: 0.3258413\ttotal: 24.9s\tremaining: 508ms\n",
      "980:\tlearn: 0.3256070\ttotal: 24.9s\tremaining: 482ms\n",
      "981:\tlearn: 0.3254544\ttotal: 24.9s\tremaining: 457ms\n",
      "982:\tlearn: 0.3253030\ttotal: 24.9s\tremaining: 431ms\n",
      "983:\tlearn: 0.3250976\ttotal: 25s\tremaining: 406ms\n",
      "984:\tlearn: 0.3249143\ttotal: 25s\tremaining: 381ms\n",
      "985:\tlearn: 0.3247884\ttotal: 25s\tremaining: 355ms\n",
      "986:\tlearn: 0.3246227\ttotal: 25s\tremaining: 330ms\n",
      "987:\tlearn: 0.3244286\ttotal: 25.1s\tremaining: 305ms\n",
      "988:\tlearn: 0.3242783\ttotal: 25.1s\tremaining: 279ms\n",
      "989:\tlearn: 0.3241055\ttotal: 25.1s\tremaining: 254ms\n",
      "990:\tlearn: 0.3239528\ttotal: 25.2s\tremaining: 228ms\n",
      "991:\tlearn: 0.3237715\ttotal: 25.2s\tremaining: 203ms\n",
      "992:\tlearn: 0.3235893\ttotal: 25.2s\tremaining: 178ms\n",
      "993:\tlearn: 0.3234266\ttotal: 25.2s\tremaining: 152ms\n",
      "994:\tlearn: 0.3232689\ttotal: 25.3s\tremaining: 127ms\n",
      "995:\tlearn: 0.3231583\ttotal: 25.3s\tremaining: 102ms\n",
      "996:\tlearn: 0.3230065\ttotal: 25.3s\tremaining: 76.1ms\n",
      "997:\tlearn: 0.3228795\ttotal: 25.3s\tremaining: 50.7ms\n",
      "998:\tlearn: 0.3227444\ttotal: 25.3s\tremaining: 25.4ms\n",
      "999:\tlearn: 0.3225854\ttotal: 25.4s\tremaining: 0us\n",
      "Accuracy: 0.73296653602653\n"
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.7308364354630402\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1c4700ab1f0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_display = ConfusionMatrixDisplay(cm, display_labels=le.classes_)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Calculating precision, recall, and F1-score\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"F1-score:\", f1_score)\n",
    "cm_display.plot(cmap=\"Reds\", xticks_rotation=90)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 33965 --- Validation Size: 8492 --- Test Size: 10615\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data_selected, to_categorical(y_resampled), test_size=0.2, random_state=RANDOM_STATE)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=RANDOM_STATE)\n",
    "print(f\"Train Size: {len(y_train)} --- Validation Size: {len(y_val)} --- Test Size: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hyperparameters to search over\n",
    "param_grid = {\n",
    "    'n_layers': [1, 2, 3, 4, 5, 6],\n",
    "    'n_nodes': [32, 64, 128, 256, 512],\n",
    "    'dropout': [0.1, 0.2, 0.3, 0.4],\n",
    "    'lr': [0.0001, 0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "param_grid_2 = {\n",
    "    'n_nodes1': [32, 64, 128],\n",
    "    'n_nodes2': [64, 128, 256],\n",
    "    'n_nodes3': [128, 256, 512],\n",
    "    'n_nodes4': [64, 128, 256],\n",
    "    'n_nodes5': [32, 64, 128],\n",
    "    'dropout': [0.2, 0.3],\n",
    "    'lr': [0.0001, 0.001],\n",
    "    'activation': ['sigmoid', 'softmax']\n",
    "}\n",
    "\n",
    "# define the callbacks\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(lr= 0.0001, n_layers=1, n_nodes=10, dropout=0.1):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            model.add(Dense(n_nodes, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "        else:\n",
    "            model.add(Dense(n_nodes, activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(len(le.classes_), activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=lr), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dominik Hahn\\AppData\\Local\\Temp\\ipykernel_26164\\2087463641.py:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=create_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 480 candidates, totalling 1440 fits\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6889 - accuracy: 0.5417\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6811 - accuracy: 0.5694\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=1, n_nodes=32;, score=0.569 total time=   3.7s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6880 - accuracy: 0.5676\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6707 - accuracy: 0.6562\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=1, n_nodes=32;, score=0.656 total time=   4.2s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6973 - accuracy: 0.5362\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6804 - accuracy: 0.6210\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=1, n_nodes=32;, score=0.621 total time=   4.3s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6816 - accuracy: 0.5756\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6702 - accuracy: 0.5942\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=1, n_nodes=64;, score=0.594 total time=   4.3s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6943 - accuracy: 0.5510\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.6427\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=1, n_nodes=64;, score=0.643 total time=   4.3s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6833 - accuracy: 0.5739\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6645 - accuracy: 0.6529\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=1, n_nodes=64;, score=0.653 total time=   4.3s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6809 - accuracy: 0.5811\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6207\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=1, n_nodes=128;, score=0.621 total time=   4.3s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6767 - accuracy: 0.6015\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6568 - accuracy: 0.6668\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=1, n_nodes=128;, score=0.667 total time=   4.3s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6777 - accuracy: 0.5908\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6585\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=1, n_nodes=128;, score=0.659 total time=   4.2s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6693 - accuracy: 0.6184\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6472 - accuracy: 0.6649\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=1, n_nodes=256;, score=0.665 total time=   3.6s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6729 - accuracy: 0.6090\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6465 - accuracy: 0.6700\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=1, n_nodes=256;, score=0.670 total time=   3.8s\n",
      "708/708 [==============================] - 3s 3ms/step - loss: 0.6736 - accuracy: 0.6076\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6603\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=1, n_nodes=256;, score=0.660 total time=   3.5s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6589 - accuracy: 0.6367\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6399 - accuracy: 0.6663\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=1, n_nodes=512;, score=0.666 total time=   4.2s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6627 - accuracy: 0.6261\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6400 - accuracy: 0.6540\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=1, n_nodes=512;, score=0.654 total time=   4.5s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6592 - accuracy: 0.6325\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6433 - accuracy: 0.6607\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=1, n_nodes=512;, score=0.661 total time=   4.3s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7076 - accuracy: 0.5125\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6801 - accuracy: 0.6237\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=2, n_nodes=32;, score=0.624 total time=   5.7s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6911 - accuracy: 0.5381\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6740 - accuracy: 0.6459\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=2, n_nodes=32;, score=0.646 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6858 - accuracy: 0.5654\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6375\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=2, n_nodes=32;, score=0.637 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6707 - accuracy: 0.6088\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6515 - accuracy: 0.6668\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=2, n_nodes=64;, score=0.667 total time=   5.0s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6731 - accuracy: 0.5934\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6424 - accuracy: 0.6535\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=2, n_nodes=64;, score=0.654 total time=   3.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6858 - accuracy: 0.5764\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6234\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=2, n_nodes=64;, score=0.623 total time=   5.3s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6670 - accuracy: 0.6122\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6391 - accuracy: 0.6684\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=2, n_nodes=128;, score=0.668 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6617 - accuracy: 0.6191\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6704\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=2, n_nodes=128;, score=0.670 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6684 - accuracy: 0.6090\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6367 - accuracy: 0.6730\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=2, n_nodes=128;, score=0.673 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6507 - accuracy: 0.6324\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6319 - accuracy: 0.6660\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=2, n_nodes=256;, score=0.666 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6577 - accuracy: 0.6215\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6700\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=2, n_nodes=256;, score=0.670 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6511 - accuracy: 0.6369\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6248 - accuracy: 0.6769\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=2, n_nodes=256;, score=0.677 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6409 - accuracy: 0.6498\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6346 - accuracy: 0.6532\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=2, n_nodes=512;, score=0.653 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6463 - accuracy: 0.6431\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6175 - accuracy: 0.6751\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=2, n_nodes=512;, score=0.675 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6423 - accuracy: 0.6475\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6664\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=2, n_nodes=512;, score=0.666 total time=   5.4s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6924 - accuracy: 0.5497\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.6547\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=3, n_nodes=32;, score=0.655 total time=   6.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6921 - accuracy: 0.5251\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6771 - accuracy: 0.6569\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=3, n_nodes=32;, score=0.657 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6842 - accuracy: 0.5658\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6480\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=3, n_nodes=32;, score=0.648 total time=   6.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6709 - accuracy: 0.6043\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6393 - accuracy: 0.6637\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=3, n_nodes=64;, score=0.664 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6860 - accuracy: 0.5617\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6623 - accuracy: 0.6645\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=3, n_nodes=64;, score=0.665 total time=   6.3s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6804 - accuracy: 0.5717\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6568 - accuracy: 0.6638\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=3, n_nodes=64;, score=0.664 total time=   6.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6618 - accuracy: 0.6092\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6490 - accuracy: 0.6264\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=3, n_nodes=128;, score=0.626 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6662 - accuracy: 0.6033\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6758\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=3, n_nodes=128;, score=0.676 total time=   6.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6618 - accuracy: 0.6159\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6725\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=3, n_nodes=128;, score=0.672 total time=   5.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6465 - accuracy: 0.6379\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6249 - accuracy: 0.6692\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=3, n_nodes=256;, score=0.669 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6505 - accuracy: 0.6327\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6171 - accuracy: 0.6759\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=3, n_nodes=256;, score=0.676 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6471 - accuracy: 0.6351\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6355 - accuracy: 0.6576\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=3, n_nodes=256;, score=0.658 total time=   6.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6362 - accuracy: 0.6506\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6642\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=3, n_nodes=512;, score=0.664 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6442 - accuracy: 0.6380\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6153 - accuracy: 0.6765\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=3, n_nodes=512;, score=0.676 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6397 - accuracy: 0.6454\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6451 - accuracy: 0.6501\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=3, n_nodes=512;, score=0.650 total time=   6.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6900 - accuracy: 0.5391\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6731 - accuracy: 0.6514\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=4, n_nodes=32;, score=0.651 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6868 - accuracy: 0.5664\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6645 - accuracy: 0.6623\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=4, n_nodes=32;, score=0.662 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6868 - accuracy: 0.5484\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6685 - accuracy: 0.6207\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=4, n_nodes=32;, score=0.621 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6795 - accuracy: 0.5736\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6443 - accuracy: 0.6623\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=4, n_nodes=64;, score=0.662 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6742 - accuracy: 0.5886\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6351 - accuracy: 0.6771\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=4, n_nodes=64;, score=0.677 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6734 - accuracy: 0.5873\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6302 - accuracy: 0.6720\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=4, n_nodes=64;, score=0.672 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6671 - accuracy: 0.5989\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6637\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=4, n_nodes=128;, score=0.664 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6619 - accuracy: 0.6058\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6147 - accuracy: 0.6790\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=4, n_nodes=128;, score=0.679 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6675 - accuracy: 0.6002\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6248 - accuracy: 0.6721\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=4, n_nodes=128;, score=0.672 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6477 - accuracy: 0.6313\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6603\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=4, n_nodes=256;, score=0.660 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6538 - accuracy: 0.6203\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6151 - accuracy: 0.6803\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=4, n_nodes=256;, score=0.680 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6484 - accuracy: 0.6344\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6221 - accuracy: 0.6668\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=4, n_nodes=256;, score=0.667 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6342 - accuracy: 0.6526\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.6697\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=4, n_nodes=512;, score=0.670 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6421 - accuracy: 0.6400\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6110 - accuracy: 0.6797\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=4, n_nodes=512;, score=0.680 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6361 - accuracy: 0.6496\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6158 - accuracy: 0.6782\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=4, n_nodes=512;, score=0.678 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6892 - accuracy: 0.5423\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6729 - accuracy: 0.6350\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=5, n_nodes=32;, score=0.635 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6899 - accuracy: 0.5445\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6730 - accuracy: 0.6338\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=5, n_nodes=32;, score=0.634 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6906 - accuracy: 0.5385\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6797 - accuracy: 0.5999\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=5, n_nodes=32;, score=0.600 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6811 - accuracy: 0.5665\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6385 - accuracy: 0.6662\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=5, n_nodes=64;, score=0.666 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6748 - accuracy: 0.5820\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6759\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=5, n_nodes=64;, score=0.676 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6825 - accuracy: 0.5739\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6435 - accuracy: 0.6630\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=5, n_nodes=64;, score=0.663 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6583 - accuracy: 0.6128\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6276 - accuracy: 0.6636\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=5, n_nodes=128;, score=0.664 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6680 - accuracy: 0.5932\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6173 - accuracy: 0.6784\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=5, n_nodes=128;, score=0.678 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6568 - accuracy: 0.6119\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6236 - accuracy: 0.6652\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=5, n_nodes=128;, score=0.665 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6453 - accuracy: 0.6380\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6605\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=5, n_nodes=256;, score=0.660 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6463 - accuracy: 0.6318\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6124 - accuracy: 0.6810\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=5, n_nodes=256;, score=0.681 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6451 - accuracy: 0.6320\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6714\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=5, n_nodes=256;, score=0.671 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6375 - accuracy: 0.6444\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6216 - accuracy: 0.6669\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=5, n_nodes=512;, score=0.667 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6435 - accuracy: 0.6383\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6230 - accuracy: 0.6708\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=5, n_nodes=512;, score=0.671 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6382 - accuracy: 0.6449\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6343 - accuracy: 0.6454\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=5, n_nodes=512;, score=0.645 total time=   7.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6915 - accuracy: 0.5338\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6840 - accuracy: 0.6235\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=6, n_nodes=32;, score=0.623 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6880 - accuracy: 0.5475\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6657 - accuracy: 0.6523\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=6, n_nodes=32;, score=0.652 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6900 - accuracy: 0.5424\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6714 - accuracy: 0.6536\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=6, n_nodes=32;, score=0.654 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6839 - accuracy: 0.5515\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6515 - accuracy: 0.6281\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=6, n_nodes=64;, score=0.628 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6821 - accuracy: 0.5619\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6389 - accuracy: 0.6704\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=6, n_nodes=64;, score=0.670 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6845 - accuracy: 0.5564\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6423 - accuracy: 0.6685\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=6, n_nodes=64;, score=0.668 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6621 - accuracy: 0.6026\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6661\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=6, n_nodes=128;, score=0.666 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6653 - accuracy: 0.5928\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6249 - accuracy: 0.6645\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=6, n_nodes=128;, score=0.665 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6603 - accuracy: 0.6057\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6218 - accuracy: 0.6733\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=6, n_nodes=128;, score=0.673 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6464 - accuracy: 0.6303\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6286 - accuracy: 0.6575\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=6, n_nodes=256;, score=0.657 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6507 - accuracy: 0.6208\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6120 - accuracy: 0.6804\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=6, n_nodes=256;, score=0.680 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6475 - accuracy: 0.6287\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6196 - accuracy: 0.6728\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=6, n_nodes=256;, score=0.673 total time=   8.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6388 - accuracy: 0.6449\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6560\n",
      "[CV 1/3] END dropout=0.1, lr=0.0001, n_layers=6, n_nodes=512;, score=0.656 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6447 - accuracy: 0.6330\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6197 - accuracy: 0.6737\n",
      "[CV 2/3] END dropout=0.1, lr=0.0001, n_layers=6, n_nodes=512;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6382 - accuracy: 0.6389\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6161 - accuracy: 0.6774\n",
      "[CV 3/3] END dropout=0.1, lr=0.0001, n_layers=6, n_nodes=512;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6554 - accuracy: 0.6274\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6675\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=1, n_nodes=32;, score=0.667 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6582 - accuracy: 0.6178\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6689\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=1, n_nodes=32;, score=0.669 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6555 - accuracy: 0.6208\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6264 - accuracy: 0.6721\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=1, n_nodes=32;, score=0.672 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6435 - accuracy: 0.6424\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6607\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=1, n_nodes=64;, score=0.661 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6444 - accuracy: 0.6402\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6163 - accuracy: 0.6776\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=1, n_nodes=64;, score=0.678 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6423 - accuracy: 0.6433\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6742\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=1, n_nodes=64;, score=0.674 total time=   4.7s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6438 - accuracy: 0.6449\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6702\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=1, n_nodes=128;, score=0.670 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6436 - accuracy: 0.6466\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6166 - accuracy: 0.6782\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=1, n_nodes=128;, score=0.678 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6381 - accuracy: 0.6527\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6198 - accuracy: 0.6771\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=1, n_nodes=128;, score=0.677 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6351 - accuracy: 0.6560\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6579\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=1, n_nodes=256;, score=0.658 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6422 - accuracy: 0.6434\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6244\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=1, n_nodes=256;, score=0.624 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6388 - accuracy: 0.6500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6244 - accuracy: 0.6675\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=1, n_nodes=256;, score=0.668 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6363 - accuracy: 0.6517\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6616\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=1, n_nodes=512;, score=0.662 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6419 - accuracy: 0.6443\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6190 - accuracy: 0.6800\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=1, n_nodes=512;, score=0.680 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6406 - accuracy: 0.6467\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6606\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=1, n_nodes=512;, score=0.661 total time=   5.0s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6449 - accuracy: 0.6374\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6683\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=2, n_nodes=32;, score=0.668 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6494 - accuracy: 0.6303\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6244 - accuracy: 0.6688\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=2, n_nodes=32;, score=0.669 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6490 - accuracy: 0.6347\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6369\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=2, n_nodes=32;, score=0.637 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6397 - accuracy: 0.6438\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6680\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=2, n_nodes=64;, score=0.668 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6436 - accuracy: 0.6400\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6250 - accuracy: 0.6625\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=2, n_nodes=64;, score=0.663 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6363 - accuracy: 0.6521\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6207 - accuracy: 0.6767\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=2, n_nodes=64;, score=0.677 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6395 - accuracy: 0.6464\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.6716\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=2, n_nodes=128;, score=0.672 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6435 - accuracy: 0.6381\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6148 - accuracy: 0.6785\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=2, n_nodes=128;, score=0.679 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6404 - accuracy: 0.6485\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6221 - accuracy: 0.6749\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=2, n_nodes=128;, score=0.675 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6334 - accuracy: 0.6532\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6590\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=2, n_nodes=256;, score=0.659 total time=   5.5s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6401 - accuracy: 0.6431\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6084 - accuracy: 0.6828\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=2, n_nodes=256;, score=0.683 total time=   6.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6344 - accuracy: 0.6546\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6210 - accuracy: 0.6719\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=2, n_nodes=256;, score=0.672 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6333 - accuracy: 0.6584\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6629\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=2, n_nodes=512;, score=0.663 total time=   5.3s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6374 - accuracy: 0.6504\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6148 - accuracy: 0.6839\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=2, n_nodes=512;, score=0.684 total time=   5.2s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6346 - accuracy: 0.6525\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6384 - accuracy: 0.6575\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=2, n_nodes=512;, score=0.658 total time=   5.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6506 - accuracy: 0.6239\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6275 - accuracy: 0.6532\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=3, n_nodes=32;, score=0.653 total time=   6.2s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6478 - accuracy: 0.6334\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6141 - accuracy: 0.6835\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=3, n_nodes=32;, score=0.684 total time=   6.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6477 - accuracy: 0.6351\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6198 - accuracy: 0.6760\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=3, n_nodes=32;, score=0.676 total time=   6.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6373 - accuracy: 0.6491\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6194 - accuracy: 0.6680\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=3, n_nodes=64;, score=0.668 total time=   5.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6406 - accuracy: 0.6451\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6157 - accuracy: 0.6741\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=3, n_nodes=64;, score=0.674 total time=   6.0s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6428 - accuracy: 0.6399\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6784\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=3, n_nodes=64;, score=0.678 total time=   5.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6348 - accuracy: 0.6521\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6249 - accuracy: 0.6688\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=3, n_nodes=128;, score=0.669 total time=   6.2s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6409 - accuracy: 0.6449\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6124 - accuracy: 0.6764\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=3, n_nodes=128;, score=0.676 total time=   6.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6343 - accuracy: 0.6514\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6581\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=3, n_nodes=128;, score=0.658 total time=   5.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6336 - accuracy: 0.6552\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6735\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=3, n_nodes=256;, score=0.673 total time=   6.3s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6403 - accuracy: 0.6463\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6135 - accuracy: 0.6824\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=3, n_nodes=256;, score=0.682 total time=   6.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6374 - accuracy: 0.6471\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6227 - accuracy: 0.6779\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=3, n_nodes=256;, score=0.678 total time=   6.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6376 - accuracy: 0.6542\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6720\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=3, n_nodes=512;, score=0.672 total time=   6.0s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6385 - accuracy: 0.6490\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6539\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=3, n_nodes=512;, score=0.654 total time=   5.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6360 - accuracy: 0.6485\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6401 - accuracy: 0.6348\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=3, n_nodes=512;, score=0.635 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6458 - accuracy: 0.6351\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6742\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=4, n_nodes=32;, score=0.674 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6508 - accuracy: 0.6239\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6160 - accuracy: 0.6773\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=4, n_nodes=32;, score=0.677 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6486 - accuracy: 0.6338\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6211 - accuracy: 0.6756\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=4, n_nodes=32;, score=0.676 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6379 - accuracy: 0.6484\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6728\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=4, n_nodes=64;, score=0.673 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6461 - accuracy: 0.6365\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6588\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=4, n_nodes=64;, score=0.659 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6384 - accuracy: 0.6468\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6782\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=4, n_nodes=64;, score=0.678 total time=   6.4s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6348 - accuracy: 0.6525\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6274 - accuracy: 0.6775\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=4, n_nodes=128;, score=0.678 total time=   7.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6429 - accuracy: 0.6396\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6237 - accuracy: 0.6721\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=4, n_nodes=128;, score=0.672 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6371 - accuracy: 0.6524\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6212 - accuracy: 0.6738\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=4, n_nodes=128;, score=0.674 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6382 - accuracy: 0.6492\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6729\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=4, n_nodes=256;, score=0.673 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6427 - accuracy: 0.6424\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6132 - accuracy: 0.6840\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=4, n_nodes=256;, score=0.684 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6366 - accuracy: 0.6480\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6203 - accuracy: 0.6746\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=4, n_nodes=256;, score=0.675 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6385 - accuracy: 0.6473\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6212 - accuracy: 0.6738\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=4, n_nodes=512;, score=0.674 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6399 - accuracy: 0.6510\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6162 - accuracy: 0.6801\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=4, n_nodes=512;, score=0.680 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6367 - accuracy: 0.6476\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6197 - accuracy: 0.6754\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=4, n_nodes=512;, score=0.675 total time=   6.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6501 - accuracy: 0.6295\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6242 - accuracy: 0.6698\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=5, n_nodes=32;, score=0.670 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6510 - accuracy: 0.6235\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6108 - accuracy: 0.6831\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=5, n_nodes=32;, score=0.683 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6516 - accuracy: 0.6246\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6354 - accuracy: 0.6464\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=5, n_nodes=32;, score=0.646 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6403 - accuracy: 0.6447\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6216 - accuracy: 0.6676\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=5, n_nodes=64;, score=0.668 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6483 - accuracy: 0.6301\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6857\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=5, n_nodes=64;, score=0.686 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6488 - accuracy: 0.6274\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6466\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=5, n_nodes=64;, score=0.647 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6387 - accuracy: 0.6466\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6525 - accuracy: 0.6239\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=5, n_nodes=128;, score=0.624 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6451 - accuracy: 0.6374\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6129 - accuracy: 0.6822\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=5, n_nodes=128;, score=0.682 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6405 - accuracy: 0.6456\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6171 - accuracy: 0.6764\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=5, n_nodes=128;, score=0.676 total time=   7.4s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6389 - accuracy: 0.6482\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6428 - accuracy: 0.6435\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=5, n_nodes=256;, score=0.644 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6439 - accuracy: 0.6405\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6591\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=5, n_nodes=256;, score=0.659 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6372 - accuracy: 0.6501\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6683\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=5, n_nodes=256;, score=0.668 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6390 - accuracy: 0.6489\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6226 - accuracy: 0.6740\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=5, n_nodes=512;, score=0.674 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6437 - accuracy: 0.6480\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6232 - accuracy: 0.6747\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=5, n_nodes=512;, score=0.675 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6386 - accuracy: 0.6484\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6518 - accuracy: 0.6154\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=5, n_nodes=512;, score=0.615 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6603 - accuracy: 0.6095\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6293 - accuracy: 0.6671\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=6, n_nodes=32;, score=0.667 total time=   7.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6587 - accuracy: 0.6121\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6145 - accuracy: 0.6825\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=6, n_nodes=32;, score=0.682 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6560 - accuracy: 0.6181\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6343 - accuracy: 0.6515\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=6, n_nodes=32;, score=0.652 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6440 - accuracy: 0.6353\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6253 - accuracy: 0.6693\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=6, n_nodes=64;, score=0.669 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6506 - accuracy: 0.6313\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6121 - accuracy: 0.6834\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=6, n_nodes=64;, score=0.683 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6402 - accuracy: 0.6494\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6236 - accuracy: 0.6740\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=6, n_nodes=64;, score=0.674 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6426 - accuracy: 0.6390\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6370 - accuracy: 0.6753\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=6, n_nodes=128;, score=0.675 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6425 - accuracy: 0.6443\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6348 - accuracy: 0.6621\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=6, n_nodes=128;, score=0.662 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6431 - accuracy: 0.6443\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6232 - accuracy: 0.6771\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=6, n_nodes=128;, score=0.677 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6430 - accuracy: 0.6368\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6244 - accuracy: 0.6691\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=6, n_nodes=256;, score=0.669 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6439 - accuracy: 0.6386\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6104 - accuracy: 0.6842\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=6, n_nodes=256;, score=0.684 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6429 - accuracy: 0.6422\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6277 - accuracy: 0.6608\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=6, n_nodes=256;, score=0.661 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6445 - accuracy: 0.6431\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6349 - accuracy: 0.6671\n",
      "[CV 1/3] END dropout=0.1, lr=0.001, n_layers=6, n_nodes=512;, score=0.667 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6450 - accuracy: 0.6438\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6497\n",
      "[CV 2/3] END dropout=0.1, lr=0.001, n_layers=6, n_nodes=512;, score=0.650 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6440 - accuracy: 0.6445\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6339 - accuracy: 0.6643\n",
      "[CV 3/3] END dropout=0.1, lr=0.001, n_layers=6, n_nodes=512;, score=0.664 total time=   8.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6449 - accuracy: 0.6296\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6451 - accuracy: 0.6255\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=1, n_nodes=32;, score=0.626 total time=   4.6s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6489 - accuracy: 0.6300\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6783\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=1, n_nodes=32;, score=0.678 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6448 - accuracy: 0.6393\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6182 - accuracy: 0.6758\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=1, n_nodes=32;, score=0.676 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6415 - accuracy: 0.6429\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6614\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=1, n_nodes=64;, score=0.661 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6456 - accuracy: 0.6409\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6230 - accuracy: 0.6699\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=1, n_nodes=64;, score=0.670 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6441 - accuracy: 0.6405\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6749\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=1, n_nodes=64;, score=0.675 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6411 - accuracy: 0.6479\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6201 - accuracy: 0.6728\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=1, n_nodes=128;, score=0.673 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6480 - accuracy: 0.6370\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6156 - accuracy: 0.6840\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=1, n_nodes=128;, score=0.684 total time=   5.6s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6406 - accuracy: 0.6475\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6188 - accuracy: 0.6771\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=1, n_nodes=128;, score=0.677 total time=   4.7s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6404 - accuracy: 0.6495\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6343 - accuracy: 0.6555\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=1, n_nodes=256;, score=0.656 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6427 - accuracy: 0.6432\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6269 - accuracy: 0.6547\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=1, n_nodes=256;, score=0.655 total time=   4.7s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6408 - accuracy: 0.6418\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6207 - accuracy: 0.6765\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=1, n_nodes=256;, score=0.677 total time=   4.7s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6449 - accuracy: 0.6412\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6212 - accuracy: 0.6723\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=1, n_nodes=512;, score=0.672 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6504 - accuracy: 0.6351\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6164 - accuracy: 0.6837\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=1, n_nodes=512;, score=0.684 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6466 - accuracy: 0.6438\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6070\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=1, n_nodes=512;, score=0.607 total time=   4.8s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6532 - accuracy: 0.6264\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6720\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=2, n_nodes=32;, score=0.672 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6631 - accuracy: 0.6096\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6367 - accuracy: 0.6556\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=2, n_nodes=32;, score=0.656 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6441 - accuracy: 0.6357\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6315 - accuracy: 0.6782\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=2, n_nodes=32;, score=0.678 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6527 - accuracy: 0.6317\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6796 - accuracy: 0.5771\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=2, n_nodes=64;, score=0.577 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6530 - accuracy: 0.6305\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6229 - accuracy: 0.6859\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=2, n_nodes=64;, score=0.686 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6476 - accuracy: 0.6275\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6533\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=2, n_nodes=64;, score=0.653 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6469 - accuracy: 0.6345\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6217 - accuracy: 0.6728\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=2, n_nodes=128;, score=0.673 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6478 - accuracy: 0.6357\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6158 - accuracy: 0.6848\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=2, n_nodes=128;, score=0.685 total time=   5.4s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6533 - accuracy: 0.6288\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6544\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=2, n_nodes=128;, score=0.654 total time=   5.1s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6505 - accuracy: 0.6338\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6708\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=2, n_nodes=256;, score=0.671 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6560 - accuracy: 0.6234\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6403\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=2, n_nodes=256;, score=0.640 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6958 - accuracy: 0.5062\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=2, n_nodes=256;, score=0.501 total time=   5.2s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6594 - accuracy: 0.6300\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6221\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=2, n_nodes=512;, score=0.622 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6611 - accuracy: 0.6178\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6232 - accuracy: 0.6799\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=2, n_nodes=512;, score=0.680 total time=   5.3s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6534 - accuracy: 0.6328\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6798\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=2, n_nodes=512;, score=0.680 total time=   5.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6933 - accuracy: 0.5018\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4973\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=3, n_nodes=32;, score=0.497 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6548 - accuracy: 0.6243\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6766\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=3, n_nodes=32;, score=0.677 total time=   6.2s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6558 - accuracy: 0.6155\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6358 - accuracy: 0.6756\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=3, n_nodes=32;, score=0.676 total time=   6.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6577 - accuracy: 0.6210\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6644\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=3, n_nodes=64;, score=0.664 total time=   6.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6564 - accuracy: 0.6258\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6857\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=3, n_nodes=64;, score=0.686 total time=   6.2s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6550 - accuracy: 0.6282\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6775\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=3, n_nodes=64;, score=0.678 total time=   5.9s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6544 - accuracy: 0.6262\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6774\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=3, n_nodes=128;, score=0.677 total time=   6.4s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6626 - accuracy: 0.6110\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6651\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=3, n_nodes=128;, score=0.665 total time=   5.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6578 - accuracy: 0.6217\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6768\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=3, n_nodes=128;, score=0.677 total time=   6.3s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6506 - accuracy: 0.6315\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6765\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=3, n_nodes=256;, score=0.676 total time=   5.9s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6609 - accuracy: 0.6203\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6400 - accuracy: 0.6501\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=3, n_nodes=256;, score=0.650 total time=   6.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6661 - accuracy: 0.6081\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6560\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=3, n_nodes=256;, score=0.656 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6985 - accuracy: 0.5320\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=3, n_nodes=512;, score=0.503 total time=   6.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6647 - accuracy: 0.6192\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6210 - accuracy: 0.6849\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=3, n_nodes=512;, score=0.685 total time=   5.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.7032 - accuracy: 0.5166\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6944 - accuracy: 0.4989\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=3, n_nodes=512;, score=0.499 total time=   5.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6916 - accuracy: 0.5108\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=4, n_nodes=32;, score=0.497 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6674 - accuracy: 0.5924\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6803\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=4, n_nodes=32;, score=0.680 total time=   6.5s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6923 - accuracy: 0.5081\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=4, n_nodes=32;, score=0.499 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6589 - accuracy: 0.6151\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6630\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=4, n_nodes=64;, score=0.663 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6660 - accuracy: 0.6098\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6814\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=4, n_nodes=64;, score=0.681 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6594 - accuracy: 0.6113\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6346 - accuracy: 0.6485\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=4, n_nodes=64;, score=0.649 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6941 - accuracy: 0.5031\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=4, n_nodes=128;, score=0.497 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6941 - accuracy: 0.5004\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=4, n_nodes=128;, score=0.501 total time=   6.5s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6919 - accuracy: 0.5062\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=4, n_nodes=128;, score=0.501 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6955 - accuracy: 0.4908\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=4, n_nodes=256;, score=0.497 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6938 - accuracy: 0.5047\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=4, n_nodes=256;, score=0.501 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6967 - accuracy: 0.4972\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=4, n_nodes=256;, score=0.501 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7004 - accuracy: 0.5006\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=4, n_nodes=512;, score=0.497 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7100 - accuracy: 0.4995\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=4, n_nodes=512;, score=0.499 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7053 - accuracy: 0.5026\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=4, n_nodes=512;, score=0.501 total time=   6.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6900 - accuracy: 0.5233\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=5, n_nodes=32;, score=0.497 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6668 - accuracy: 0.6055\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6775\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=5, n_nodes=32;, score=0.678 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6679 - accuracy: 0.6017\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6774\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=5, n_nodes=32;, score=0.677 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6923 - accuracy: 0.5077\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=5, n_nodes=64;, score=0.503 total time=   7.3s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6893 - accuracy: 0.5274\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=5, n_nodes=64;, score=0.501 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6884 - accuracy: 0.5231\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=5, n_nodes=64;, score=0.501 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6941 - accuracy: 0.4968\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6935 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=5, n_nodes=128;, score=0.497 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6937 - accuracy: 0.5075\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=5, n_nodes=128;, score=0.501 total time=   7.4s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6912 - accuracy: 0.5127\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.4987\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=5, n_nodes=128;, score=0.499 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6940 - accuracy: 0.5189\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=5, n_nodes=256;, score=0.503 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6766 - accuracy: 0.5895\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6315 - accuracy: 0.6606\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=5, n_nodes=256;, score=0.661 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6960 - accuracy: 0.5109\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=5, n_nodes=256;, score=0.501 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7200 - accuracy: 0.4989\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=5, n_nodes=512;, score=0.503 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7088 - accuracy: 0.5018\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=5, n_nodes=512;, score=0.501 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.7202 - accuracy: 0.5138\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=5, n_nodes=512;, score=0.499 total time=   6.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6919 - accuracy: 0.5062\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=6, n_nodes=32;, score=0.497 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6657 - accuracy: 0.6073\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.7334 - accuracy: 0.5393\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=6, n_nodes=32;, score=0.539 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6938 - accuracy: 0.5045\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=6, n_nodes=32;, score=0.501 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6934 - accuracy: 0.5020\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=6, n_nodes=64;, score=0.503 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6940 - accuracy: 0.4969\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=6, n_nodes=64;, score=0.501 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6654 - accuracy: 0.6094\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6559 - accuracy: 0.6448\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=6, n_nodes=64;, score=0.645 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6941 - accuracy: 0.4989\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6934 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=6, n_nodes=128;, score=0.497 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6651 - accuracy: 0.6088\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6838 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=6, n_nodes=128;, score=0.499 total time=   8.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6931 - accuracy: 0.5174\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=6, n_nodes=128;, score=0.501 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.7027 - accuracy: 0.4998\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6934 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=6, n_nodes=256;, score=0.497 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6952 - accuracy: 0.5013\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=6, n_nodes=256;, score=0.499 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6956 - accuracy: 0.5033\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6936 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=6, n_nodes=256;, score=0.499 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.7089 - accuracy: 0.4972\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.1, lr=0.01, n_layers=6, n_nodes=512;, score=0.497 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.7012 - accuracy: 0.5036\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.1, lr=0.01, n_layers=6, n_nodes=512;, score=0.501 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.7383 - accuracy: 0.5022\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6944 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.1, lr=0.01, n_layers=6, n_nodes=512;, score=0.499 total time=   8.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7060 - accuracy: 0.4948\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=1, n_nodes=32;, score=0.503 total time=   4.6s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7019 - accuracy: 0.4979\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=1, n_nodes=32;, score=0.499 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7114 - accuracy: 0.5007\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6945 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=1, n_nodes=32;, score=0.499 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7095 - accuracy: 0.5014\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=1, n_nodes=64;, score=0.503 total time=   4.7s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7187 - accuracy: 0.4963\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=1, n_nodes=64;, score=0.501 total time=   4.7s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7057 - accuracy: 0.5044\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6955 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=1, n_nodes=64;, score=0.499 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7412 - accuracy: 0.5014\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6954 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=1, n_nodes=128;, score=0.503 total time=   4.7s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7210 - accuracy: 0.4999\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6951 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=1, n_nodes=128;, score=0.499 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7188 - accuracy: 0.5020\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=1, n_nodes=128;, score=0.499 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7486 - accuracy: 0.5028\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=1, n_nodes=256;, score=0.497 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7532 - accuracy: 0.5013\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6979 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=1, n_nodes=256;, score=0.499 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7322 - accuracy: 0.5030\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6998 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=1, n_nodes=256;, score=0.499 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.8146 - accuracy: 0.5012\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6946 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=1, n_nodes=512;, score=0.503 total time=   4.7s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7807 - accuracy: 0.5075\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6953 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=1, n_nodes=512;, score=0.499 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.8333 - accuracy: 0.4960\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.7085 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=1, n_nodes=512;, score=0.499 total time=   4.8s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7033 - accuracy: 0.5001\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6943 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=2, n_nodes=32;, score=0.503 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7057 - accuracy: 0.4977\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=2, n_nodes=32;, score=0.501 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7014 - accuracy: 0.5001\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=2, n_nodes=32;, score=0.499 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7122 - accuracy: 0.5023\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=2, n_nodes=64;, score=0.503 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7278 - accuracy: 0.5003\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=2, n_nodes=64;, score=0.499 total time=   5.5s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.7146 - accuracy: 0.5018\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6975 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=2, n_nodes=64;, score=0.499 total time=   6.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7389 - accuracy: 0.4999\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=2, n_nodes=128;, score=0.497 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7422 - accuracy: 0.5006\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6962 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=2, n_nodes=128;, score=0.501 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7489 - accuracy: 0.5006\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=2, n_nodes=128;, score=0.499 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.9176 - accuracy: 0.4958\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=2, n_nodes=256;, score=0.503 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 1.0914 - accuracy: 0.4978\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6967 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=2, n_nodes=256;, score=0.501 total time=   5.3s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.9886 - accuracy: 0.5024\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=2, n_nodes=256;, score=0.499 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 2.1852 - accuracy: 0.5005\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=2, n_nodes=512;, score=0.497 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 1.9184 - accuracy: 0.4987\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6945 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=2, n_nodes=512;, score=0.501 total time=   5.3s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 1.1915 - accuracy: 0.5010\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=2, n_nodes=512;, score=0.499 total time=   5.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.7036 - accuracy: 0.4990\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6972 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=3, n_nodes=32;, score=0.503 total time=   6.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.7000 - accuracy: 0.4961\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6963 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=3, n_nodes=32;, score=0.499 total time=   6.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.7044 - accuracy: 0.4969\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=3, n_nodes=32;, score=0.501 total time=   6.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7558 - accuracy: 0.5003\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6959 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=3, n_nodes=64;, score=0.503 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7395 - accuracy: 0.4993\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=3, n_nodes=64;, score=0.501 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7487 - accuracy: 0.4982\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.7066 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=3, n_nodes=64;, score=0.501 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.8520 - accuracy: 0.5016\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6952 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=3, n_nodes=128;, score=0.503 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.8381 - accuracy: 0.5020\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.7073 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=3, n_nodes=128;, score=0.499 total time=   6.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.9340 - accuracy: 0.4997\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=3, n_nodes=128;, score=0.501 total time=   5.8s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 2.7143 - accuracy: 0.5044\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6951 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=3, n_nodes=256;, score=0.503 total time=   6.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 1.3276 - accuracy: 0.5051\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6948 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=3, n_nodes=256;, score=0.499 total time=   5.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 1.1805 - accuracy: 0.5033\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=3, n_nodes=256;, score=0.499 total time=   5.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 11.1618 - accuracy: 0.4998\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6962 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=3, n_nodes=512;, score=0.497 total time=   6.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 7.7858 - accuracy: 0.4983\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6968 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=3, n_nodes=512;, score=0.501 total time=   5.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 5.2003 - accuracy: 0.5010\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.7006 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=3, n_nodes=512;, score=0.501 total time=   5.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7046 - accuracy: 0.5001\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=4, n_nodes=32;, score=0.503 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6971 - accuracy: 0.4956\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=4, n_nodes=32;, score=0.501 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.7074 - accuracy: 0.5002\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6954 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=4, n_nodes=32;, score=0.501 total time=   7.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.8528 - accuracy: 0.5047\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=4, n_nodes=64;, score=0.503 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.8361 - accuracy: 0.4980\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6940 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=4, n_nodes=64;, score=0.501 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7299 - accuracy: 0.5049\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6975 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=4, n_nodes=64;, score=0.499 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 1.8463 - accuracy: 0.4994\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.7036 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=4, n_nodes=128;, score=0.497 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 1.4441 - accuracy: 0.4947\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6951 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=4, n_nodes=128;, score=0.501 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 1.0773 - accuracy: 0.5040\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.7046 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=4, n_nodes=128;, score=0.501 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 32.5210 - accuracy: 0.5033\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=4, n_nodes=256;, score=0.503 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 11.0425 - accuracy: 0.4995\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=4, n_nodes=256;, score=0.499 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 6.2474 - accuracy: 0.5010\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=4, n_nodes=256;, score=0.499 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 55.1004 - accuracy: 0.5029\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=4, n_nodes=512;, score=0.497 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 21.0826 - accuracy: 0.5028\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=4, n_nodes=512;, score=0.499 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 92.4886 - accuracy: 0.4976\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6998 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=4, n_nodes=512;, score=0.501 total time=   6.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6970 - accuracy: 0.5013\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6934 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=5, n_nodes=32;, score=0.503 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6984 - accuracy: 0.4979\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=5, n_nodes=32;, score=0.501 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6982 - accuracy: 0.5034\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6951 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=5, n_nodes=32;, score=0.499 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7385 - accuracy: 0.5004\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6946 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=5, n_nodes=64;, score=0.497 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7892 - accuracy: 0.5044\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=5, n_nodes=64;, score=0.501 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7305 - accuracy: 0.4982\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6935 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=5, n_nodes=64;, score=0.501 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 1.7304 - accuracy: 0.4965\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6937 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=5, n_nodes=128;, score=0.497 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 1.2873 - accuracy: 0.4998\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6934 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=5, n_nodes=128;, score=0.501 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 1.9621 - accuracy: 0.5024\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.7030 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=5, n_nodes=128;, score=0.499 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 43.1650 - accuracy: 0.5029\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=5, n_nodes=256;, score=0.503 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 41.6037 - accuracy: 0.5026\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6954 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=5, n_nodes=256;, score=0.501 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 28.3347 - accuracy: 0.4951\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=5, n_nodes=256;, score=0.499 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 222.1788 - accuracy: 0.4935\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6990 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=5, n_nodes=512;, score=0.497 total time=   7.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 1580.0117 - accuracy: 0.5030\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=5, n_nodes=512;, score=0.499 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 420.5585 - accuracy: 0.5053\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=5, n_nodes=512;, score=0.499 total time=   7.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.7172 - accuracy: 0.4987\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6942 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=6, n_nodes=32;, score=0.503 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.7218 - accuracy: 0.4975\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6934 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=6, n_nodes=32;, score=0.499 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.7026 - accuracy: 0.4997\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=6, n_nodes=32;, score=0.501 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.7470 - accuracy: 0.4996\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6936 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=6, n_nodes=64;, score=0.503 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.7255 - accuracy: 0.4998\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6938 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=6, n_nodes=64;, score=0.499 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.7577 - accuracy: 0.4990\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=6, n_nodes=64;, score=0.501 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 7.0991 - accuracy: 0.4943\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6933 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=6, n_nodes=128;, score=0.497 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 4.7702 - accuracy: 0.4983\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6949 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=6, n_nodes=128;, score=0.499 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 6.2892 - accuracy: 0.5014\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6940 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=6, n_nodes=128;, score=0.501 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 60.9051 - accuracy: 0.5035\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6933 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=6, n_nodes=256;, score=0.503 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 239.4604 - accuracy: 0.4985\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6959 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=6, n_nodes=256;, score=0.499 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 297.2555 - accuracy: 0.5026\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.7074 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=6, n_nodes=256;, score=0.501 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 3264.4543 - accuracy: 0.4993\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.1, lr=0.1, n_layers=6, n_nodes=512;, score=0.497 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 10582.7627 - accuracy: 0.5010\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.1, lr=0.1, n_layers=6, n_nodes=512;, score=0.501 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 4690.5054 - accuracy: 0.5024\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6945 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.1, lr=0.1, n_layers=6, n_nodes=512;, score=0.501 total time=   7.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6966 - accuracy: 0.5361\n",
      "354/354 [==============================] - 2s 3ms/step - loss: 0.6765 - accuracy: 0.5854\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=1, n_nodes=32;, score=0.585 total time=   5.5s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6906 - accuracy: 0.5617\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6605 - accuracy: 0.6509\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=1, n_nodes=32;, score=0.651 total time=   4.6s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6937 - accuracy: 0.5499\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6737 - accuracy: 0.6228\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=1, n_nodes=32;, score=0.623 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6909 - accuracy: 0.5485\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6741 - accuracy: 0.5907\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=1, n_nodes=64;, score=0.591 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6956 - accuracy: 0.5467\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6689 - accuracy: 0.6267\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=1, n_nodes=64;, score=0.627 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6877 - accuracy: 0.5604\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6674 - accuracy: 0.6064\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=1, n_nodes=64;, score=0.606 total time=   4.7s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6828 - accuracy: 0.5739\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6667\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=1, n_nodes=128;, score=0.667 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6852 - accuracy: 0.5597\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6736\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=1, n_nodes=128;, score=0.674 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6805 - accuracy: 0.5749\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6484\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=1, n_nodes=128;, score=0.648 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6711 - accuracy: 0.6045\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6496 - accuracy: 0.6677\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=1, n_nodes=256;, score=0.668 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6762 - accuracy: 0.5918\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6218\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=1, n_nodes=256;, score=0.622 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6759 - accuracy: 0.5928\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6709\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=1, n_nodes=256;, score=0.671 total time=   4.7s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6619 - accuracy: 0.6246\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6410 - accuracy: 0.6695\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=1, n_nodes=512;, score=0.669 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6657 - accuracy: 0.6194\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6387 - accuracy: 0.6765\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=1, n_nodes=512;, score=0.676 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6620 - accuracy: 0.6205\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6391 - accuracy: 0.6709\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=1, n_nodes=512;, score=0.671 total time=   4.7s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6930 - accuracy: 0.5284\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6775 - accuracy: 0.6277\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=2, n_nodes=32;, score=0.628 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6958 - accuracy: 0.5198\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6847 - accuracy: 0.6092\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=2, n_nodes=32;, score=0.609 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7141 - accuracy: 0.5166\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6882 - accuracy: 0.5733\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=2, n_nodes=32;, score=0.573 total time=   5.3s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6845 - accuracy: 0.5665\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6655 - accuracy: 0.6449\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=2, n_nodes=64;, score=0.645 total time=   5.3s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6813 - accuracy: 0.5742\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6601\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=2, n_nodes=64;, score=0.660 total time=   5.6s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6817 - accuracy: 0.5788\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6605 - accuracy: 0.6578\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=2, n_nodes=64;, score=0.658 total time=   5.2s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6748 - accuracy: 0.5858\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6466 - accuracy: 0.6611\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=2, n_nodes=128;, score=0.661 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6704 - accuracy: 0.6012\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6341 - accuracy: 0.6748\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=2, n_nodes=128;, score=0.675 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6727 - accuracy: 0.5939\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6686\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=2, n_nodes=128;, score=0.669 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6570 - accuracy: 0.6223\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6686\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=2, n_nodes=256;, score=0.669 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6613 - accuracy: 0.6183\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6662\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=2, n_nodes=256;, score=0.666 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6580 - accuracy: 0.6225\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6686\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=2, n_nodes=256;, score=0.669 total time=   5.2s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6444 - accuracy: 0.6440\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6647\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=2, n_nodes=512;, score=0.665 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6477 - accuracy: 0.6349\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6249 - accuracy: 0.6645\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=2, n_nodes=512;, score=0.665 total time=   5.3s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6435 - accuracy: 0.6407\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6226 - accuracy: 0.6734\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=2, n_nodes=512;, score=0.673 total time=   5.3s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6976 - accuracy: 0.5150\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6850 - accuracy: 0.6224\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=3, n_nodes=32;, score=0.622 total time=   5.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7000 - accuracy: 0.5363\n",
      "354/354 [==============================] - 2s 3ms/step - loss: 0.6724 - accuracy: 0.6449\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=3, n_nodes=32;, score=0.645 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6887 - accuracy: 0.5476\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6700 - accuracy: 0.6446\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=3, n_nodes=32;, score=0.645 total time=   6.2s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6891 - accuracy: 0.5445\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6623\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=3, n_nodes=64;, score=0.662 total time=   6.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6903 - accuracy: 0.5475\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6576 - accuracy: 0.6559\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=3, n_nodes=64;, score=0.656 total time=   6.2s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6817 - accuracy: 0.5668\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6574 - accuracy: 0.6460\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=3, n_nodes=64;, score=0.646 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6762 - accuracy: 0.5785\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6368 - accuracy: 0.6671\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=3, n_nodes=128;, score=0.667 total time=   6.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6725 - accuracy: 0.5868\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6419 - accuracy: 0.6349\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=3, n_nodes=128;, score=0.635 total time=   5.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6717 - accuracy: 0.5843\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6545\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=3, n_nodes=128;, score=0.655 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6568 - accuracy: 0.6220\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6681\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=3, n_nodes=256;, score=0.668 total time=   6.2s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6603 - accuracy: 0.6107\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6163 - accuracy: 0.6760\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=3, n_nodes=256;, score=0.676 total time=   6.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6578 - accuracy: 0.6175\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6216 - accuracy: 0.6749\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=3, n_nodes=256;, score=0.675 total time=   6.2s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6455 - accuracy: 0.6383\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6229 - accuracy: 0.6725\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=3, n_nodes=512;, score=0.672 total time=   6.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6463 - accuracy: 0.6353\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6102 - accuracy: 0.6843\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=3, n_nodes=512;, score=0.684 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6452 - accuracy: 0.6352\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6202 - accuracy: 0.6745\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=3, n_nodes=512;, score=0.674 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6987 - accuracy: 0.5085\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6884 - accuracy: 0.5801\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=4, n_nodes=32;, score=0.580 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6925 - accuracy: 0.5226\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6852 - accuracy: 0.6279\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=4, n_nodes=32;, score=0.628 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6948 - accuracy: 0.5116\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6862 - accuracy: 0.6434\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=4, n_nodes=32;, score=0.643 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6869 - accuracy: 0.5452\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6397\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=4, n_nodes=64;, score=0.640 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6916 - accuracy: 0.5290\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6753 - accuracy: 0.6431\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=4, n_nodes=64;, score=0.643 total time=   6.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6875 - accuracy: 0.5492\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6663\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=4, n_nodes=64;, score=0.666 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6838 - accuracy: 0.5578\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6432 - accuracy: 0.6585\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=4, n_nodes=128;, score=0.658 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6803 - accuracy: 0.5685\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6414 - accuracy: 0.6433\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=4, n_nodes=128;, score=0.643 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6793 - accuracy: 0.5663\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6686\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=4, n_nodes=128;, score=0.669 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6558 - accuracy: 0.6165\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6490 - accuracy: 0.6459\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=4, n_nodes=256;, score=0.646 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6601 - accuracy: 0.6068\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6759\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=4, n_nodes=256;, score=0.676 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6590 - accuracy: 0.6095\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6776\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=4, n_nodes=256;, score=0.678 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6463 - accuracy: 0.6317\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6229 - accuracy: 0.6689\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=4, n_nodes=512;, score=0.669 total time=   7.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6483 - accuracy: 0.6325\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6104 - accuracy: 0.6797\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=4, n_nodes=512;, score=0.680 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6473 - accuracy: 0.6268\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6640\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=4, n_nodes=512;, score=0.664 total time=   6.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6933 - accuracy: 0.5088\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6904 - accuracy: 0.5321\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=5, n_nodes=32;, score=0.532 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6935 - accuracy: 0.5165\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6891 - accuracy: 0.5514\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=5, n_nodes=32;, score=0.551 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6972 - accuracy: 0.5096\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6896 - accuracy: 0.6314\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=5, n_nodes=32;, score=0.631 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6919 - accuracy: 0.5276\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6809 - accuracy: 0.6027\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=5, n_nodes=64;, score=0.603 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6926 - accuracy: 0.5232\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6786 - accuracy: 0.6574\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=5, n_nodes=64;, score=0.657 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6923 - accuracy: 0.5253\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6799 - accuracy: 0.6220\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=5, n_nodes=64;, score=0.622 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6885 - accuracy: 0.5383\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6577\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=5, n_nodes=128;, score=0.658 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6833 - accuracy: 0.5527\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6652\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=5, n_nodes=128;, score=0.665 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6747 - accuracy: 0.5739\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6264 - accuracy: 0.6707\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=5, n_nodes=128;, score=0.671 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6601 - accuracy: 0.6044\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6685\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=5, n_nodes=256;, score=0.669 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6634 - accuracy: 0.6023\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6146 - accuracy: 0.6790\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=5, n_nodes=256;, score=0.679 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6649 - accuracy: 0.5932\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6218 - accuracy: 0.6752\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=5, n_nodes=256;, score=0.675 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6495 - accuracy: 0.6255\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6293 - accuracy: 0.6692\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=5, n_nodes=512;, score=0.669 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6535 - accuracy: 0.6182\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6157 - accuracy: 0.6771\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=5, n_nodes=512;, score=0.677 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6484 - accuracy: 0.6263\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6717\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=5, n_nodes=512;, score=0.672 total time=   7.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6944 - accuracy: 0.4996\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6925 - accuracy: 0.5129\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=6, n_nodes=32;, score=0.513 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6937 - accuracy: 0.5119\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6889 - accuracy: 0.6131\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=6, n_nodes=32;, score=0.613 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6929 - accuracy: 0.5163\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6886 - accuracy: 0.6344\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=6, n_nodes=32;, score=0.634 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6907 - accuracy: 0.5280\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6685 - accuracy: 0.6595\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=6, n_nodes=64;, score=0.660 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6925 - accuracy: 0.5223\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6804 - accuracy: 0.6165\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=6, n_nodes=64;, score=0.616 total time=   8.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6935 - accuracy: 0.5101\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6880 - accuracy: 0.6067\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=6, n_nodes=64;, score=0.607 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6825 - accuracy: 0.5579\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6387 - accuracy: 0.6577\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=6, n_nodes=128;, score=0.658 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6826 - accuracy: 0.5561\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6266 - accuracy: 0.6747\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=6, n_nodes=128;, score=0.675 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6798 - accuracy: 0.5670\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6334 - accuracy: 0.6585\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=6, n_nodes=128;, score=0.659 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6643 - accuracy: 0.5966\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6248 - accuracy: 0.6691\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=6, n_nodes=256;, score=0.669 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6654 - accuracy: 0.5929\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6138 - accuracy: 0.6824\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=6, n_nodes=256;, score=0.682 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6648 - accuracy: 0.5923\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6258 - accuracy: 0.6613\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=6, n_nodes=256;, score=0.661 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6477 - accuracy: 0.6302\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6216 - accuracy: 0.6695\n",
      "[CV 1/3] END dropout=0.2, lr=0.0001, n_layers=6, n_nodes=512;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6563 - accuracy: 0.6150\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6796\n",
      "[CV 2/3] END dropout=0.2, lr=0.0001, n_layers=6, n_nodes=512;, score=0.680 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6545 - accuracy: 0.6098\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6234 - accuracy: 0.6688\n",
      "[CV 3/3] END dropout=0.2, lr=0.0001, n_layers=6, n_nodes=512;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.6524 - accuracy: 0.6331\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6664\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=1, n_nodes=32;, score=0.666 total time=   4.5s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6633 - accuracy: 0.6144\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6615\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=1, n_nodes=32;, score=0.662 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6529 - accuracy: 0.6306\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6244 - accuracy: 0.6756\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=1, n_nodes=32;, score=0.676 total time=   4.9s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.6507 - accuracy: 0.6360\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6637\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=1, n_nodes=64;, score=0.664 total time=   4.6s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6576 - accuracy: 0.6201\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6183 - accuracy: 0.6787\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=1, n_nodes=64;, score=0.679 total time=   4.3s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6485 - accuracy: 0.6384\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6253 - accuracy: 0.6756\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=1, n_nodes=64;, score=0.676 total time=   4.7s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6397 - accuracy: 0.6501\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6310 - accuracy: 0.6579\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=1, n_nodes=128;, score=0.658 total time=   4.6s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6483 - accuracy: 0.6375\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6163 - accuracy: 0.6810\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=1, n_nodes=128;, score=0.681 total time=   4.7s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6426 - accuracy: 0.6416\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6706 - accuracy: 0.5877\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=1, n_nodes=128;, score=0.588 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6380 - accuracy: 0.6514\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6331 - accuracy: 0.6562\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=1, n_nodes=256;, score=0.656 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6440 - accuracy: 0.6414\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6157 - accuracy: 0.6819\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=1, n_nodes=256;, score=0.682 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6418 - accuracy: 0.6434\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6584\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=1, n_nodes=256;, score=0.658 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6378 - accuracy: 0.6491\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6710\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=1, n_nodes=512;, score=0.671 total time=   4.7s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6434 - accuracy: 0.6392\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6119 - accuracy: 0.6825\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=1, n_nodes=512;, score=0.682 total time=   4.7s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6407 - accuracy: 0.6472\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6234 - accuracy: 0.6741\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=1, n_nodes=512;, score=0.674 total time=   5.1s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6646 - accuracy: 0.6046\n",
      "354/354 [==============================] - 2s 3ms/step - loss: 0.6341 - accuracy: 0.6543\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=2, n_nodes=32;, score=0.654 total time=   6.1s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6559 - accuracy: 0.6209\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6220 - accuracy: 0.6716\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=2, n_nodes=32;, score=0.672 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6601 - accuracy: 0.6092\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6358 - accuracy: 0.6497\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=2, n_nodes=32;, score=0.650 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6430 - accuracy: 0.6409\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6744\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=2, n_nodes=64;, score=0.674 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6470 - accuracy: 0.6358\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6188 - accuracy: 0.6804\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=2, n_nodes=64;, score=0.680 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6449 - accuracy: 0.6364\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6090\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=2, n_nodes=64;, score=0.609 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6410 - accuracy: 0.6453\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6221 - accuracy: 0.6719\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=2, n_nodes=128;, score=0.672 total time=   5.7s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6431 - accuracy: 0.6403\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6238 - accuracy: 0.6702\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=2, n_nodes=128;, score=0.670 total time=   5.2s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6395 - accuracy: 0.6456\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6696\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=2, n_nodes=128;, score=0.670 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6371 - accuracy: 0.6536\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6225 - accuracy: 0.6700\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=2, n_nodes=256;, score=0.670 total time=   5.3s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6444 - accuracy: 0.6380\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6146 - accuracy: 0.6798\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=2, n_nodes=256;, score=0.680 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6396 - accuracy: 0.6485\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6195 - accuracy: 0.6727\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=2, n_nodes=256;, score=0.673 total time=   5.3s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6346 - accuracy: 0.6507\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6216 - accuracy: 0.6713\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=2, n_nodes=512;, score=0.671 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6443 - accuracy: 0.6366\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6362\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=2, n_nodes=512;, score=0.636 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6379 - accuracy: 0.6485\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6195 - accuracy: 0.6751\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=2, n_nodes=512;, score=0.675 total time=   5.3s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6605 - accuracy: 0.6059\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6637\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=3, n_nodes=32;, score=0.664 total time=   5.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6615 - accuracy: 0.6107\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6246 - accuracy: 0.6743\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=3, n_nodes=32;, score=0.674 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6575 - accuracy: 0.6179\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6246 - accuracy: 0.6712\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=3, n_nodes=32;, score=0.671 total time=   6.2s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6455 - accuracy: 0.6374\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6590\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=3, n_nodes=64;, score=0.659 total time=   6.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6507 - accuracy: 0.6304\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6168 - accuracy: 0.6822\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=3, n_nodes=64;, score=0.682 total time=   6.2s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6508 - accuracy: 0.6270\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6194 - accuracy: 0.6727\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=3, n_nodes=64;, score=0.673 total time=   5.9s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6403 - accuracy: 0.6462\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6207 - accuracy: 0.6728\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=3, n_nodes=128;, score=0.673 total time=   6.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6478 - accuracy: 0.6338\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6201 - accuracy: 0.6765\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=3, n_nodes=128;, score=0.676 total time=   5.8s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6429 - accuracy: 0.6396\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6216 - accuracy: 0.6739\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=3, n_nodes=128;, score=0.674 total time=   5.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6391 - accuracy: 0.6487\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6204 - accuracy: 0.6717\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=3, n_nodes=256;, score=0.672 total time=   6.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6442 - accuracy: 0.6395\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6125 - accuracy: 0.6851\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=3, n_nodes=256;, score=0.685 total time=   5.8s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6417 - accuracy: 0.6448\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6229 - accuracy: 0.6701\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=3, n_nodes=256;, score=0.670 total time=   5.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6372 - accuracy: 0.6489\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6736\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=3, n_nodes=512;, score=0.674 total time=   6.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6442 - accuracy: 0.6394\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6107 - accuracy: 0.6804\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=3, n_nodes=512;, score=0.680 total time=   5.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6378 - accuracy: 0.6494\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6297 - accuracy: 0.6642\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=3, n_nodes=512;, score=0.664 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6629 - accuracy: 0.6078\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6329 - accuracy: 0.6647\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=4, n_nodes=32;, score=0.665 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6575 - accuracy: 0.6180\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6167 - accuracy: 0.6827\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=4, n_nodes=32;, score=0.683 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6571 - accuracy: 0.6184\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6249 - accuracy: 0.6736\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=4, n_nodes=32;, score=0.674 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6462 - accuracy: 0.6345\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6647\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=4, n_nodes=64;, score=0.665 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6523 - accuracy: 0.6281\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6623\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=4, n_nodes=64;, score=0.662 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6514 - accuracy: 0.6264\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6251 - accuracy: 0.6749\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=4, n_nodes=64;, score=0.675 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6448 - accuracy: 0.6358\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6628\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=4, n_nodes=128;, score=0.663 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6480 - accuracy: 0.6326\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6598\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=4, n_nodes=128;, score=0.660 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6425 - accuracy: 0.6461\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6212 - accuracy: 0.6734\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=4, n_nodes=128;, score=0.673 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6392 - accuracy: 0.6474\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6229 - accuracy: 0.6554\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=4, n_nodes=256;, score=0.655 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6437 - accuracy: 0.6444\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6677\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=4, n_nodes=256;, score=0.668 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6452 - accuracy: 0.6357\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6766\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=4, n_nodes=256;, score=0.677 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6408 - accuracy: 0.6449\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6718\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=4, n_nodes=512;, score=0.672 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6447 - accuracy: 0.6400\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6737\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=4, n_nodes=512;, score=0.674 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6441 - accuracy: 0.6391\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6762\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=4, n_nodes=512;, score=0.676 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6735 - accuracy: 0.5752\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6590\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=5, n_nodes=32;, score=0.659 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6692 - accuracy: 0.5916\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6238 - accuracy: 0.6730\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=5, n_nodes=32;, score=0.673 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6662 - accuracy: 0.5992\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6216 - accuracy: 0.6762\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=5, n_nodes=32;, score=0.676 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6471 - accuracy: 0.6379\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6241 - accuracy: 0.6682\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=5, n_nodes=64;, score=0.668 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6581 - accuracy: 0.6190\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6212 - accuracy: 0.6749\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=5, n_nodes=64;, score=0.675 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6626 - accuracy: 0.5972\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6252 - accuracy: 0.6680\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=5, n_nodes=64;, score=0.668 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6445 - accuracy: 0.6402\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6336 - accuracy: 0.6611\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=5, n_nodes=128;, score=0.661 total time=   7.2s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6515 - accuracy: 0.6243\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6485 - accuracy: 0.6116\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=5, n_nodes=128;, score=0.612 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6439 - accuracy: 0.6388\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6240 - accuracy: 0.6722\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=5, n_nodes=128;, score=0.672 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6427 - accuracy: 0.6421\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6252 - accuracy: 0.6773\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=5, n_nodes=256;, score=0.677 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6502 - accuracy: 0.6315\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6145 - accuracy: 0.6812\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=5, n_nodes=256;, score=0.681 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6479 - accuracy: 0.6344\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6212 - accuracy: 0.6773\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=5, n_nodes=256;, score=0.677 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6423 - accuracy: 0.6428\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6435 - accuracy: 0.6388\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=5, n_nodes=512;, score=0.639 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6483 - accuracy: 0.6343\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6130 - accuracy: 0.6796\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=5, n_nodes=512;, score=0.680 total time=   7.4s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6423 - accuracy: 0.6438\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6253 - accuracy: 0.6731\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=5, n_nodes=512;, score=0.673 total time=   6.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6681 - accuracy: 0.5899\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6359 - accuracy: 0.6474\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=6, n_nodes=32;, score=0.647 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6691 - accuracy: 0.5883\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.6721\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=6, n_nodes=32;, score=0.672 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6751 - accuracy: 0.5697\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6251 - accuracy: 0.6676\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=6, n_nodes=32;, score=0.668 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6574 - accuracy: 0.6103\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6271 - accuracy: 0.6694\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=6, n_nodes=64;, score=0.669 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6569 - accuracy: 0.6208\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6202 - accuracy: 0.6834\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=6, n_nodes=64;, score=0.683 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6567 - accuracy: 0.6172\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6230 - accuracy: 0.6688\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=6, n_nodes=64;, score=0.669 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6479 - accuracy: 0.6377\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6232 - accuracy: 0.6738\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=6, n_nodes=128;, score=0.674 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6543 - accuracy: 0.6229\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6141 - accuracy: 0.6804\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=6, n_nodes=128;, score=0.680 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6499 - accuracy: 0.6295\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6324 - accuracy: 0.6733\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=6, n_nodes=128;, score=0.673 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6454 - accuracy: 0.6376\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6710\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=6, n_nodes=256;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6523 - accuracy: 0.6310\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6397 - accuracy: 0.6853\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=6, n_nodes=256;, score=0.685 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6474 - accuracy: 0.6384\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6368 - accuracy: 0.6764\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=6, n_nodes=256;, score=0.676 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6442 - accuracy: 0.6425\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6623\n",
      "[CV 1/3] END dropout=0.2, lr=0.001, n_layers=6, n_nodes=512;, score=0.662 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6568 - accuracy: 0.6199\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6610\n",
      "[CV 2/3] END dropout=0.2, lr=0.001, n_layers=6, n_nodes=512;, score=0.661 total time=   8.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6474 - accuracy: 0.6389\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6785\n",
      "[CV 3/3] END dropout=0.2, lr=0.001, n_layers=6, n_nodes=512;, score=0.678 total time=   7.7s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6463 - accuracy: 0.6389\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6379 - accuracy: 0.6566\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=1, n_nodes=32;, score=0.657 total time=   4.7s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6581 - accuracy: 0.6103\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6778\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=1, n_nodes=32;, score=0.678 total time=   4.6s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6553 - accuracy: 0.6257\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6608\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=1, n_nodes=32;, score=0.661 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6424 - accuracy: 0.6451\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6668\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=1, n_nodes=64;, score=0.667 total time=   4.6s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6478 - accuracy: 0.6304\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6151 - accuracy: 0.6842\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=1, n_nodes=64;, score=0.684 total time=   4.6s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6432 - accuracy: 0.6439\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6710\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=1, n_nodes=64;, score=0.671 total time=   4.7s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6481 - accuracy: 0.6357\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6237 - accuracy: 0.6726\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=1, n_nodes=128;, score=0.673 total time=   4.7s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6506 - accuracy: 0.6302\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6143 - accuracy: 0.6810\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=1, n_nodes=128;, score=0.681 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6512 - accuracy: 0.6338\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6230 - accuracy: 0.6715\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=1, n_nodes=128;, score=0.671 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6507 - accuracy: 0.6297\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6230 - accuracy: 0.6719\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=1, n_nodes=256;, score=0.672 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6513 - accuracy: 0.6312\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6120 - accuracy: 0.6827\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=1, n_nodes=256;, score=0.683 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6487 - accuracy: 0.6332\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.6748\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=1, n_nodes=256;, score=0.675 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6476 - accuracy: 0.6371\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6237 - accuracy: 0.6719\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=1, n_nodes=512;, score=0.672 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6499 - accuracy: 0.6341\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6737\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=1, n_nodes=512;, score=0.674 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6566 - accuracy: 0.6223\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6209 - accuracy: 0.6773\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=1, n_nodes=512;, score=0.677 total time=   4.7s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6934 - accuracy: 0.4998\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4976\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=2, n_nodes=32;, score=0.498 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6606 - accuracy: 0.6130\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6508 - accuracy: 0.6454\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=2, n_nodes=32;, score=0.645 total time=   5.4s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6595 - accuracy: 0.6104\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6423 - accuracy: 0.6631\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=2, n_nodes=32;, score=0.663 total time=   5.2s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6511 - accuracy: 0.6214\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6391 - accuracy: 0.6588\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=2, n_nodes=64;, score=0.659 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6605 - accuracy: 0.6049\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6469\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=2, n_nodes=64;, score=0.647 total time=   5.4s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6628 - accuracy: 0.6093\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6400 - accuracy: 0.6796\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=2, n_nodes=64;, score=0.680 total time=   4.7s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6638 - accuracy: 0.6039\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6728\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=2, n_nodes=128;, score=0.673 total time=   3.8s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6570 - accuracy: 0.6097\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6173 - accuracy: 0.6783\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=2, n_nodes=128;, score=0.678 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6534 - accuracy: 0.6290\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6610\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=2, n_nodes=128;, score=0.661 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6589 - accuracy: 0.6205\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6590\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=2, n_nodes=256;, score=0.659 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6651 - accuracy: 0.6111\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6817\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=2, n_nodes=256;, score=0.682 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6604 - accuracy: 0.6148\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6232 - accuracy: 0.6750\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=2, n_nodes=256;, score=0.675 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6572 - accuracy: 0.6129\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6435 - accuracy: 0.6469\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=2, n_nodes=512;, score=0.647 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6741 - accuracy: 0.5941\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6441 - accuracy: 0.6245\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=2, n_nodes=512;, score=0.625 total time=   5.5s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6628 - accuracy: 0.6215\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6381 - accuracy: 0.6775\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=2, n_nodes=512;, score=0.678 total time=   6.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6570 - accuracy: 0.6129\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6731\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=3, n_nodes=32;, score=0.673 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6788 - accuracy: 0.5672\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6814\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=3, n_nodes=32;, score=0.681 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6775 - accuracy: 0.5688\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6696\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=3, n_nodes=32;, score=0.670 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6756 - accuracy: 0.5751\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.4981\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=3, n_nodes=64;, score=0.498 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6700 - accuracy: 0.5930\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6458 - accuracy: 0.6550\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=3, n_nodes=64;, score=0.655 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6654 - accuracy: 0.6096\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6486 - accuracy: 0.6450\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=3, n_nodes=64;, score=0.645 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6619 - accuracy: 0.6138\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6292 - accuracy: 0.6603\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=3, n_nodes=128;, score=0.660 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6729 - accuracy: 0.5901\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6843\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=3, n_nodes=128;, score=0.684 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6682 - accuracy: 0.6037\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6765\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=3, n_nodes=128;, score=0.677 total time=   6.2s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6901 - accuracy: 0.5342\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6939 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=3, n_nodes=256;, score=0.503 total time=   5.9s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6861 - accuracy: 0.5532\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.5017\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=3, n_nodes=256;, score=0.502 total time=   5.6s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6971 - accuracy: 0.5031\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=3, n_nodes=256;, score=0.499 total time=   4.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6782 - accuracy: 0.6060\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6726\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=3, n_nodes=512;, score=0.673 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6983 - accuracy: 0.5156\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=3, n_nodes=512;, score=0.499 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7008 - accuracy: 0.4996\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=3, n_nodes=512;, score=0.499 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6931 - accuracy: 0.5054\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=4, n_nodes=32;, score=0.503 total time=   5.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6757 - accuracy: 0.5746\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6466 - accuracy: 0.6797\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=4, n_nodes=32;, score=0.680 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6730 - accuracy: 0.5946\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6676\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=4, n_nodes=32;, score=0.668 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6661 - accuracy: 0.6017\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6556\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=4, n_nodes=64;, score=0.656 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6713 - accuracy: 0.5914\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6192 - accuracy: 0.6841\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=4, n_nodes=64;, score=0.684 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6940 - accuracy: 0.5012\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=4, n_nodes=64;, score=0.499 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6759 - accuracy: 0.5881\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6354\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=4, n_nodes=128;, score=0.635 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6938 - accuracy: 0.5060\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=4, n_nodes=128;, score=0.501 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6805 - accuracy: 0.5737\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=4, n_nodes=128;, score=0.499 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6954 - accuracy: 0.5071\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=4, n_nodes=256;, score=0.503 total time=   6.3s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6954 - accuracy: 0.5071\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=4, n_nodes=256;, score=0.501 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6944 - accuracy: 0.4999\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=4, n_nodes=256;, score=0.501 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7205 - accuracy: 0.4972\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=4, n_nodes=512;, score=0.503 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7137 - accuracy: 0.5110\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6946 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=4, n_nodes=512;, score=0.501 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7066 - accuracy: 0.5065\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=4, n_nodes=512;, score=0.501 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6935 - accuracy: 0.5005\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=5, n_nodes=32;, score=0.503 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6938 - accuracy: 0.5010\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=5, n_nodes=32;, score=0.501 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6939 - accuracy: 0.5072\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=5, n_nodes=32;, score=0.501 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6939 - accuracy: 0.5027\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=5, n_nodes=64;, score=0.497 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6939 - accuracy: 0.4970\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=5, n_nodes=64;, score=0.499 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6927 - accuracy: 0.5057\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=5, n_nodes=64;, score=0.499 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6948 - accuracy: 0.4995\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=5, n_nodes=128;, score=0.503 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6942 - accuracy: 0.4979\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=5, n_nodes=128;, score=0.501 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6933 - accuracy: 0.5065\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6946 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=5, n_nodes=128;, score=0.499 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6950 - accuracy: 0.5007\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=5, n_nodes=256;, score=0.497 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6965 - accuracy: 0.5006\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=5, n_nodes=256;, score=0.499 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6980 - accuracy: 0.4978\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=5, n_nodes=256;, score=0.501 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7038 - accuracy: 0.4981\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=5, n_nodes=512;, score=0.497 total time=   6.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7092 - accuracy: 0.4975\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=5, n_nodes=512;, score=0.499 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7028 - accuracy: 0.4973\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=5, n_nodes=512;, score=0.501 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6937 - accuracy: 0.5008\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=6, n_nodes=32;, score=0.497 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6937 - accuracy: 0.5026\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=6, n_nodes=32;, score=0.499 total time=   7.4s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6938 - accuracy: 0.4970\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=6, n_nodes=32;, score=0.501 total time=   8.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6940 - accuracy: 0.5028\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=6, n_nodes=64;, score=0.503 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6942 - accuracy: 0.5011\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=6, n_nodes=64;, score=0.499 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6942 - accuracy: 0.4996\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=6, n_nodes=64;, score=0.501 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6942 - accuracy: 0.5019\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=6, n_nodes=128;, score=0.503 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6934 - accuracy: 0.5042\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=6, n_nodes=128;, score=0.501 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6941 - accuracy: 0.4988\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=6, n_nodes=128;, score=0.501 total time=   7.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6976 - accuracy: 0.4988\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=6, n_nodes=256;, score=0.497 total time=   6.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6967 - accuracy: 0.4987\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=6, n_nodes=256;, score=0.499 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6958 - accuracy: 0.5020\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=6, n_nodes=256;, score=0.501 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7603 - accuracy: 0.4965\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.2, lr=0.01, n_layers=6, n_nodes=512;, score=0.503 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7098 - accuracy: 0.5002\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.01, n_layers=6, n_nodes=512;, score=0.501 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7275 - accuracy: 0.4996\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6949 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.01, n_layers=6, n_nodes=512;, score=0.499 total time=   7.5s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.7019 - accuracy: 0.5080\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.7011 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=1, n_nodes=32;, score=0.497 total time=   4.5s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6997 - accuracy: 0.5061\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=1, n_nodes=32;, score=0.499 total time=   4.5s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.7082 - accuracy: 0.4976\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6972 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=1, n_nodes=32;, score=0.501 total time=   4.5s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.7094 - accuracy: 0.5009\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=1, n_nodes=64;, score=0.503 total time=   4.4s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7096 - accuracy: 0.5006\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=1, n_nodes=64;, score=0.499 total time=   4.5s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.7100 - accuracy: 0.5041\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6973 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=1, n_nodes=64;, score=0.501 total time=   4.5s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.7182 - accuracy: 0.5035\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.7120 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=1, n_nodes=128;, score=0.497 total time=   4.3s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.7081 - accuracy: 0.5997\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6251 - accuracy: 0.6902\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=1, n_nodes=128;, score=0.690 total time=   4.4s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.7237 - accuracy: 0.4978\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=1, n_nodes=128;, score=0.499 total time=   4.5s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.7753 - accuracy: 0.5025\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6976 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=1, n_nodes=256;, score=0.497 total time=   4.4s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.7391 - accuracy: 0.5009\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.4987\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=1, n_nodes=256;, score=0.499 total time=   4.4s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.7520 - accuracy: 0.5026\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=1, n_nodes=256;, score=0.501 total time=   4.5s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.8190 - accuracy: 0.5015\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=1, n_nodes=512;, score=0.503 total time=   4.3s\n",
      "708/708 [==============================] - 4s 4ms/step - loss: 0.8091 - accuracy: 0.5142\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6945 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=1, n_nodes=512;, score=0.499 total time=   4.6s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.8013 - accuracy: 0.5099\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=1, n_nodes=512;, score=0.501 total time=   3.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7008 - accuracy: 0.5013\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=2, n_nodes=32;, score=0.497 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7031 - accuracy: 0.4993\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=2, n_nodes=32;, score=0.501 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6996 - accuracy: 0.4966\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.7001 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=2, n_nodes=32;, score=0.501 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7096 - accuracy: 0.4991\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6953 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=2, n_nodes=64;, score=0.503 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7319 - accuracy: 0.5010\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=2, n_nodes=64;, score=0.501 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7062 - accuracy: 0.4974\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=2, n_nodes=64;, score=0.499 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7302 - accuracy: 0.5059\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=2, n_nodes=128;, score=0.503 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.8360 - accuracy: 0.4975\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6943 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=2, n_nodes=128;, score=0.499 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7436 - accuracy: 0.4966\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=2, n_nodes=128;, score=0.499 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.9913 - accuracy: 0.5012\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=2, n_nodes=256;, score=0.497 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.9060 - accuracy: 0.5039\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=2, n_nodes=256;, score=0.501 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.8805 - accuracy: 0.5001\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=2, n_nodes=256;, score=0.499 total time=   5.1s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 1.3806 - accuracy: 0.4977\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=2, n_nodes=512;, score=0.503 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 2.1388 - accuracy: 0.5001\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=2, n_nodes=512;, score=0.501 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 1.4707 - accuracy: 0.4977\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6951 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=2, n_nodes=512;, score=0.499 total time=   5.1s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7064 - accuracy: 0.5012\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6954 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=3, n_nodes=32;, score=0.497 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7008 - accuracy: 0.5007\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6953 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=3, n_nodes=32;, score=0.501 total time=   5.7s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7230 - accuracy: 0.4953\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6939 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=3, n_nodes=32;, score=0.499 total time=   5.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.7164 - accuracy: 0.5024\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6949 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=3, n_nodes=64;, score=0.497 total time=   5.8s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7245 - accuracy: 0.4957\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=3, n_nodes=64;, score=0.501 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7195 - accuracy: 0.5012\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=3, n_nodes=64;, score=0.501 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.8036 - accuracy: 0.4985\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6949 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=3, n_nodes=128;, score=0.497 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.8187 - accuracy: 0.5011\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6945 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=3, n_nodes=128;, score=0.501 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7887 - accuracy: 0.4984\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=3, n_nodes=128;, score=0.499 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 1.6801 - accuracy: 0.5047\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6963 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=3, n_nodes=256;, score=0.503 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 1.6659 - accuracy: 0.4975\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=3, n_nodes=256;, score=0.501 total time=   5.5s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 1.6651 - accuracy: 0.5101\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=3, n_nodes=256;, score=0.499 total time=   6.4s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 15.5287 - accuracy: 0.4946\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6982 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=3, n_nodes=512;, score=0.497 total time=   5.7s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 13.0630 - accuracy: 0.4959\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=3, n_nodes=512;, score=0.501 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 6.0824 - accuracy: 0.4973\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6951 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=3, n_nodes=512;, score=0.499 total time=   5.2s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7003 - accuracy: 0.5075\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=4, n_nodes=32;, score=0.503 total time=   5.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6999 - accuracy: 0.5002\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=4, n_nodes=32;, score=0.501 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6989 - accuracy: 0.4999\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=4, n_nodes=32;, score=0.499 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7304 - accuracy: 0.4942\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=4, n_nodes=64;, score=0.497 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7490 - accuracy: 0.5047\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6957 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=4, n_nodes=64;, score=0.501 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7284 - accuracy: 0.4975\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=4, n_nodes=64;, score=0.501 total time=   6.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 2.2313 - accuracy: 0.5038\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=4, n_nodes=128;, score=0.497 total time=   5.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.8126 - accuracy: 0.5053\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6935 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=4, n_nodes=128;, score=0.501 total time=   5.7s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 1.0772 - accuracy: 0.4983\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6945 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=4, n_nodes=128;, score=0.499 total time=   5.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 3.0371 - accuracy: 0.5032\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6975 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=4, n_nodes=256;, score=0.497 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 2.6317 - accuracy: 0.5060\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6966 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=4, n_nodes=256;, score=0.501 total time=   6.3s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 3.6851 - accuracy: 0.4979\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=4, n_nodes=256;, score=0.499 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 125.8275 - accuracy: 0.4950\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6988 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=4, n_nodes=512;, score=0.503 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 150.0431 - accuracy: 0.5085\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6987 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=4, n_nodes=512;, score=0.501 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 80.9363 - accuracy: 0.5004\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6944 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=4, n_nodes=512;, score=0.501 total time=   6.3s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.7079 - accuracy: 0.4979\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6939 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=5, n_nodes=32;, score=0.503 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.7001 - accuracy: 0.4936\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6959 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=5, n_nodes=32;, score=0.501 total time=   6.8s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.7013 - accuracy: 0.5010\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6951 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=5, n_nodes=32;, score=0.501 total time=   5.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7770 - accuracy: 0.5003\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6939 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=5, n_nodes=64;, score=0.497 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.8270 - accuracy: 0.5017\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6961 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=5, n_nodes=64;, score=0.501 total time=   6.8s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.8781 - accuracy: 0.5038\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6991 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=5, n_nodes=64;, score=0.499 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 1.4919 - accuracy: 0.4970\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=5, n_nodes=128;, score=0.497 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 2.2247 - accuracy: 0.4941\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=5, n_nodes=128;, score=0.501 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 1.4025 - accuracy: 0.4985\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=5, n_nodes=128;, score=0.501 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 23.8175 - accuracy: 0.4988\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=5, n_nodes=256;, score=0.497 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 30.8390 - accuracy: 0.4982\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6939 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=5, n_nodes=256;, score=0.501 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 17.2634 - accuracy: 0.4962\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=5, n_nodes=256;, score=0.501 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 139.8468 - accuracy: 0.4958\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6965 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=5, n_nodes=512;, score=0.503 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 816.9182 - accuracy: 0.4993\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6967 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=5, n_nodes=512;, score=0.501 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 390.4722 - accuracy: 0.4947\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=5, n_nodes=512;, score=0.499 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7025 - accuracy: 0.5017\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=6, n_nodes=32;, score=0.497 total time=   6.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7066 - accuracy: 0.4977\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.7023 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=6, n_nodes=32;, score=0.501 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6987 - accuracy: 0.5023\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=6, n_nodes=32;, score=0.499 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7387 - accuracy: 0.4947\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6975 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=6, n_nodes=64;, score=0.497 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7463 - accuracy: 0.4918\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=6, n_nodes=64;, score=0.501 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7683 - accuracy: 0.5058\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6940 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=6, n_nodes=64;, score=0.501 total time=   7.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 2.1806 - accuracy: 0.5002\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=6, n_nodes=128;, score=0.497 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 2.9848 - accuracy: 0.4990\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=6, n_nodes=128;, score=0.501 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 2.9601 - accuracy: 0.5037\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6960 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=6, n_nodes=128;, score=0.499 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 114.7744 - accuracy: 0.5034\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=6, n_nodes=256;, score=0.503 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 206.6900 - accuracy: 0.4990\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6959 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=6, n_nodes=256;, score=0.499 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 12.3556 - accuracy: 0.5034\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6993 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=6, n_nodes=256;, score=0.499 total time=   7.3s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 6613.3267 - accuracy: 0.4921\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.2, lr=0.1, n_layers=6, n_nodes=512;, score=0.503 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 2087.9734 - accuracy: 0.4994\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6966 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.2, lr=0.1, n_layers=6, n_nodes=512;, score=0.499 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 2300.7053 - accuracy: 0.5021\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.2, lr=0.1, n_layers=6, n_nodes=512;, score=0.501 total time=   7.2s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6965 - accuracy: 0.5272\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6831 - accuracy: 0.6296\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=1, n_nodes=32;, score=0.630 total time=   4.4s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6894 - accuracy: 0.5460\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6716 - accuracy: 0.6320\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=1, n_nodes=32;, score=0.632 total time=   4.5s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6937 - accuracy: 0.5344\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6807 - accuracy: 0.6360\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=1, n_nodes=32;, score=0.636 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7062 - accuracy: 0.5379\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6756 - accuracy: 0.6453\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=1, n_nodes=64;, score=0.645 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6873 - accuracy: 0.5519\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6677 - accuracy: 0.6713\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=1, n_nodes=64;, score=0.671 total time=   5.0s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.6901 - accuracy: 0.5623\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6689 - accuracy: 0.6626\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=1, n_nodes=64;, score=0.663 total time=   4.4s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6841 - accuracy: 0.5793\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6558\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=1, n_nodes=128;, score=0.656 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6869 - accuracy: 0.5578\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6594 - accuracy: 0.6589\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=1, n_nodes=128;, score=0.659 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6820 - accuracy: 0.5750\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6567\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=1, n_nodes=128;, score=0.657 total time=   4.9s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6769 - accuracy: 0.5856\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6546 - accuracy: 0.6688\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=1, n_nodes=256;, score=0.669 total time=   4.1s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6739 - accuracy: 0.5912\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6310\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=1, n_nodes=256;, score=0.631 total time=   3.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6792 - accuracy: 0.5814\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6520\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=1, n_nodes=256;, score=0.652 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6692 - accuracy: 0.6045\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6463 - accuracy: 0.6435\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=1, n_nodes=512;, score=0.644 total time=   4.6s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6684 - accuracy: 0.6110\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6413 - accuracy: 0.6762\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=1, n_nodes=512;, score=0.676 total time=   4.5s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.6676 - accuracy: 0.6152\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6427 - accuracy: 0.6752\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=1, n_nodes=512;, score=0.675 total time=   4.5s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7041 - accuracy: 0.5103\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6807 - accuracy: 0.6078\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=2, n_nodes=32;, score=0.608 total time=   5.0s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6992 - accuracy: 0.5277\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6807 - accuracy: 0.6451\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=2, n_nodes=32;, score=0.645 total time=   4.4s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7123 - accuracy: 0.5226\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6818 - accuracy: 0.6180\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=2, n_nodes=32;, score=0.618 total time=   4.8s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6895 - accuracy: 0.5495\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6691 - accuracy: 0.6517\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=2, n_nodes=64;, score=0.652 total time=   5.4s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6963 - accuracy: 0.5368\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6630 - accuracy: 0.6421\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=2, n_nodes=64;, score=0.642 total time=   5.0s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6946 - accuracy: 0.5512\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6581\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=2, n_nodes=64;, score=0.658 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6821 - accuracy: 0.5716\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6484 - accuracy: 0.6621\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=2, n_nodes=128;, score=0.662 total time=   5.6s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6840 - accuracy: 0.5705\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6494 - accuracy: 0.6532\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=2, n_nodes=128;, score=0.653 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6833 - accuracy: 0.5691\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6505 - accuracy: 0.6582\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=2, n_nodes=128;, score=0.658 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6656 - accuracy: 0.6069\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6547\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=2, n_nodes=256;, score=0.655 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6665 - accuracy: 0.6039\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6385 - accuracy: 0.6370\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=2, n_nodes=256;, score=0.637 total time=   5.2s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6677 - accuracy: 0.5956\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.6710\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=2, n_nodes=256;, score=0.671 total time=   5.4s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6473 - accuracy: 0.6363\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6414\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=2, n_nodes=512;, score=0.641 total time=   5.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6524 - accuracy: 0.6306\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6234 - accuracy: 0.6657\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=2, n_nodes=512;, score=0.666 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6520 - accuracy: 0.6304\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6616\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=2, n_nodes=512;, score=0.662 total time=   5.0s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7057 - accuracy: 0.5066\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5807\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=3, n_nodes=32;, score=0.581 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7069 - accuracy: 0.5122\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6881 - accuracy: 0.5796\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=3, n_nodes=32;, score=0.580 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7009 - accuracy: 0.5146\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6848 - accuracy: 0.6316\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=3, n_nodes=32;, score=0.632 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6975 - accuracy: 0.5356\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6795 - accuracy: 0.5945\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=3, n_nodes=64;, score=0.595 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6943 - accuracy: 0.5287\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6744 - accuracy: 0.6414\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=3, n_nodes=64;, score=0.641 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7036 - accuracy: 0.5265\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6785 - accuracy: 0.6317\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=3, n_nodes=64;, score=0.632 total time=   5.5s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6833 - accuracy: 0.5611\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6526 - accuracy: 0.6297\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=3, n_nodes=128;, score=0.630 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6842 - accuracy: 0.5606\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6670\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=3, n_nodes=128;, score=0.667 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6888 - accuracy: 0.5465\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6743\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=3, n_nodes=128;, score=0.674 total time=   4.6s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6648 - accuracy: 0.6026\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6319 - accuracy: 0.6591\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=3, n_nodes=256;, score=0.659 total time=   5.3s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6687 - accuracy: 0.5973\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6638\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=3, n_nodes=256;, score=0.664 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6748 - accuracy: 0.5833\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6686\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=3, n_nodes=256;, score=0.669 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6529 - accuracy: 0.6250\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6264 - accuracy: 0.6651\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=3, n_nodes=512;, score=0.665 total time=   5.7s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6549 - accuracy: 0.6219\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6143 - accuracy: 0.6834\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=3, n_nodes=512;, score=0.683 total time=   5.7s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6557 - accuracy: 0.6172\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6227 - accuracy: 0.6704\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=3, n_nodes=512;, score=0.670 total time=   5.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6979 - accuracy: 0.5054\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6895 - accuracy: 0.5823\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=4, n_nodes=32;, score=0.582 total time=   6.2s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6952 - accuracy: 0.5086\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5645\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=4, n_nodes=32;, score=0.564 total time=   5.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6970 - accuracy: 0.5081\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6877 - accuracy: 0.6255\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=4, n_nodes=32;, score=0.625 total time=   6.4s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6996 - accuracy: 0.5117\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6817 - accuracy: 0.6547\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=4, n_nodes=64;, score=0.655 total time=   5.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7001 - accuracy: 0.5052\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6891 - accuracy: 0.6048\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=4, n_nodes=64;, score=0.605 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6963 - accuracy: 0.5150\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6824 - accuracy: 0.6631\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=4, n_nodes=64;, score=0.663 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6901 - accuracy: 0.5369\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6617 - accuracy: 0.6504\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=4, n_nodes=128;, score=0.650 total time=   6.3s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6876 - accuracy: 0.5436\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6710\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=4, n_nodes=128;, score=0.671 total time=   5.4s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6915 - accuracy: 0.5329\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6613 - accuracy: 0.6611\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=4, n_nodes=128;, score=0.661 total time=   7.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6667 - accuracy: 0.5974\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6713\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=4, n_nodes=256;, score=0.671 total time=   5.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6754 - accuracy: 0.5760\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6203 - accuracy: 0.6848\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=4, n_nodes=256;, score=0.685 total time=   6.3s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6708 - accuracy: 0.5881\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6286 - accuracy: 0.6648\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=4, n_nodes=256;, score=0.665 total time=   6.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6559 - accuracy: 0.6177\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6702\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=4, n_nodes=512;, score=0.670 total time=   6.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6595 - accuracy: 0.6082\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6184 - accuracy: 0.6764\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=4, n_nodes=512;, score=0.676 total time=   5.9s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6608 - accuracy: 0.6033\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.6712\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=4, n_nodes=512;, score=0.671 total time=   5.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7010 - accuracy: 0.5065\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5616\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=5, n_nodes=32;, score=0.562 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6986 - accuracy: 0.5036\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5592\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=5, n_nodes=32;, score=0.559 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6968 - accuracy: 0.5087\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.6090\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=5, n_nodes=32;, score=0.609 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6968 - accuracy: 0.5066\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6906 - accuracy: 0.5644\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=5, n_nodes=64;, score=0.564 total time=   6.6s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6974 - accuracy: 0.5053\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6900 - accuracy: 0.6217\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=5, n_nodes=64;, score=0.622 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6977 - accuracy: 0.5060\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6890 - accuracy: 0.5854\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=5, n_nodes=64;, score=0.585 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6929 - accuracy: 0.5284\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6778 - accuracy: 0.6608\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=5, n_nodes=128;, score=0.661 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6928 - accuracy: 0.5253\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6730 - accuracy: 0.6676\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=5, n_nodes=128;, score=0.668 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6946 - accuracy: 0.5113\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6887 - accuracy: 0.5567\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=5, n_nodes=128;, score=0.557 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6834 - accuracy: 0.5503\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6554\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=5, n_nodes=256;, score=0.655 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6857 - accuracy: 0.5481\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6343 - accuracy: 0.6572\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=5, n_nodes=256;, score=0.657 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6765 - accuracy: 0.5686\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6739\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=5, n_nodes=256;, score=0.674 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6590 - accuracy: 0.6105\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6244 - accuracy: 0.6648\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=5, n_nodes=512;, score=0.665 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6620 - accuracy: 0.6015\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6225 - accuracy: 0.6698\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=5, n_nodes=512;, score=0.670 total time=   6.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6582 - accuracy: 0.6063\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6241 - accuracy: 0.6726\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=5, n_nodes=512;, score=0.673 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6948 - accuracy: 0.5107\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5776\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=6, n_nodes=32;, score=0.578 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6964 - accuracy: 0.5079\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6905 - accuracy: 0.6527\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=6, n_nodes=32;, score=0.653 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6954 - accuracy: 0.4980\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6927 - accuracy: 0.5019\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=6, n_nodes=32;, score=0.502 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6972 - accuracy: 0.5021\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=6, n_nodes=64;, score=0.503 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6951 - accuracy: 0.5075\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6906 - accuracy: 0.5265\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=6, n_nodes=64;, score=0.526 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6963 - accuracy: 0.5028\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.6040\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=6, n_nodes=64;, score=0.604 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6957 - accuracy: 0.5029\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.5850\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=6, n_nodes=128;, score=0.585 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6943 - accuracy: 0.5073\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.6570\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=6, n_nodes=128;, score=0.657 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6935 - accuracy: 0.5155\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6861 - accuracy: 0.5653\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=6, n_nodes=128;, score=0.565 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6859 - accuracy: 0.5442\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6341 - accuracy: 0.6610\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=6, n_nodes=256;, score=0.661 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6873 - accuracy: 0.5390\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6518\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=6, n_nodes=256;, score=0.652 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6872 - accuracy: 0.5382\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6355 - accuracy: 0.6544\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=6, n_nodes=256;, score=0.654 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6585 - accuracy: 0.6067\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6277 - accuracy: 0.6593\n",
      "[CV 1/3] END dropout=0.3, lr=0.0001, n_layers=6, n_nodes=512;, score=0.659 total time=   7.3s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6678 - accuracy: 0.5845\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6154 - accuracy: 0.6776\n",
      "[CV 2/3] END dropout=0.3, lr=0.0001, n_layers=6, n_nodes=512;, score=0.678 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6636 - accuracy: 0.5977\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6205 - accuracy: 0.6773\n",
      "[CV 3/3] END dropout=0.3, lr=0.0001, n_layers=6, n_nodes=512;, score=0.677 total time=   6.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6526 - accuracy: 0.6274\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6598\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=1, n_nodes=32;, score=0.660 total time=   4.5s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6704 - accuracy: 0.6013\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6804\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=1, n_nodes=32;, score=0.680 total time=   4.4s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.6569 - accuracy: 0.6301\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6230 - accuracy: 0.6779\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=1, n_nodes=32;, score=0.678 total time=   4.4s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.6492 - accuracy: 0.6378\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6696\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=1, n_nodes=64;, score=0.670 total time=   4.6s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6532 - accuracy: 0.6277\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6382 - accuracy: 0.6452\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=1, n_nodes=64;, score=0.645 total time=   4.3s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6539 - accuracy: 0.6299\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6695\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=1, n_nodes=64;, score=0.669 total time=   4.4s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6494 - accuracy: 0.6391\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6694\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=1, n_nodes=128;, score=0.669 total time=   4.4s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.6497 - accuracy: 0.6352\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6234 - accuracy: 0.6688\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=1, n_nodes=128;, score=0.669 total time=   4.4s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.6482 - accuracy: 0.6386\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6236 - accuracy: 0.6718\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=1, n_nodes=128;, score=0.672 total time=   4.4s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6427 - accuracy: 0.6409\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6734\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=1, n_nodes=256;, score=0.673 total time=   4.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6455 - accuracy: 0.6398\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6160 - accuracy: 0.6780\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=1, n_nodes=256;, score=0.678 total time=   5.2s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.6435 - accuracy: 0.6460\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6420 - accuracy: 0.6429\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=1, n_nodes=256;, score=0.643 total time=   4.5s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6411 - accuracy: 0.6441\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6250 - accuracy: 0.6722\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=1, n_nodes=512;, score=0.672 total time=   3.8s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6420 - accuracy: 0.6444\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6177 - accuracy: 0.6816\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=1, n_nodes=512;, score=0.682 total time=   3.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6412 - accuracy: 0.6438\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6746\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=1, n_nodes=512;, score=0.675 total time=   4.6s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6607 - accuracy: 0.6146\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6365 - accuracy: 0.6542\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=2, n_nodes=32;, score=0.654 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6641 - accuracy: 0.6118\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6768\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=2, n_nodes=32;, score=0.677 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6562 - accuracy: 0.6179\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6331 - accuracy: 0.6650\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=2, n_nodes=32;, score=0.665 total time=   5.3s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6483 - accuracy: 0.6330\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6232 - accuracy: 0.6693\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=2, n_nodes=64;, score=0.669 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6589 - accuracy: 0.6140\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6225 - accuracy: 0.6846\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=2, n_nodes=64;, score=0.685 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6504 - accuracy: 0.6272\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6205 - accuracy: 0.6754\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=2, n_nodes=64;, score=0.675 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6455 - accuracy: 0.6402\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6354 - accuracy: 0.6504\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=2, n_nodes=128;, score=0.650 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6484 - accuracy: 0.6320\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6143 - accuracy: 0.6824\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=2, n_nodes=128;, score=0.682 total time=   5.1s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6461 - accuracy: 0.6348\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6555 - accuracy: 0.6386\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=2, n_nodes=128;, score=0.639 total time=   4.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6400 - accuracy: 0.6452\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6706\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=2, n_nodes=256;, score=0.671 total time=   4.7s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6466 - accuracy: 0.6367\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6173 - accuracy: 0.6767\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=2, n_nodes=256;, score=0.677 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6404 - accuracy: 0.6423\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6754\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=2, n_nodes=256;, score=0.675 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6406 - accuracy: 0.6458\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6237 - accuracy: 0.6723\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=2, n_nodes=512;, score=0.672 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6449 - accuracy: 0.6406\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6148 - accuracy: 0.6816\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=2, n_nodes=512;, score=0.682 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6417 - accuracy: 0.6424\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6207 - accuracy: 0.6722\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=2, n_nodes=512;, score=0.672 total time=   5.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6670 - accuracy: 0.5943\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.6666\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=3, n_nodes=32;, score=0.667 total time=   5.7s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6632 - accuracy: 0.6049\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6221 - accuracy: 0.6779\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=3, n_nodes=32;, score=0.678 total time=   5.5s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6623 - accuracy: 0.6067\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6265 - accuracy: 0.6728\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=3, n_nodes=32;, score=0.673 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6610 - accuracy: 0.6089\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6684\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=3, n_nodes=64;, score=0.668 total time=   4.9s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6579 - accuracy: 0.6157\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6817\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=3, n_nodes=64;, score=0.682 total time=   5.5s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6567 - accuracy: 0.6212\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6773\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=3, n_nodes=64;, score=0.677 total time=   4.5s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6457 - accuracy: 0.6312\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6567\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=3, n_nodes=128;, score=0.657 total time=   5.7s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6490 - accuracy: 0.6345\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6691\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=3, n_nodes=128;, score=0.669 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6475 - accuracy: 0.6318\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6201 - accuracy: 0.6722\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=3, n_nodes=128;, score=0.672 total time=   5.5s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6427 - accuracy: 0.6433\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.6711\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=3, n_nodes=256;, score=0.671 total time=   6.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6504 - accuracy: 0.6287\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6157 - accuracy: 0.6775\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=3, n_nodes=256;, score=0.678 total time=   5.6s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6453 - accuracy: 0.6405\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6399 - accuracy: 0.6443\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=3, n_nodes=256;, score=0.644 total time=   5.8s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6431 - accuracy: 0.6408\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6428 - accuracy: 0.6429\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=3, n_nodes=512;, score=0.643 total time=   5.5s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6444 - accuracy: 0.6395\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6227 - accuracy: 0.6831\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=3, n_nodes=512;, score=0.683 total time=   5.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6410 - accuracy: 0.6447\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6750\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=3, n_nodes=512;, score=0.675 total time=   5.9s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6676 - accuracy: 0.6040\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6454 - accuracy: 0.6470\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=4, n_nodes=32;, score=0.647 total time=   5.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6867 - accuracy: 0.5437\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6742\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=4, n_nodes=32;, score=0.674 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6712 - accuracy: 0.5873\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6399 - accuracy: 0.6544\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=4, n_nodes=32;, score=0.654 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6636 - accuracy: 0.6013\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6715\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=4, n_nodes=64;, score=0.672 total time=   6.2s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6629 - accuracy: 0.6085\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.6735\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=4, n_nodes=64;, score=0.673 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6652 - accuracy: 0.6040\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6203 - accuracy: 0.6783\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=4, n_nodes=64;, score=0.678 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6487 - accuracy: 0.6353\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6237 - accuracy: 0.6711\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=4, n_nodes=128;, score=0.671 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6533 - accuracy: 0.6211\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6570 - accuracy: 0.6018\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=4, n_nodes=128;, score=0.602 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6526 - accuracy: 0.6252\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6208 - accuracy: 0.6770\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=4, n_nodes=128;, score=0.677 total time=   6.2s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6458 - accuracy: 0.6366\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6562\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=4, n_nodes=256;, score=0.656 total time=   6.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6524 - accuracy: 0.6270\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6168 - accuracy: 0.6811\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=4, n_nodes=256;, score=0.681 total time=   5.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6467 - accuracy: 0.6373\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6726\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=4, n_nodes=256;, score=0.673 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6441 - accuracy: 0.6420\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6696\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=4, n_nodes=512;, score=0.670 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6489 - accuracy: 0.6336\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6284 - accuracy: 0.6552\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=4, n_nodes=512;, score=0.655 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6495 - accuracy: 0.6346\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6767\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=4, n_nodes=512;, score=0.677 total time=   6.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6744 - accuracy: 0.5788\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6589\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=5, n_nodes=32;, score=0.659 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6769 - accuracy: 0.5729\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6761\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=5, n_nodes=32;, score=0.676 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6770 - accuracy: 0.5763\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6296 - accuracy: 0.6702\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=5, n_nodes=32;, score=0.670 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6567 - accuracy: 0.6227\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6728\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=5, n_nodes=64;, score=0.673 total time=   6.8s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6666 - accuracy: 0.5983\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6785\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=5, n_nodes=64;, score=0.679 total time=   6.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6676 - accuracy: 0.5889\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6207 - accuracy: 0.6727\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=5, n_nodes=64;, score=0.673 total time=   5.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6542 - accuracy: 0.6189\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6462 - accuracy: 0.6476\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=5, n_nodes=128;, score=0.648 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6603 - accuracy: 0.6120\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6170 - accuracy: 0.6784\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=5, n_nodes=128;, score=0.678 total time=   6.4s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6554 - accuracy: 0.6207\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6241 - accuracy: 0.6700\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=5, n_nodes=128;, score=0.670 total time=   5.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6467 - accuracy: 0.6352\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6241 - accuracy: 0.6721\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=5, n_nodes=256;, score=0.672 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6552 - accuracy: 0.6191\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6124 - accuracy: 0.6829\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=5, n_nodes=256;, score=0.683 total time=   6.4s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6487 - accuracy: 0.6324\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6751\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=5, n_nodes=256;, score=0.675 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6465 - accuracy: 0.6376\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6676\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=5, n_nodes=512;, score=0.668 total time=   7.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6524 - accuracy: 0.6298\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6606\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=5, n_nodes=512;, score=0.661 total time=   5.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6501 - accuracy: 0.6321\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6653\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=5, n_nodes=512;, score=0.665 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6808 - accuracy: 0.5596\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6346 - accuracy: 0.6693\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=6, n_nodes=32;, score=0.669 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6937 - accuracy: 0.5043\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6925 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=6, n_nodes=32;, score=0.499 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6759 - accuracy: 0.5831\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6501 - accuracy: 0.6484\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=6, n_nodes=32;, score=0.648 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6660 - accuracy: 0.6000\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6689\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=6, n_nodes=64;, score=0.669 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6744 - accuracy: 0.5780\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6250 - accuracy: 0.6779\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=6, n_nodes=64;, score=0.678 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6788 - accuracy: 0.5642\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6421 - accuracy: 0.6727\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=6, n_nodes=64;, score=0.673 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6610 - accuracy: 0.6055\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6339 - accuracy: 0.6657\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=6, n_nodes=128;, score=0.666 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6755 - accuracy: 0.5702\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6280 - accuracy: 0.6662\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=6, n_nodes=128;, score=0.666 total time=   7.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6635 - accuracy: 0.5996\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6215 - accuracy: 0.6690\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=6, n_nodes=128;, score=0.669 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6577 - accuracy: 0.6181\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6712\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=6, n_nodes=256;, score=0.671 total time=   6.6s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6588 - accuracy: 0.6145\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6252 - accuracy: 0.6787\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=6, n_nodes=256;, score=0.679 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6565 - accuracy: 0.6211\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6778\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=6, n_nodes=256;, score=0.678 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6552 - accuracy: 0.6295\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6713\n",
      "[CV 1/3] END dropout=0.3, lr=0.001, n_layers=6, n_nodes=512;, score=0.671 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6569 - accuracy: 0.6231\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6288 - accuracy: 0.6573\n",
      "[CV 2/3] END dropout=0.3, lr=0.001, n_layers=6, n_nodes=512;, score=0.657 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6486 - accuracy: 0.6331\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6666\n",
      "[CV 3/3] END dropout=0.3, lr=0.001, n_layers=6, n_nodes=512;, score=0.667 total time=   7.3s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6691 - accuracy: 0.5907\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6464\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=1, n_nodes=32;, score=0.646 total time=   3.9s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6572 - accuracy: 0.6078\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6197 - accuracy: 0.6819\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=1, n_nodes=32;, score=0.682 total time=   4.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6655 - accuracy: 0.5970\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6484 - accuracy: 0.6620\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=1, n_nodes=32;, score=0.662 total time=   4.6s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.6592 - accuracy: 0.6167\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6460\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=1, n_nodes=64;, score=0.646 total time=   4.5s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6574 - accuracy: 0.6142\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6773\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=1, n_nodes=64;, score=0.677 total time=   4.4s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6501 - accuracy: 0.6311\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6361 - accuracy: 0.6781\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=1, n_nodes=64;, score=0.678 total time=   4.6s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.6505 - accuracy: 0.6312\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6343 - accuracy: 0.6718\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=1, n_nodes=128;, score=0.672 total time=   4.5s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6564 - accuracy: 0.6204\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6365 - accuracy: 0.6834\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=1, n_nodes=128;, score=0.683 total time=   4.5s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.6531 - accuracy: 0.6279\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6772\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=1, n_nodes=128;, score=0.677 total time=   4.5s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.6523 - accuracy: 0.6299\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6762\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=1, n_nodes=256;, score=0.676 total time=   4.6s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6590 - accuracy: 0.6203\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6310 - accuracy: 0.6691\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=1, n_nodes=256;, score=0.669 total time=   4.3s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6518 - accuracy: 0.6282\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.6819\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=1, n_nodes=256;, score=0.682 total time=   4.7s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6499 - accuracy: 0.6368\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6357 - accuracy: 0.6490\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=1, n_nodes=512;, score=0.649 total time=   3.8s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6545 - accuracy: 0.6297\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6198 - accuracy: 0.6809\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=1, n_nodes=512;, score=0.681 total time=   4.0s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.6517 - accuracy: 0.6344\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6730\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=1, n_nodes=512;, score=0.673 total time=   4.5s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6942 - accuracy: 0.5033\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=2, n_nodes=32;, score=0.503 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6797 - accuracy: 0.5723\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6797\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=2, n_nodes=32;, score=0.680 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6941 - accuracy: 0.5000\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=2, n_nodes=32;, score=0.501 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6690 - accuracy: 0.6000\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6768\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=2, n_nodes=64;, score=0.677 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6746 - accuracy: 0.5835\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6605\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=2, n_nodes=64;, score=0.660 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6744 - accuracy: 0.5868\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6575\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=2, n_nodes=64;, score=0.657 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6689 - accuracy: 0.5857\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6662\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=2, n_nodes=128;, score=0.666 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6912 - accuracy: 0.5181\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5017\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=2, n_nodes=128;, score=0.502 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6761 - accuracy: 0.5744\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6746 - accuracy: 0.5875\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=2, n_nodes=128;, score=0.587 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6808 - accuracy: 0.5718\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6620\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=2, n_nodes=256;, score=0.662 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6922 - accuracy: 0.5152\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5013\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=2, n_nodes=256;, score=0.501 total time=   5.0s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6964 - accuracy: 0.5004\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=2, n_nodes=256;, score=0.501 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6961 - accuracy: 0.5185\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.4981\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=2, n_nodes=512;, score=0.498 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6965 - accuracy: 0.5141\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6945 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=2, n_nodes=512;, score=0.501 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6767 - accuracy: 0.5876\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6405 - accuracy: 0.6732\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=2, n_nodes=512;, score=0.673 total time=   5.2s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6939 - accuracy: 0.5064\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=3, n_nodes=32;, score=0.497 total time=   5.6s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6938 - accuracy: 0.4950\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=3, n_nodes=32;, score=0.501 total time=   5.8s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6925 - accuracy: 0.5097\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=3, n_nodes=32;, score=0.501 total time=   5.7s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6819 - accuracy: 0.5656\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6981 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=3, n_nodes=64;, score=0.503 total time=   5.4s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6930 - accuracy: 0.5143\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=3, n_nodes=64;, score=0.499 total time=   4.4s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6916 - accuracy: 0.5083\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=3, n_nodes=64;, score=0.501 total time=   5.5s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6766 - accuracy: 0.5836\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6457 - accuracy: 0.6661\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=3, n_nodes=128;, score=0.666 total time=   4.7s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6949 - accuracy: 0.5028\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=3, n_nodes=128;, score=0.499 total time=   5.6s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6853 - accuracy: 0.5452\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6782\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=3, n_nodes=128;, score=0.678 total time=   5.9s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6733 - accuracy: 0.5914\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6432\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=3, n_nodes=256;, score=0.643 total time=   5.8s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6932 - accuracy: 0.5204\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=3, n_nodes=256;, score=0.501 total time=   6.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6794 - accuracy: 0.5745\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6577\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=3, n_nodes=256;, score=0.658 total time=   5.8s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6981 - accuracy: 0.5098\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=3, n_nodes=512;, score=0.503 total time=   5.8s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6922 - accuracy: 0.5703\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6867\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=3, n_nodes=512;, score=0.687 total time=   5.8s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7091 - accuracy: 0.5069\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=3, n_nodes=512;, score=0.499 total time=   5.2s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6766 - accuracy: 0.5792\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6838 - accuracy: 0.5630\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=4, n_nodes=32;, score=0.563 total time=   5.7s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6935 - accuracy: 0.5082\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=4, n_nodes=32;, score=0.501 total time=   5.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6938 - accuracy: 0.5051\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=4, n_nodes=32;, score=0.501 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6937 - accuracy: 0.5064\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=4, n_nodes=64;, score=0.503 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6942 - accuracy: 0.5022\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=4, n_nodes=64;, score=0.499 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6942 - accuracy: 0.4990\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=4, n_nodes=64;, score=0.501 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6923 - accuracy: 0.5106\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=4, n_nodes=128;, score=0.497 total time=   6.4s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6942 - accuracy: 0.5011\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4984\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=4, n_nodes=128;, score=0.498 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6824 - accuracy: 0.5669\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6577\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=4, n_nodes=128;, score=0.658 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6962 - accuracy: 0.5021\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6944 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=4, n_nodes=256;, score=0.497 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6961 - accuracy: 0.4980\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=4, n_nodes=256;, score=0.499 total time=   5.9s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6952 - accuracy: 0.5031\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=4, n_nodes=256;, score=0.501 total time=   5.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7156 - accuracy: 0.4957\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=4, n_nodes=512;, score=0.497 total time=   6.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.7068 - accuracy: 0.5021\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=4, n_nodes=512;, score=0.499 total time=   5.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6989 - accuracy: 0.5015\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=4, n_nodes=512;, score=0.501 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6942 - accuracy: 0.4946\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=5, n_nodes=32;, score=0.497 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6938 - accuracy: 0.5073\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=5, n_nodes=32;, score=0.501 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6926 - accuracy: 0.5050\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=5, n_nodes=32;, score=0.499 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6942 - accuracy: 0.5063\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=5, n_nodes=64;, score=0.497 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6939 - accuracy: 0.5050\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=5, n_nodes=64;, score=0.501 total time=   6.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6940 - accuracy: 0.5049\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=5, n_nodes=64;, score=0.499 total time=   5.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6945 - accuracy: 0.4927\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=5, n_nodes=128;, score=0.503 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6847 - accuracy: 0.5649\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6739 - accuracy: 0.6632\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=5, n_nodes=128;, score=0.663 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6944 - accuracy: 0.5024\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=5, n_nodes=128;, score=0.501 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6974 - accuracy: 0.5025\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=5, n_nodes=256;, score=0.503 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6965 - accuracy: 0.5047\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=5, n_nodes=256;, score=0.501 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6955 - accuracy: 0.4978\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=5, n_nodes=256;, score=0.499 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7295 - accuracy: 0.4983\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=5, n_nodes=512;, score=0.503 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.7100 - accuracy: 0.5026\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=5, n_nodes=512;, score=0.501 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6994 - accuracy: 0.4978\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=5, n_nodes=512;, score=0.501 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6939 - accuracy: 0.5040\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=6, n_nodes=32;, score=0.503 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6935 - accuracy: 0.5045\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=6, n_nodes=32;, score=0.499 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6944 - accuracy: 0.4924\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=6, n_nodes=32;, score=0.501 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6944 - accuracy: 0.4986\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=6, n_nodes=64;, score=0.497 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6942 - accuracy: 0.4999\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6931 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=6, n_nodes=64;, score=0.501 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6944 - accuracy: 0.4996\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=6, n_nodes=64;, score=0.499 total time=   7.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6948 - accuracy: 0.5059\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=6, n_nodes=128;, score=0.497 total time=   6.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6944 - accuracy: 0.5016\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=6, n_nodes=128;, score=0.499 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6951 - accuracy: 0.4995\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6934 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=6, n_nodes=128;, score=0.501 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6963 - accuracy: 0.5005\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=6, n_nodes=256;, score=0.497 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7014 - accuracy: 0.4930\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=6, n_nodes=256;, score=0.501 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6960 - accuracy: 0.4992\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=6, n_nodes=256;, score=0.501 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7121 - accuracy: 0.5045\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6945 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.3, lr=0.01, n_layers=6, n_nodes=512;, score=0.497 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7096 - accuracy: 0.4971\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.3, lr=0.01, n_layers=6, n_nodes=512;, score=0.499 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7313 - accuracy: 0.5015\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.01, n_layers=6, n_nodes=512;, score=0.501 total time=   7.6s\n",
      "708/708 [==============================] - 3s 3ms/step - loss: 0.7009 - accuracy: 0.4988\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6939 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=1, n_nodes=32;, score=0.497 total time=   3.5s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.7060 - accuracy: 0.4978\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=1, n_nodes=32;, score=0.499 total time=   4.5s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.7046 - accuracy: 0.5023\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6937 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=1, n_nodes=32;, score=0.499 total time=   4.3s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.7057 - accuracy: 0.4964\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=1, n_nodes=64;, score=0.503 total time=   4.5s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7098 - accuracy: 0.4972\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6992 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=1, n_nodes=64;, score=0.501 total time=   4.6s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.7118 - accuracy: 0.5027\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=1, n_nodes=64;, score=0.501 total time=   4.5s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.7233 - accuracy: 0.4942\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=1, n_nodes=128;, score=0.497 total time=   4.6s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.7209 - accuracy: 0.4953\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=1, n_nodes=128;, score=0.499 total time=   4.5s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.7221 - accuracy: 0.5016\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=1, n_nodes=128;, score=0.499 total time=   4.5s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7463 - accuracy: 0.4997\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=1, n_nodes=256;, score=0.503 total time=   4.6s\n",
      "708/708 [==============================] - 4s 4ms/step - loss: 0.7547 - accuracy: 0.5025\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6945 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=1, n_nodes=256;, score=0.501 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7696 - accuracy: 0.4978\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6939 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=1, n_nodes=256;, score=0.499 total time=   4.6s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.8129 - accuracy: 0.5044\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=1, n_nodes=512;, score=0.503 total time=   3.6s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.8050 - accuracy: 0.5036\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6982 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=1, n_nodes=512;, score=0.501 total time=   4.4s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.8089 - accuracy: 0.5006\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6999 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=1, n_nodes=512;, score=0.499 total time=   4.5s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7019 - accuracy: 0.5026\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=2, n_nodes=32;, score=0.503 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6977 - accuracy: 0.4957\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=2, n_nodes=32;, score=0.501 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7106 - accuracy: 0.5020\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=2, n_nodes=32;, score=0.499 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7177 - accuracy: 0.4990\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=2, n_nodes=64;, score=0.503 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7093 - accuracy: 0.5031\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=2, n_nodes=64;, score=0.501 total time=   5.3s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7076 - accuracy: 0.5004\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=2, n_nodes=64;, score=0.499 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7740 - accuracy: 0.5076\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6954 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=2, n_nodes=128;, score=0.497 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7469 - accuracy: 0.4936\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=2, n_nodes=128;, score=0.499 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7623 - accuracy: 0.5023\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=2, n_nodes=128;, score=0.501 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.8657 - accuracy: 0.5037\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6971 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=2, n_nodes=256;, score=0.503 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.8761 - accuracy: 0.4952\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=2, n_nodes=256;, score=0.501 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.9229 - accuracy: 0.5019\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6947 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=2, n_nodes=256;, score=0.501 total time=   5.1s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 1.5867 - accuracy: 0.4989\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6990 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=2, n_nodes=512;, score=0.497 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 2.5416 - accuracy: 0.4958\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=2, n_nodes=512;, score=0.499 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 1.2000 - accuracy: 0.4997\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6943 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=2, n_nodes=512;, score=0.501 total time=   5.2s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.7009 - accuracy: 0.5008\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6979 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=3, n_nodes=32;, score=0.497 total time=   5.9s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.7103 - accuracy: 0.4968\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6949 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=3, n_nodes=32;, score=0.501 total time=   5.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6980 - accuracy: 0.4983\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6950 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=3, n_nodes=32;, score=0.501 total time=   5.8s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.7167 - accuracy: 0.4979\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6976 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=3, n_nodes=64;, score=0.497 total time=   5.8s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.7173 - accuracy: 0.5000\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6951 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=3, n_nodes=64;, score=0.501 total time=   5.8s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.7237 - accuracy: 0.4992\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6978 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=3, n_nodes=64;, score=0.499 total time=   5.5s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.8371 - accuracy: 0.4981\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6946 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=3, n_nodes=128;, score=0.503 total time=   4.9s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.8612 - accuracy: 0.5013\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6934 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=3, n_nodes=128;, score=0.501 total time=   4.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.8701 - accuracy: 0.4960\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6943 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=3, n_nodes=128;, score=0.499 total time=   5.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 1.9882 - accuracy: 0.5025\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6979 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=3, n_nodes=256;, score=0.497 total time=   6.5s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 1.8999 - accuracy: 0.4981\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=3, n_nodes=256;, score=0.501 total time=   4.7s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 2.0490 - accuracy: 0.5004\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=3, n_nodes=256;, score=0.501 total time=   5.5s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 8.7402 - accuracy: 0.4981\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6953 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=3, n_nodes=512;, score=0.503 total time=   5.7s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 5.1393 - accuracy: 0.5026\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6964 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=3, n_nodes=512;, score=0.501 total time=   5.6s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 8.6487 - accuracy: 0.4936\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6953 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=3, n_nodes=512;, score=0.501 total time=   4.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7053 - accuracy: 0.5044\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=4, n_nodes=32;, score=0.503 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7106 - accuracy: 0.5017\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6991 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=4, n_nodes=32;, score=0.499 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7000 - accuracy: 0.4986\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6974 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=4, n_nodes=32;, score=0.501 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7211 - accuracy: 0.4937\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=4, n_nodes=64;, score=0.497 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7425 - accuracy: 0.4976\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.7001 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=4, n_nodes=64;, score=0.501 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7452 - accuracy: 0.5014\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6990 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=4, n_nodes=64;, score=0.501 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 1.4772 - accuracy: 0.4944\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.7016 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=4, n_nodes=128;, score=0.497 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7911 - accuracy: 0.4924\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6989 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=4, n_nodes=128;, score=0.501 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 1.2941 - accuracy: 0.4986\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.7004 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=4, n_nodes=128;, score=0.501 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 4.8499 - accuracy: 0.4930\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6944 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=4, n_nodes=256;, score=0.497 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 2.8019 - accuracy: 0.5046\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.7052 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=4, n_nodes=256;, score=0.499 total time=   6.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 3.6370 - accuracy: 0.5002\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=4, n_nodes=256;, score=0.499 total time=   5.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 74.4810 - accuracy: 0.5006\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=4, n_nodes=512;, score=0.503 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 14.7353 - accuracy: 0.5008\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=4, n_nodes=512;, score=0.499 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 65.4009 - accuracy: 0.4985\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=4, n_nodes=512;, score=0.499 total time=   6.5s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.7821 - accuracy: 0.5007\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=5, n_nodes=32;, score=0.503 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.7170 - accuracy: 0.4986\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6955 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=5, n_nodes=32;, score=0.499 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6986 - accuracy: 0.5002\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6967 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=5, n_nodes=32;, score=0.501 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.7390 - accuracy: 0.5075\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=5, n_nodes=64;, score=0.497 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.7486 - accuracy: 0.4975\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=5, n_nodes=64;, score=0.501 total time=   7.5s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.7413 - accuracy: 0.4981\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=5, n_nodes=64;, score=0.499 total time=   5.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.8748 - accuracy: 0.4990\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6985 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=5, n_nodes=128;, score=0.503 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 1.2087 - accuracy: 0.4991\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6954 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=5, n_nodes=128;, score=0.499 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 1.6734 - accuracy: 0.4999\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=5, n_nodes=128;, score=0.501 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 6.5168 - accuracy: 0.5001\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6939 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=5, n_nodes=256;, score=0.497 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 25.3813 - accuracy: 0.4956\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=5, n_nodes=256;, score=0.499 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 14.0988 - accuracy: 0.5020\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=5, n_nodes=256;, score=0.501 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 1632.4996 - accuracy: 0.4973\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 129.0706 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=5, n_nodes=512;, score=0.503 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 336.2430 - accuracy: 0.4963\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=5, n_nodes=512;, score=0.501 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 833.1103 - accuracy: 0.4930\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6944 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=5, n_nodes=512;, score=0.499 total time=   6.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7178 - accuracy: 0.5080\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6936 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=6, n_nodes=32;, score=0.503 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7002 - accuracy: 0.4953\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6952 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=6, n_nodes=32;, score=0.501 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7131 - accuracy: 0.4937\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6940 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=6, n_nodes=32;, score=0.499 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7320 - accuracy: 0.4999\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6971 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=6, n_nodes=64;, score=0.503 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7257 - accuracy: 0.5022\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6950 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=6, n_nodes=64;, score=0.499 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7705 - accuracy: 0.5048\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=6, n_nodes=64;, score=0.501 total time=   7.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 6.1715 - accuracy: 0.4933\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6944 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=6, n_nodes=128;, score=0.503 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 2.7893 - accuracy: 0.4928\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6962 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=6, n_nodes=128;, score=0.499 total time=   6.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 23.9178 - accuracy: 0.4975\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6950 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=6, n_nodes=128;, score=0.501 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 17.1264 - accuracy: 0.4974\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.7136 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=6, n_nodes=256;, score=0.497 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 33.5297 - accuracy: 0.5034\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=6, n_nodes=256;, score=0.501 total time=   7.4s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 56.5537 - accuracy: 0.4999\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6943 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=6, n_nodes=256;, score=0.501 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 8094.6304 - accuracy: 0.4993\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.3, lr=0.1, n_layers=6, n_nodes=512;, score=0.503 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 6891.3940 - accuracy: 0.4955\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.3, lr=0.1, n_layers=6, n_nodes=512;, score=0.499 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 3299.2927 - accuracy: 0.4984\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.3, lr=0.1, n_layers=6, n_nodes=512;, score=0.501 total time=   7.4s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.7171 - accuracy: 0.5025\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6858 - accuracy: 0.5848\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=1, n_nodes=32;, score=0.585 total time=   4.5s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.7008 - accuracy: 0.5416\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6789 - accuracy: 0.6468\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=1, n_nodes=32;, score=0.647 total time=   4.5s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.7244 - accuracy: 0.5216\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6773 - accuracy: 0.6394\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=1, n_nodes=32;, score=0.639 total time=   4.4s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.7001 - accuracy: 0.5408\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6735 - accuracy: 0.6358\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=1, n_nodes=64;, score=0.636 total time=   4.5s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6903 - accuracy: 0.5538\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6700 - accuracy: 0.6277\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=1, n_nodes=64;, score=0.628 total time=   4.3s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.7119 - accuracy: 0.5293\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6777 - accuracy: 0.6319\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=1, n_nodes=64;, score=0.632 total time=   4.5s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.6814 - accuracy: 0.5769\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6593 - accuracy: 0.6548\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=1, n_nodes=128;, score=0.655 total time=   4.5s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6952 - accuracy: 0.5472\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6632 - accuracy: 0.6642\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=1, n_nodes=128;, score=0.664 total time=   4.2s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.6802 - accuracy: 0.5816\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6609 - accuracy: 0.6644\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=1, n_nodes=128;, score=0.664 total time=   4.6s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6806 - accuracy: 0.5779\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6665\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=1, n_nodes=256;, score=0.666 total time=   4.4s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6837 - accuracy: 0.5674\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6769\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=1, n_nodes=256;, score=0.677 total time=   4.4s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6720 - accuracy: 0.5988\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6642\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=1, n_nodes=256;, score=0.664 total time=   4.4s\n",
      "708/708 [==============================] - 3s 5ms/step - loss: 0.6687 - accuracy: 0.6012\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6451 - accuracy: 0.6659\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=1, n_nodes=512;, score=0.666 total time=   4.5s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6708 - accuracy: 0.5997\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6438 - accuracy: 0.6754\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=1, n_nodes=512;, score=0.675 total time=   4.3s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6696 - accuracy: 0.6054\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6783\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=1, n_nodes=512;, score=0.678 total time=   4.3s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7169 - accuracy: 0.5154\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6841 - accuracy: 0.5899\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=2, n_nodes=32;, score=0.590 total time=   5.2s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.7278 - accuracy: 0.4960\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6877 - accuracy: 0.5725\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=2, n_nodes=32;, score=0.573 total time=   4.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7312 - accuracy: 0.5064\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6871 - accuracy: 0.5515\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=2, n_nodes=32;, score=0.552 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7067 - accuracy: 0.5168\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6811 - accuracy: 0.5888\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=2, n_nodes=64;, score=0.589 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7094 - accuracy: 0.5025\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6841 - accuracy: 0.6193\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=2, n_nodes=64;, score=0.619 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7058 - accuracy: 0.5333\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6754 - accuracy: 0.6478\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=2, n_nodes=64;, score=0.648 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6841 - accuracy: 0.5762\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6583\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=2, n_nodes=128;, score=0.658 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6872 - accuracy: 0.5601\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6774\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=2, n_nodes=128;, score=0.677 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6834 - accuracy: 0.5675\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6694\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=2, n_nodes=128;, score=0.669 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6759 - accuracy: 0.5868\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6400 - accuracy: 0.6623\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=2, n_nodes=256;, score=0.662 total time=   4.7s\n",
      "708/708 [==============================] - 3s 4ms/step - loss: 0.6740 - accuracy: 0.5888\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6796\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=2, n_nodes=256;, score=0.680 total time=   4.3s\n",
      "708/708 [==============================] - 5s 5ms/step - loss: 0.6761 - accuracy: 0.5850\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6724\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=2, n_nodes=256;, score=0.672 total time=   5.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6536 - accuracy: 0.6292\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6576\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=2, n_nodes=512;, score=0.658 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6572 - accuracy: 0.6187\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6714\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=2, n_nodes=512;, score=0.671 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6559 - accuracy: 0.6227\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6759\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=2, n_nodes=512;, score=0.676 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7384 - accuracy: 0.5080\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6918 - accuracy: 0.5033\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=3, n_nodes=32;, score=0.503 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7058 - accuracy: 0.5135\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6882 - accuracy: 0.5898\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=3, n_nodes=32;, score=0.590 total time=   5.2s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7170 - accuracy: 0.5007\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5604\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=3, n_nodes=32;, score=0.560 total time=   5.6s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6997 - accuracy: 0.5209\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6778 - accuracy: 0.6472\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=3, n_nodes=64;, score=0.647 total time=   5.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.7020 - accuracy: 0.5093\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6854 - accuracy: 0.6476\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=3, n_nodes=64;, score=0.648 total time=   5.9s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7026 - accuracy: 0.5136\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6872 - accuracy: 0.5111\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=3, n_nodes=64;, score=0.511 total time=   5.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6948 - accuracy: 0.5334\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6687 - accuracy: 0.6419\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=3, n_nodes=128;, score=0.642 total time=   5.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6950 - accuracy: 0.5331\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6731 - accuracy: 0.6364\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=3, n_nodes=128;, score=0.636 total time=   5.8s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6933 - accuracy: 0.5310\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6736 - accuracy: 0.6239\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=3, n_nodes=128;, score=0.624 total time=   5.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6846 - accuracy: 0.5599\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6417 - accuracy: 0.6643\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=3, n_nodes=256;, score=0.664 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6868 - accuracy: 0.5555\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6800\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=3, n_nodes=256;, score=0.680 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6808 - accuracy: 0.5692\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6714\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=3, n_nodes=256;, score=0.671 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6598 - accuracy: 0.6111\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6693\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=3, n_nodes=512;, score=0.669 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6628 - accuracy: 0.6062\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6764\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=3, n_nodes=512;, score=0.676 total time=   6.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6621 - accuracy: 0.6078\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6246 - accuracy: 0.6711\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=3, n_nodes=512;, score=0.671 total time=   6.3s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.7025 - accuracy: 0.5005\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6925 - accuracy: 0.5541\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=4, n_nodes=32;, score=0.554 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7021 - accuracy: 0.5024\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6927 - accuracy: 0.5143\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=4, n_nodes=32;, score=0.514 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7056 - accuracy: 0.5056\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6917 - accuracy: 0.5939\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=4, n_nodes=32;, score=0.594 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7055 - accuracy: 0.4975\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6921 - accuracy: 0.5459\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=4, n_nodes=64;, score=0.546 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7066 - accuracy: 0.5060\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6880 - accuracy: 0.5430\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=4, n_nodes=64;, score=0.543 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7046 - accuracy: 0.5100\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6919 - accuracy: 0.5379\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=4, n_nodes=64;, score=0.538 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7008 - accuracy: 0.5142\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6854 - accuracy: 0.5979\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=4, n_nodes=128;, score=0.598 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6991 - accuracy: 0.5177\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6828 - accuracy: 0.6244\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=4, n_nodes=128;, score=0.624 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7000 - accuracy: 0.5100\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6885 - accuracy: 0.5909\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=4, n_nodes=128;, score=0.591 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6828 - accuracy: 0.5636\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6391 - accuracy: 0.6695\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=4, n_nodes=256;, score=0.669 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6863 - accuracy: 0.5542\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6375 - accuracy: 0.6750\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=4, n_nodes=256;, score=0.675 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6924 - accuracy: 0.5353\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6605 - accuracy: 0.6694\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=4, n_nodes=256;, score=0.669 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6687 - accuracy: 0.5893\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6686\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=4, n_nodes=512;, score=0.669 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6711 - accuracy: 0.5844\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6194 - accuracy: 0.6737\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=4, n_nodes=512;, score=0.674 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6741 - accuracy: 0.5806\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6529\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=4, n_nodes=512;, score=0.653 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7040 - accuracy: 0.5089\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6928 - accuracy: 0.5307\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=5, n_nodes=32;, score=0.531 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7013 - accuracy: 0.4987\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6935 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=5, n_nodes=32;, score=0.501 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7021 - accuracy: 0.5032\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6929 - accuracy: 0.5021\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=5, n_nodes=32;, score=0.502 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7028 - accuracy: 0.4994\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6929 - accuracy: 0.5334\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=5, n_nodes=64;, score=0.533 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7042 - accuracy: 0.5040\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6922 - accuracy: 0.5013\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=5, n_nodes=64;, score=0.501 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7005 - accuracy: 0.5054\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6913 - accuracy: 0.6272\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=5, n_nodes=64;, score=0.627 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6987 - accuracy: 0.5037\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6919 - accuracy: 0.5454\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=5, n_nodes=128;, score=0.545 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6977 - accuracy: 0.5093\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6898 - accuracy: 0.5783\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=5, n_nodes=128;, score=0.578 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6984 - accuracy: 0.5092\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6891 - accuracy: 0.6564\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=5, n_nodes=128;, score=0.656 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6955 - accuracy: 0.5176\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6831 - accuracy: 0.5518\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=5, n_nodes=256;, score=0.552 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6958 - accuracy: 0.5175\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6823 - accuracy: 0.5662\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=5, n_nodes=256;, score=0.566 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6921 - accuracy: 0.5333\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6638 - accuracy: 0.6043\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=5, n_nodes=256;, score=0.604 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6805 - accuracy: 0.5637\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6340 - accuracy: 0.6732\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=5, n_nodes=512;, score=0.673 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6822 - accuracy: 0.5551\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6225 - accuracy: 0.6773\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=5, n_nodes=512;, score=0.677 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6831 - accuracy: 0.5509\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6682\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=5, n_nodes=512;, score=0.668 total time=   7.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.7006 - accuracy: 0.5027\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=6, n_nodes=32;, score=0.503 total time=   8.1s\n",
      "708/708 [==============================] - 8s 10ms/step - loss: 0.7035 - accuracy: 0.4988\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5072\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=6, n_nodes=32;, score=0.507 total time=   9.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.7048 - accuracy: 0.4966\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6931 - accuracy: 0.4857\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=6, n_nodes=32;, score=0.486 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6979 - accuracy: 0.5026\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6929 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=6, n_nodes=64;, score=0.497 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6971 - accuracy: 0.5032\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6928 - accuracy: 0.5181\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=6, n_nodes=64;, score=0.518 total time=   8.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6977 - accuracy: 0.5102\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6924 - accuracy: 0.5172\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=6, n_nodes=64;, score=0.517 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6985 - accuracy: 0.4976\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6925 - accuracy: 0.5163\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=6, n_nodes=128;, score=0.516 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6992 - accuracy: 0.5050\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6924 - accuracy: 0.5311\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=6, n_nodes=128;, score=0.531 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6988 - accuracy: 0.5032\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6915 - accuracy: 0.6178\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=6, n_nodes=128;, score=0.618 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6959 - accuracy: 0.5091\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6889 - accuracy: 0.6381\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=6, n_nodes=256;, score=0.638 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6966 - accuracy: 0.5050\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6924 - accuracy: 0.6194\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=6, n_nodes=256;, score=0.619 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6960 - accuracy: 0.5093\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6904 - accuracy: 0.6138\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=6, n_nodes=256;, score=0.614 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6877 - accuracy: 0.5358\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6373\n",
      "[CV 1/3] END dropout=0.4, lr=0.0001, n_layers=6, n_nodes=512;, score=0.637 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6897 - accuracy: 0.5290\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6548\n",
      "[CV 2/3] END dropout=0.4, lr=0.0001, n_layers=6, n_nodes=512;, score=0.655 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6900 - accuracy: 0.5344\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6715\n",
      "[CV 3/3] END dropout=0.4, lr=0.0001, n_layers=6, n_nodes=512;, score=0.671 total time=   7.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6700 - accuracy: 0.5969\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6397 - accuracy: 0.6627\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=1, n_nodes=32;, score=0.663 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6731 - accuracy: 0.5977\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6389 - accuracy: 0.6572\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=1, n_nodes=32;, score=0.657 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6622 - accuracy: 0.6197\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6333 - accuracy: 0.6680\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=1, n_nodes=32;, score=0.668 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6558 - accuracy: 0.6261\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6707\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=1, n_nodes=64;, score=0.671 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6569 - accuracy: 0.6240\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6705\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=1, n_nodes=64;, score=0.670 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6605 - accuracy: 0.6155\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6717\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=1, n_nodes=64;, score=0.672 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6460 - accuracy: 0.6413\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6250 - accuracy: 0.6702\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=1, n_nodes=128;, score=0.670 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6542 - accuracy: 0.6250\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6201 - accuracy: 0.6812\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=1, n_nodes=128;, score=0.681 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6501 - accuracy: 0.6320\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6238 - accuracy: 0.6742\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=1, n_nodes=128;, score=0.674 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6441 - accuracy: 0.6447\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6628\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=1, n_nodes=256;, score=0.663 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6475 - accuracy: 0.6385\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6269 - accuracy: 0.6623\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=1, n_nodes=256;, score=0.662 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6468 - accuracy: 0.6388\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6250 - accuracy: 0.6767\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=1, n_nodes=256;, score=0.677 total time=   4.9s\n",
      "708/708 [==============================] - 5s 5ms/step - loss: 0.6435 - accuracy: 0.6422\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6705\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=1, n_nodes=512;, score=0.670 total time=   5.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6465 - accuracy: 0.6353\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6158 - accuracy: 0.6790\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=1, n_nodes=512;, score=0.679 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6448 - accuracy: 0.6388\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6315 - accuracy: 0.6593\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=1, n_nodes=512;, score=0.659 total time=   4.9s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6676 - accuracy: 0.6000\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6692\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=2, n_nodes=32;, score=0.669 total time=   5.5s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6668 - accuracy: 0.6053\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6395\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=2, n_nodes=32;, score=0.639 total time=   5.8s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6624 - accuracy: 0.6152\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6463 - accuracy: 0.6261\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=2, n_nodes=32;, score=0.626 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6558 - accuracy: 0.6218\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6355 - accuracy: 0.6577\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=2, n_nodes=64;, score=0.658 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6615 - accuracy: 0.6186\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6675\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=2, n_nodes=64;, score=0.668 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6564 - accuracy: 0.6184\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6231 - accuracy: 0.6787\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=2, n_nodes=64;, score=0.679 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6489 - accuracy: 0.6353\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6230 - accuracy: 0.6728\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=2, n_nodes=128;, score=0.673 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6490 - accuracy: 0.6383\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6153 - accuracy: 0.6807\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=2, n_nodes=128;, score=0.681 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6516 - accuracy: 0.6301\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6582\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=2, n_nodes=128;, score=0.658 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6447 - accuracy: 0.6385\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6684\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=2, n_nodes=256;, score=0.668 total time=   5.8s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6497 - accuracy: 0.6322\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6127 - accuracy: 0.6822\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=2, n_nodes=256;, score=0.682 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6448 - accuracy: 0.6392\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6363 - accuracy: 0.6498\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=2, n_nodes=256;, score=0.650 total time=   5.5s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6434 - accuracy: 0.6421\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6127\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=2, n_nodes=512;, score=0.613 total time=   5.8s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6463 - accuracy: 0.6389\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6135 - accuracy: 0.6832\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=2, n_nodes=512;, score=0.683 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6454 - accuracy: 0.6377\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6744\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=2, n_nodes=512;, score=0.674 total time=   5.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6739 - accuracy: 0.5800\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6383 - accuracy: 0.6653\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=3, n_nodes=32;, score=0.665 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6787 - accuracy: 0.5776\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6377 - accuracy: 0.6691\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=3, n_nodes=32;, score=0.669 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6823 - accuracy: 0.5558\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6744\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=3, n_nodes=32;, score=0.674 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6621 - accuracy: 0.6127\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6317 - accuracy: 0.6620\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=3, n_nodes=64;, score=0.662 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6722 - accuracy: 0.5853\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6299 - accuracy: 0.6628\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=3, n_nodes=64;, score=0.663 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6625 - accuracy: 0.6160\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6743\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=3, n_nodes=64;, score=0.674 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6556 - accuracy: 0.6218\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.6517\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=3, n_nodes=128;, score=0.652 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6583 - accuracy: 0.6163\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6198 - accuracy: 0.6744\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=3, n_nodes=128;, score=0.674 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6568 - accuracy: 0.6207\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6315 - accuracy: 0.6772\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=3, n_nodes=128;, score=0.677 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6490 - accuracy: 0.6300\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6623\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=3, n_nodes=256;, score=0.662 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6549 - accuracy: 0.6234\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6133 - accuracy: 0.6827\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=3, n_nodes=256;, score=0.683 total time=   6.3s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6521 - accuracy: 0.6237\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.6695\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=3, n_nodes=256;, score=0.669 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6478 - accuracy: 0.6365\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6337 - accuracy: 0.6719\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=3, n_nodes=512;, score=0.672 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6510 - accuracy: 0.6321\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6174 - accuracy: 0.6778\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=3, n_nodes=512;, score=0.678 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6485 - accuracy: 0.6346\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6732\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=3, n_nodes=512;, score=0.673 total time=   6.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6811 - accuracy: 0.5729\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6349 - accuracy: 0.6634\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=4, n_nodes=32;, score=0.663 total time=   7.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6863 - accuracy: 0.5492\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6671 - accuracy: 0.6165\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=4, n_nodes=32;, score=0.616 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6875 - accuracy: 0.5396\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6666 - accuracy: 0.6591\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=4, n_nodes=32;, score=0.659 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6757 - accuracy: 0.5743\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6476 - accuracy: 0.6267\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=4, n_nodes=64;, score=0.627 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6812 - accuracy: 0.5611\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6239 - accuracy: 0.6819\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=4, n_nodes=64;, score=0.682 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6669 - accuracy: 0.5999\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6650\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=4, n_nodes=64;, score=0.665 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6595 - accuracy: 0.6133\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6348 - accuracy: 0.6625\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=4, n_nodes=128;, score=0.663 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6657 - accuracy: 0.6055\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6144 - accuracy: 0.6826\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=4, n_nodes=128;, score=0.683 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6663 - accuracy: 0.6011\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6368 - accuracy: 0.6612\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=4, n_nodes=128;, score=0.661 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6551 - accuracy: 0.6206\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6269 - accuracy: 0.6710\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=4, n_nodes=256;, score=0.671 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6588 - accuracy: 0.6175\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6177 - accuracy: 0.6818\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=4, n_nodes=256;, score=0.682 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6542 - accuracy: 0.6231\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6667\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=4, n_nodes=256;, score=0.667 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6576 - accuracy: 0.6191\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6229 - accuracy: 0.6723\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=4, n_nodes=512;, score=0.672 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6528 - accuracy: 0.6240\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6148 - accuracy: 0.6819\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=4, n_nodes=512;, score=0.682 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6497 - accuracy: 0.6311\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6751\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=4, n_nodes=512;, score=0.675 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6932 - accuracy: 0.5229\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6770 - accuracy: 0.6388\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=5, n_nodes=32;, score=0.639 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6856 - accuracy: 0.5545\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6555 - accuracy: 0.6591\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=5, n_nodes=32;, score=0.659 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6911 - accuracy: 0.5285\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6944 - accuracy: 0.5013\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=5, n_nodes=32;, score=0.501 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6791 - accuracy: 0.5677\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6343 - accuracy: 0.6740\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=5, n_nodes=64;, score=0.674 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6851 - accuracy: 0.5459\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6270 - accuracy: 0.6818\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=5, n_nodes=64;, score=0.682 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6767 - accuracy: 0.5764\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6535 - accuracy: 0.6223\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=5, n_nodes=64;, score=0.622 total time=   7.7s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6659 - accuracy: 0.5954\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6456 - accuracy: 0.6181\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=5, n_nodes=128;, score=0.618 total time=   8.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6744 - accuracy: 0.5757\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6210 - accuracy: 0.6751\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=5, n_nodes=128;, score=0.675 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6727 - accuracy: 0.5792\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6336 - accuracy: 0.6639\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=5, n_nodes=128;, score=0.664 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6602 - accuracy: 0.6114\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6602\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=5, n_nodes=256;, score=0.660 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6643 - accuracy: 0.6046\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6366 - accuracy: 0.6815\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=5, n_nodes=256;, score=0.682 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6580 - accuracy: 0.6134\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.6635\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=5, n_nodes=256;, score=0.663 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6557 - accuracy: 0.6230\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6244 - accuracy: 0.6661\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=5, n_nodes=512;, score=0.666 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6608 - accuracy: 0.6157\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6825\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=5, n_nodes=512;, score=0.682 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6559 - accuracy: 0.6207\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6535 - accuracy: 0.6054\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=5, n_nodes=512;, score=0.605 total time=   7.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6917 - accuracy: 0.5203\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6797 - accuracy: 0.6054\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=6, n_nodes=32;, score=0.605 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6930 - accuracy: 0.5157\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.6352\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=6, n_nodes=32;, score=0.635 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6895 - accuracy: 0.5411\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6801 - accuracy: 0.5904\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=6, n_nodes=32;, score=0.590 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6895 - accuracy: 0.5387\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6638 - accuracy: 0.5840\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=6, n_nodes=64;, score=0.584 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6935 - accuracy: 0.5115\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6809 - accuracy: 0.6716\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=6, n_nodes=64;, score=0.672 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6944 - accuracy: 0.5031\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6919 - accuracy: 0.5076\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=6, n_nodes=64;, score=0.508 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6845 - accuracy: 0.5474\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6393 - accuracy: 0.6587\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=6, n_nodes=128;, score=0.659 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6766 - accuracy: 0.5731\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6255 - accuracy: 0.6714\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=6, n_nodes=128;, score=0.671 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6874 - accuracy: 0.5404\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6321 - accuracy: 0.6791\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=6, n_nodes=128;, score=0.679 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6674 - accuracy: 0.5922\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6324 - accuracy: 0.6709\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=6, n_nodes=256;, score=0.671 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6739 - accuracy: 0.5794\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6153 - accuracy: 0.6829\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=6, n_nodes=256;, score=0.683 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6684 - accuracy: 0.5942\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6394 - accuracy: 0.6791\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=6, n_nodes=256;, score=0.679 total time=   8.4s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6637 - accuracy: 0.6043\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6291 - accuracy: 0.6629\n",
      "[CV 1/3] END dropout=0.4, lr=0.001, n_layers=6, n_nodes=512;, score=0.663 total time=   9.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6649 - accuracy: 0.6057\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6428 - accuracy: 0.6650\n",
      "[CV 2/3] END dropout=0.4, lr=0.001, n_layers=6, n_nodes=512;, score=0.665 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6610 - accuracy: 0.6145\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6339 - accuracy: 0.6542\n",
      "[CV 3/3] END dropout=0.4, lr=0.001, n_layers=6, n_nodes=512;, score=0.654 total time=   8.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6723 - accuracy: 0.5761\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6704\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=1, n_nodes=32;, score=0.670 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6770 - accuracy: 0.5668\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6463 - accuracy: 0.6833\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=1, n_nodes=32;, score=0.683 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6916 - accuracy: 0.5116\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=1, n_nodes=32;, score=0.499 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6663 - accuracy: 0.5800\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6637\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=1, n_nodes=64;, score=0.664 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6699 - accuracy: 0.5820\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6313 - accuracy: 0.6787\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=1, n_nodes=64;, score=0.679 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6688 - accuracy: 0.5976\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6464\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=1, n_nodes=64;, score=0.646 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6635 - accuracy: 0.6060\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6363 - accuracy: 0.6712\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=1, n_nodes=128;, score=0.671 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6747 - accuracy: 0.5809\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6690 - accuracy: 0.6102\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=1, n_nodes=128;, score=0.610 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6675 - accuracy: 0.5867\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6360 - accuracy: 0.6727\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=1, n_nodes=128;, score=0.673 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6527 - accuracy: 0.6261\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6677\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=1, n_nodes=256;, score=0.668 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6934 - accuracy: 0.5134\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=1, n_nodes=256;, score=0.499 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6622 - accuracy: 0.6087\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6402 - accuracy: 0.6771\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=1, n_nodes=256;, score=0.677 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6628 - accuracy: 0.6156\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6580 - accuracy: 0.6427\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=1, n_nodes=512;, score=0.643 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6696 - accuracy: 0.5985\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6389 - accuracy: 0.6806\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=1, n_nodes=512;, score=0.681 total time=   4.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6636 - accuracy: 0.6086\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6417 - accuracy: 0.6720\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=1, n_nodes=512;, score=0.672 total time=   4.8s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6937 - accuracy: 0.5016\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=2, n_nodes=32;, score=0.497 total time=   5.8s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6927 - accuracy: 0.5145\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=2, n_nodes=32;, score=0.501 total time=   5.7s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6941 - accuracy: 0.5095\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.5012\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=2, n_nodes=32;, score=0.501 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6944 - accuracy: 0.5043\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=2, n_nodes=64;, score=0.497 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6802 - accuracy: 0.5649\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6563 - accuracy: 0.6727\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=2, n_nodes=64;, score=0.673 total time=   5.7s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6941 - accuracy: 0.5110\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=2, n_nodes=64;, score=0.499 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6773 - accuracy: 0.5784\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6352\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=2, n_nodes=128;, score=0.635 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6948 - accuracy: 0.4980\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=2, n_nodes=128;, score=0.499 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6761 - accuracy: 0.5749\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6436 - accuracy: 0.6666\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=2, n_nodes=128;, score=0.667 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6775 - accuracy: 0.5753\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6563\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=2, n_nodes=256;, score=0.656 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6893 - accuracy: 0.5271\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=2, n_nodes=256;, score=0.499 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6952 - accuracy: 0.5134\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4987\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=2, n_nodes=256;, score=0.499 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6812 - accuracy: 0.5700\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6732\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=2, n_nodes=512;, score=0.673 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6995 - accuracy: 0.5056\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=2, n_nodes=512;, score=0.499 total time=   5.4s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6953 - accuracy: 0.5185\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.7377 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=2, n_nodes=512;, score=0.501 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6867 - accuracy: 0.5448\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6584 - accuracy: 0.6668\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=3, n_nodes=32;, score=0.667 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6797 - accuracy: 0.5675\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6553\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=3, n_nodes=32;, score=0.655 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6944 - accuracy: 0.5031\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=3, n_nodes=32;, score=0.501 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6943 - accuracy: 0.5099\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=3, n_nodes=64;, score=0.497 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6943 - accuracy: 0.5026\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=3, n_nodes=64;, score=0.499 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6946 - accuracy: 0.4962\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=3, n_nodes=64;, score=0.499 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6923 - accuracy: 0.5193\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=3, n_nodes=128;, score=0.497 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6954 - accuracy: 0.5040\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=3, n_nodes=128;, score=0.501 total time=   6.4s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6955 - accuracy: 0.5011\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=3, n_nodes=128;, score=0.499 total time=   6.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6837 - accuracy: 0.5746\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6512 - accuracy: 0.6673\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=3, n_nodes=256;, score=0.667 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6952 - accuracy: 0.5026\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6948 - accuracy: 0.4989\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=3, n_nodes=256;, score=0.499 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6985 - accuracy: 0.4994\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=3, n_nodes=256;, score=0.499 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7053 - accuracy: 0.5007\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=3, n_nodes=512;, score=0.497 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7026 - accuracy: 0.4979\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=3, n_nodes=512;, score=0.499 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7002 - accuracy: 0.5116\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6958 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=3, n_nodes=512;, score=0.499 total time=   6.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6940 - accuracy: 0.5012\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=4, n_nodes=32;, score=0.503 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6939 - accuracy: 0.4950\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=4, n_nodes=32;, score=0.501 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6939 - accuracy: 0.5008\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6935 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=4, n_nodes=32;, score=0.501 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6921 - accuracy: 0.5255\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6934 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=4, n_nodes=64;, score=0.503 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6943 - accuracy: 0.5054\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=4, n_nodes=64;, score=0.501 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6949 - accuracy: 0.4947\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=4, n_nodes=64;, score=0.499 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6952 - accuracy: 0.4992\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6936 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=4, n_nodes=128;, score=0.497 total time=   7.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6956 - accuracy: 0.5024\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6931 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=4, n_nodes=128;, score=0.499 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6950 - accuracy: 0.5067\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=4, n_nodes=128;, score=0.501 total time=   7.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6958 - accuracy: 0.5060\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=4, n_nodes=256;, score=0.503 total time=   6.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6956 - accuracy: 0.4984\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=4, n_nodes=256;, score=0.501 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6982 - accuracy: 0.5028\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=4, n_nodes=256;, score=0.501 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7035 - accuracy: 0.5070\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=4, n_nodes=512;, score=0.503 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7039 - accuracy: 0.5028\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.4782\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=4, n_nodes=512;, score=0.478 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7178 - accuracy: 0.5026\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.4989\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=4, n_nodes=512;, score=0.499 total time=   6.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6937 - accuracy: 0.5058\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=5, n_nodes=32;, score=0.503 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6940 - accuracy: 0.5001\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6938 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=5, n_nodes=32;, score=0.499 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6939 - accuracy: 0.5005\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=5, n_nodes=32;, score=0.499 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6944 - accuracy: 0.5007\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6933 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=5, n_nodes=64;, score=0.497 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6944 - accuracy: 0.5015\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6934 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=5, n_nodes=64;, score=0.499 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6949 - accuracy: 0.4981\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6933 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=5, n_nodes=64;, score=0.501 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6949 - accuracy: 0.4976\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=5, n_nodes=128;, score=0.503 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6954 - accuracy: 0.4937\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=5, n_nodes=128;, score=0.499 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6951 - accuracy: 0.4985\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=5, n_nodes=128;, score=0.501 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6970 - accuracy: 0.5001\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6939 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=5, n_nodes=256;, score=0.503 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6969 - accuracy: 0.5018\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=5, n_nodes=256;, score=0.501 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6956 - accuracy: 0.4963\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=5, n_nodes=256;, score=0.499 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7017 - accuracy: 0.4958\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6948 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=5, n_nodes=512;, score=0.503 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7056 - accuracy: 0.4992\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=5, n_nodes=512;, score=0.501 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7120 - accuracy: 0.4996\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.7014 - accuracy: 0.5011\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=5, n_nodes=512;, score=0.501 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6944 - accuracy: 0.5009\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=6, n_nodes=32;, score=0.497 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6940 - accuracy: 0.5065\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=6, n_nodes=32;, score=0.499 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6939 - accuracy: 0.4989\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=6, n_nodes=32;, score=0.499 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6945 - accuracy: 0.5017\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6936 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=6, n_nodes=64;, score=0.503 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6950 - accuracy: 0.5013\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=6, n_nodes=64;, score=0.501 total time=   9.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6945 - accuracy: 0.5000\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6934 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=6, n_nodes=64;, score=0.499 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6954 - accuracy: 0.4996\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6952 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=6, n_nodes=128;, score=0.503 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6951 - accuracy: 0.5000\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=6, n_nodes=128;, score=0.499 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6954 - accuracy: 0.5007\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5011\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=6, n_nodes=128;, score=0.501 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6965 - accuracy: 0.4973\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=6, n_nodes=256;, score=0.503 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6968 - accuracy: 0.4998\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=6, n_nodes=256;, score=0.499 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6969 - accuracy: 0.5000\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=6, n_nodes=256;, score=0.501 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.7261 - accuracy: 0.4917\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6937 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.01, n_layers=6, n_nodes=512;, score=0.497 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.7171 - accuracy: 0.4973\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6935 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.01, n_layers=6, n_nodes=512;, score=0.499 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.7029 - accuracy: 0.4986\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.01, n_layers=6, n_nodes=512;, score=0.501 total time=   8.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7009 - accuracy: 0.4988\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6966 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=1, n_nodes=32;, score=0.497 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7067 - accuracy: 0.5037\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6975 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=1, n_nodes=32;, score=0.501 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7019 - accuracy: 0.4962\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=1, n_nodes=32;, score=0.501 total time=   5.3s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7144 - accuracy: 0.4994\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6943 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=1, n_nodes=64;, score=0.497 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7069 - accuracy: 0.4959\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6950 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=1, n_nodes=64;, score=0.501 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7078 - accuracy: 0.5009\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=1, n_nodes=64;, score=0.501 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7242 - accuracy: 0.5009\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=1, n_nodes=128;, score=0.497 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7234 - accuracy: 0.5064\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=1, n_nodes=128;, score=0.501 total time=   4.9s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7235 - accuracy: 0.5004\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=1, n_nodes=128;, score=0.499 total time=   5.1s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7892 - accuracy: 0.4949\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=1, n_nodes=256;, score=0.503 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7655 - accuracy: 0.4968\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6983 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=1, n_nodes=256;, score=0.499 total time=   5.2s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7561 - accuracy: 0.5020\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6939 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=1, n_nodes=256;, score=0.501 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7983 - accuracy: 0.4987\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=1, n_nodes=512;, score=0.497 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.7972 - accuracy: 0.4953\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6945 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=1, n_nodes=512;, score=0.501 total time=   5.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.8255 - accuracy: 0.4981\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6952 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=1, n_nodes=512;, score=0.499 total time=   5.2s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.7019 - accuracy: 0.5025\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=2, n_nodes=32;, score=0.503 total time=   5.8s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6994 - accuracy: 0.4980\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6980 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=2, n_nodes=32;, score=0.499 total time=   6.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6973 - accuracy: 0.4993\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6956 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=2, n_nodes=32;, score=0.499 total time=   5.6s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.7118 - accuracy: 0.5020\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=2, n_nodes=64;, score=0.497 total time=   5.8s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7098 - accuracy: 0.4948\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6951 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=2, n_nodes=64;, score=0.501 total time=   5.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.7150 - accuracy: 0.4925\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=2, n_nodes=64;, score=0.501 total time=   5.8s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7845 - accuracy: 0.5056\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6985 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=2, n_nodes=128;, score=0.497 total time=   5.8s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.7591 - accuracy: 0.4980\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6943 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=2, n_nodes=128;, score=0.499 total time=   5.8s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.7999 - accuracy: 0.5066\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6994 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=2, n_nodes=128;, score=0.499 total time=   5.7s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 1.0337 - accuracy: 0.4999\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.7016 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=2, n_nodes=256;, score=0.503 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 1.2242 - accuracy: 0.4991\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6945 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=2, n_nodes=256;, score=0.499 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.8038 - accuracy: 0.5038\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=2, n_nodes=256;, score=0.499 total time=   5.7s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 1.5286 - accuracy: 0.4966\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6959 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=2, n_nodes=512;, score=0.497 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 1.1312 - accuracy: 0.5006\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6943 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=2, n_nodes=512;, score=0.501 total time=   5.7s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 1.3990 - accuracy: 0.5014\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6972 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=2, n_nodes=512;, score=0.501 total time=   5.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7014 - accuracy: 0.4984\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6979 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=3, n_nodes=32;, score=0.503 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6986 - accuracy: 0.4997\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6936 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=3, n_nodes=32;, score=0.499 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7159 - accuracy: 0.4997\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6958 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=3, n_nodes=32;, score=0.499 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7190 - accuracy: 0.5009\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=3, n_nodes=64;, score=0.497 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7096 - accuracy: 0.4991\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=3, n_nodes=64;, score=0.499 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7225 - accuracy: 0.5031\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6955 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=3, n_nodes=64;, score=0.501 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.8292 - accuracy: 0.4987\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=3, n_nodes=128;, score=0.503 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.7866 - accuracy: 0.4999\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=3, n_nodes=128;, score=0.501 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.8047 - accuracy: 0.4962\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6966 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=3, n_nodes=128;, score=0.499 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 1.4294 - accuracy: 0.4967\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=3, n_nodes=256;, score=0.497 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 2.1871 - accuracy: 0.5010\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6947 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=3, n_nodes=256;, score=0.499 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 1.0545 - accuracy: 0.4987\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=3, n_nodes=256;, score=0.501 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 7.7619 - accuracy: 0.5009\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6964 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=3, n_nodes=512;, score=0.497 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 6.7179 - accuracy: 0.5004\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6952 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=3, n_nodes=512;, score=0.499 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 5.4475 - accuracy: 0.4986\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6933 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=3, n_nodes=512;, score=0.501 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6987 - accuracy: 0.4956\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=4, n_nodes=32;, score=0.497 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6980 - accuracy: 0.4961\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6946 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=4, n_nodes=32;, score=0.499 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7008 - accuracy: 0.4991\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6993 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=4, n_nodes=32;, score=0.501 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7384 - accuracy: 0.4954\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=4, n_nodes=64;, score=0.497 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7545 - accuracy: 0.5020\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=4, n_nodes=64;, score=0.499 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7430 - accuracy: 0.4963\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6950 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=4, n_nodes=64;, score=0.501 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.9946 - accuracy: 0.5039\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6953 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=4, n_nodes=128;, score=0.497 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.9140 - accuracy: 0.4966\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6933 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=4, n_nodes=128;, score=0.501 total time=   7.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.7881 - accuracy: 0.5014\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6957 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=4, n_nodes=128;, score=0.499 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 6.0020 - accuracy: 0.5028\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=4, n_nodes=256;, score=0.503 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 6.3273 - accuracy: 0.5029\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6947 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=4, n_nodes=256;, score=0.501 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 2.9813 - accuracy: 0.4975\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6977 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=4, n_nodes=256;, score=0.501 total time=   7.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 48.7778 - accuracy: 0.4987\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.7003 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=4, n_nodes=512;, score=0.497 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 201.3233 - accuracy: 0.5002\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6948 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=4, n_nodes=512;, score=0.501 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 36.4486 - accuracy: 0.5008\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6968 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=4, n_nodes=512;, score=0.501 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7034 - accuracy: 0.5036\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6933 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=5, n_nodes=32;, score=0.497 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6982 - accuracy: 0.4975\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6937 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=5, n_nodes=32;, score=0.499 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6987 - accuracy: 0.4985\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6940 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=5, n_nodes=32;, score=0.501 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7389 - accuracy: 0.4995\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6971 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=5, n_nodes=64;, score=0.503 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.7337 - accuracy: 0.4993\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.7041 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=5, n_nodes=64;, score=0.499 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.7311 - accuracy: 0.5023\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6942 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=5, n_nodes=64;, score=0.499 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 1.4609 - accuracy: 0.5031\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6937 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=5, n_nodes=128;, score=0.503 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 1.0855 - accuracy: 0.5030\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6960 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=5, n_nodes=128;, score=0.499 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 1.3311 - accuracy: 0.5016\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6937 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=5, n_nodes=128;, score=0.501 total time=   7.8s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 7.0379 - accuracy: 0.5043\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=5, n_nodes=256;, score=0.497 total time=   8.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 3.8801 - accuracy: 0.5009\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6948 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=5, n_nodes=256;, score=0.499 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 13.4692 - accuracy: 0.5056\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6940 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=5, n_nodes=256;, score=0.501 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 90.6604 - accuracy: 0.4912\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.5026\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=5, n_nodes=512;, score=0.503 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 242.0736 - accuracy: 0.5055\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6934 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=5, n_nodes=512;, score=0.501 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 325.6722 - accuracy: 0.5034\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6970 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=5, n_nodes=512;, score=0.501 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.7125 - accuracy: 0.4972\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6962 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=6, n_nodes=32;, score=0.497 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.7368 - accuracy: 0.5028\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=6, n_nodes=32;, score=0.501 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.7010 - accuracy: 0.4960\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6985 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=6, n_nodes=32;, score=0.499 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.7611 - accuracy: 0.4980\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=6, n_nodes=64;, score=0.497 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.7589 - accuracy: 0.5021\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6935 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=6, n_nodes=64;, score=0.501 total time=   8.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.7492 - accuracy: 0.5074\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6961 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=6, n_nodes=64;, score=0.501 total time=   8.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 2.1856 - accuracy: 0.4935\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6933 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=6, n_nodes=128;, score=0.497 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 1.6228 - accuracy: 0.5047\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.4986\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=6, n_nodes=128;, score=0.499 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.9599 - accuracy: 0.5020\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6936 - accuracy: 0.4986\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=6, n_nodes=128;, score=0.499 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 99.6089 - accuracy: 0.4997\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.7051 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=6, n_nodes=256;, score=0.497 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 11.5151 - accuracy: 0.5025\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6945 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=6, n_nodes=256;, score=0.501 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 23.9828 - accuracy: 0.4985\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6975 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=6, n_nodes=256;, score=0.501 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 1053.2802 - accuracy: 0.4979\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6933 - accuracy: 0.4974\n",
      "[CV 1/3] END dropout=0.4, lr=0.1, n_layers=6, n_nodes=512;, score=0.497 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 764.2578 - accuracy: 0.4977\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6932 - accuracy: 0.5014\n",
      "[CV 2/3] END dropout=0.4, lr=0.1, n_layers=6, n_nodes=512;, score=0.501 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 5040.9731 - accuracy: 0.4978\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6965 - accuracy: 0.5014\n",
      "[CV 3/3] END dropout=0.4, lr=0.1, n_layers=6, n_nodes=512;, score=0.501 total time=   8.1s\n",
      "1062/1062 [==============================] - 9s 8ms/step - loss: 0.6364 - accuracy: 0.6507\n",
      "Best Parameters: {'dropout': 0.2, 'lr': 0.001, 'n_layers': 5, 'n_nodes': 256}\n",
      "Best Accuracy: 0.68\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_model)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, verbose=3, n_jobs=1)\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "\n",
    "print(f'Best Parameters: {grid_result.best_params_}')\n",
    "print(f'Best Accuracy: {grid_result.best_score_:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_opt_dense(lr= 0.0001, n_nodes1=32, n_nodes2=32, n_nodes3=32, n_nodes4=32, n_nodes5=32,dropout=0.1, activation=\"sigmoid\"):\n",
    "    model = Sequential([\n",
    "        Dense(n_nodes1, activation='relu', input_shape=(len(x_train.columns),)),\n",
    "        Dropout(dropout),\n",
    "        Dense(n_nodes2, activation='relu'),\n",
    "        Dropout(dropout),\n",
    "        Dense(n_nodes3, activation='relu'),\n",
    "        Dropout(dropout),\n",
    "        Dense(n_nodes4, activation='relu'),\n",
    "        Dropout(dropout),\n",
    "        Dense(n_nodes5, activation='relu'),\n",
    "        Dropout(dropout),\n",
    "        Dense(len(le.classes_), activation=activation)\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=lr), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dominik Hahn\\AppData\\Local\\Temp\\ipykernel_26164\\2395483213.py:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=create_model_opt_dense)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1944 candidates, totalling 5832 fits\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6909 - accuracy: 0.5313\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6743 - accuracy: 0.6624\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.662 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6918 - accuracy: 0.5175\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6785 - accuracy: 0.6391\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.639 total time=   7.5s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6894 - accuracy: 0.5341\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6682 - accuracy: 0.6613\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.661 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6911 - accuracy: 0.5302\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6756 - accuracy: 0.6150\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.615 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6910 - accuracy: 0.5289\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6800 - accuracy: 0.5772\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.577 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6930 - accuracy: 0.5197\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6832 - accuracy: 0.5749\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.575 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6928 - accuracy: 0.5187\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6825 - accuracy: 0.6217\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.622 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6941 - accuracy: 0.5066\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6899 - accuracy: 0.6377\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.638 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6907 - accuracy: 0.5294\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6751 - accuracy: 0.5920\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.592 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6935 - accuracy: 0.5155\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6872 - accuracy: 0.6048\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.605 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6883 - accuracy: 0.5427\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6738 - accuracy: 0.5987\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.599 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6912 - accuracy: 0.5265\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6726 - accuracy: 0.6307\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.631 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6940 - accuracy: 0.5232\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6832 - accuracy: 0.5729\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.573 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6916 - accuracy: 0.5267\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6832 - accuracy: 0.5936\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.594 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6884 - accuracy: 0.5407\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6613 - accuracy: 0.6532\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.653 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6834 - accuracy: 0.5555\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6492 - accuracy: 0.6298\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.630 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6854 - accuracy: 0.5487\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6455 - accuracy: 0.6629\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.663 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6872 - accuracy: 0.5489\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6606 - accuracy: 0.6504\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.650 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6900 - accuracy: 0.5349\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6739 - accuracy: 0.6141\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.614 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6893 - accuracy: 0.5331\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6684 - accuracy: 0.6417\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.642 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6843 - accuracy: 0.5533\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6523 - accuracy: 0.6370\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.637 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6897 - accuracy: 0.5374\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6723 - accuracy: 0.6320\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.632 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6883 - accuracy: 0.5404\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6609 - accuracy: 0.6396\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.640 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6911 - accuracy: 0.5260\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6750 - accuracy: 0.6190\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.619 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6916 - accuracy: 0.5244\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6809 - accuracy: 0.6286\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.629 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6872 - accuracy: 0.5332\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6589 - accuracy: 0.6592\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.659 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6888 - accuracy: 0.5355\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6184\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.618 total time=   7.4s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6899 - accuracy: 0.5355\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6675 - accuracy: 0.6542\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.654 total time=   8.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6910 - accuracy: 0.5365\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6764 - accuracy: 0.6308\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.631 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6853 - accuracy: 0.5611\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6519 - accuracy: 0.6585\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.659 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6929 - accuracy: 0.5257\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6767 - accuracy: 0.6123\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.612 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6876 - accuracy: 0.5403\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6595 - accuracy: 0.6367\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.637 total time=   7.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6901 - accuracy: 0.5347\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6654 - accuracy: 0.6344\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.634 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6896 - accuracy: 0.5317\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.6455\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.645 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6929 - accuracy: 0.5161\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6835 - accuracy: 0.6285\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.629 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6909 - accuracy: 0.5227\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6735 - accuracy: 0.6279\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.628 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6797 - accuracy: 0.5662\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6440 - accuracy: 0.6477\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.648 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6904 - accuracy: 0.5333\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6694 - accuracy: 0.6645\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.665 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6933 - accuracy: 0.5173\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6823 - accuracy: 0.6300\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.630 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6905 - accuracy: 0.5245\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6752 - accuracy: 0.5870\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.587 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6838 - accuracy: 0.5542\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6499 - accuracy: 0.6628\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.663 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6865 - accuracy: 0.5479\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6447\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.645 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6808 - accuracy: 0.5642\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6380\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.638 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6902 - accuracy: 0.5358\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6665 - accuracy: 0.6516\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.652 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6815 - accuracy: 0.5585\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6340 - accuracy: 0.6658\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.666 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6897 - accuracy: 0.5299\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6714 - accuracy: 0.6349\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.635 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6936 - accuracy: 0.5096\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6911 - accuracy: 0.5014\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.501 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6897 - accuracy: 0.5374\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6663 - accuracy: 0.6529\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.653 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6915 - accuracy: 0.5258\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6742 - accuracy: 0.5913\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.591 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6737 - accuracy: 0.5832\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6713\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6932 - accuracy: 0.5144\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6835 - accuracy: 0.6180\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.618 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6848 - accuracy: 0.5527\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6483 - accuracy: 0.6412\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.641 total time=   7.6s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6858 - accuracy: 0.5466\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6604 - accuracy: 0.6135\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.613 total time=   8.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6831 - accuracy: 0.5584\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6481 - accuracy: 0.6752\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.675 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6847 - accuracy: 0.5503\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6529 - accuracy: 0.6434\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.643 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6911 - accuracy: 0.5236\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6766 - accuracy: 0.6328\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.633 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6893 - accuracy: 0.5279\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6718 - accuracy: 0.6094\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.609 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6884 - accuracy: 0.5339\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6638 - accuracy: 0.6229\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.623 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6877 - accuracy: 0.5440\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6577 - accuracy: 0.6465\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.647 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6926 - accuracy: 0.5166\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6837 - accuracy: 0.5370\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.537 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6868 - accuracy: 0.5454\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6565 - accuracy: 0.6607\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.661 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6775 - accuracy: 0.5731\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6296 - accuracy: 0.6702\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.670 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6789 - accuracy: 0.5691\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.6567\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.657 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6922 - accuracy: 0.5201\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6776 - accuracy: 0.6522\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.652 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6877 - accuracy: 0.5424\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6566 - accuracy: 0.6482\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.648 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6899 - accuracy: 0.5312\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6706 - accuracy: 0.5893\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.589 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6910 - accuracy: 0.5298\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6771 - accuracy: 0.6381\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.638 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6805 - accuracy: 0.5634\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6333 - accuracy: 0.6738\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.674 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6722 - accuracy: 0.5783\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6676\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.668 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6885 - accuracy: 0.5350\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6620 - accuracy: 0.6265\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.626 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6779 - accuracy: 0.5689\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6680\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6790 - accuracy: 0.5663\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6393\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.639 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6781 - accuracy: 0.5751\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6426 - accuracy: 0.6486\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.649 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6794 - accuracy: 0.5623\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6476 - accuracy: 0.6603\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.660 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6847 - accuracy: 0.5470\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6471 - accuracy: 0.6532\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.653 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6783 - accuracy: 0.5684\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6379 - accuracy: 0.6592\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.659 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6868 - accuracy: 0.5452\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6489 - accuracy: 0.6616\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.662 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6854 - accuracy: 0.5481\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6487 - accuracy: 0.6544\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.654 total time=   8.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6831 - accuracy: 0.5560\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.6528\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.653 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6834 - accuracy: 0.5562\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6482 - accuracy: 0.6257\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.626 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6816 - accuracy: 0.5555\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6427 - accuracy: 0.6734\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.673 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6949 - accuracy: 0.5134\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6872 - accuracy: 0.6238\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.624 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6850 - accuracy: 0.5611\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6546 - accuracy: 0.6541\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.654 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6898 - accuracy: 0.5393\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6697 - accuracy: 0.6506\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.651 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6855 - accuracy: 0.5556\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6547 - accuracy: 0.6444\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.644 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6917 - accuracy: 0.5295\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6743 - accuracy: 0.6536\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.654 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6902 - accuracy: 0.5320\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6662 - accuracy: 0.6518\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.652 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6905 - accuracy: 0.5328\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6744 - accuracy: 0.6517\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.652 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6891 - accuracy: 0.5355\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6575 - accuracy: 0.6653\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.665 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6863 - accuracy: 0.5464\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6476 - accuracy: 0.6650\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.665 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6893 - accuracy: 0.5402\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6710 - accuracy: 0.6419\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.642 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6830 - accuracy: 0.5607\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6644\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.664 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6858 - accuracy: 0.5493\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6485 - accuracy: 0.6602\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.660 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6883 - accuracy: 0.5392\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6629 - accuracy: 0.6358\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.636 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6895 - accuracy: 0.5336\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6675 - accuracy: 0.6575\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.657 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6902 - accuracy: 0.5343\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6538\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.654 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6867 - accuracy: 0.5418\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6525 - accuracy: 0.6437\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.644 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6894 - accuracy: 0.5347\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6754\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.675 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6866 - accuracy: 0.5462\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6559\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.656 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6822 - accuracy: 0.5625\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6428 - accuracy: 0.6605\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.660 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6881 - accuracy: 0.5339\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6512\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.651 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6796 - accuracy: 0.5649\n",
      "354/354 [==============================] - 2s 3ms/step - loss: 0.6486 - accuracy: 0.6329\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.633 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6861 - accuracy: 0.5429\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6469\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.647 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6888 - accuracy: 0.5369\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6499\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.650 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6930 - accuracy: 0.5109\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6856 - accuracy: 0.6399\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.640 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6759 - accuracy: 0.5709\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6336 - accuracy: 0.6582\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.658 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6842 - accuracy: 0.5526\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6424 - accuracy: 0.6564\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.656 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6927 - accuracy: 0.5135\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6841 - accuracy: 0.6307\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.631 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6875 - accuracy: 0.5445\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6570 - accuracy: 0.6467\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.647 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6906 - accuracy: 0.5248\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6746 - accuracy: 0.5613\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.561 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6913 - accuracy: 0.5279\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6720 - accuracy: 0.6383\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.638 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6812 - accuracy: 0.5641\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6451 - accuracy: 0.6517\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.652 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6841 - accuracy: 0.5520\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6612\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.661 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6919 - accuracy: 0.5234\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6805 - accuracy: 0.6141\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.614 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6893 - accuracy: 0.5335\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6631 - accuracy: 0.6431\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.643 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6927 - accuracy: 0.5178\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6810 - accuracy: 0.6410\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.641 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6896 - accuracy: 0.5363\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6627 - accuracy: 0.6509\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.651 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6919 - accuracy: 0.5277\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6730 - accuracy: 0.6237\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.624 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6919 - accuracy: 0.5272\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6738 - accuracy: 0.6672\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.667 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6861 - accuracy: 0.5521\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6506 - accuracy: 0.6528\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.653 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6820 - accuracy: 0.5597\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6597 - accuracy: 0.5978\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.598 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6776 - accuracy: 0.5720\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6629\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.663 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6798 - accuracy: 0.5643\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6465 - accuracy: 0.6408\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.641 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6771 - accuracy: 0.5762\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6354 - accuracy: 0.6658\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.666 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6918 - accuracy: 0.5253\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6770 - accuracy: 0.6102\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.610 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6860 - accuracy: 0.5433\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6498 - accuracy: 0.6471\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.647 total time=   7.6s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6843 - accuracy: 0.5516\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6548 - accuracy: 0.6140\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.614 total time=   8.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6842 - accuracy: 0.5541\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6379 - accuracy: 0.6718\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.672 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6794 - accuracy: 0.5686\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6405 - accuracy: 0.6638\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.664 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6820 - accuracy: 0.5581\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6410 - accuracy: 0.6622\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.662 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6841 - accuracy: 0.5509\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6457 - accuracy: 0.6475\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.648 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6907 - accuracy: 0.5251\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6742 - accuracy: 0.6320\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.632 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6726 - accuracy: 0.5815\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6330 - accuracy: 0.6555\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.656 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6800 - accuracy: 0.5659\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6340 - accuracy: 0.6686\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.669 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6876 - accuracy: 0.5420\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6505 - accuracy: 0.6573\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.657 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6770 - accuracy: 0.5737\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6541\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.654 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6915 - accuracy: 0.5222\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6745 - accuracy: 0.6119\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.612 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6905 - accuracy: 0.5314\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6642 - accuracy: 0.6138\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.614 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6889 - accuracy: 0.5319\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6649 - accuracy: 0.6127\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.613 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6808 - accuracy: 0.5675\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6741\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6796 - accuracy: 0.5673\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6327 - accuracy: 0.6710\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.671 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6865 - accuracy: 0.5408\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6577 - accuracy: 0.6295\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.629 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6799 - accuracy: 0.5679\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6390 - accuracy: 0.6628\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.663 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6896 - accuracy: 0.5322\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6647 - accuracy: 0.6512\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.651 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6844 - accuracy: 0.5520\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6510\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.651 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6861 - accuracy: 0.5458\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6509\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.651 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6772 - accuracy: 0.5707\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6584\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.658 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6756 - accuracy: 0.5778\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6638\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.664 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6786 - accuracy: 0.5711\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6350 - accuracy: 0.6621\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.662 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6719 - accuracy: 0.5897\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6352 - accuracy: 0.6576\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.658 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6817 - accuracy: 0.5545\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6443 - accuracy: 0.6380\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.638 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6773 - accuracy: 0.5683\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6769\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.677 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6794 - accuracy: 0.5625\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6633\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.663 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6802 - accuracy: 0.5631\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6433 - accuracy: 0.6487\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.649 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6819 - accuracy: 0.5568\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6346 - accuracy: 0.6673\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.667 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6796 - accuracy: 0.5660\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6612\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.661 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6833 - accuracy: 0.5537\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6389 - accuracy: 0.6602\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.660 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6852 - accuracy: 0.5429\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6497 - accuracy: 0.6469\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.647 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6766 - accuracy: 0.5707\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6348 - accuracy: 0.6585\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.659 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6733 - accuracy: 0.5803\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6431 - accuracy: 0.6365\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.637 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6719 - accuracy: 0.5785\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6292 - accuracy: 0.6686\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.669 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6790 - accuracy: 0.5669\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6365 - accuracy: 0.6552\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.655 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6848 - accuracy: 0.5511\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6574\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.657 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6914 - accuracy: 0.5213\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6736 - accuracy: 0.6101\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.610 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6902 - accuracy: 0.5403\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6686 - accuracy: 0.6592\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.659 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6891 - accuracy: 0.5469\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6633 - accuracy: 0.6471\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.647 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6875 - accuracy: 0.5492\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6469\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.647 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6813 - accuracy: 0.5666\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6373 - accuracy: 0.6623\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.662 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6842 - accuracy: 0.5543\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6438 - accuracy: 0.6517\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.652 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6829 - accuracy: 0.5595\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6397 - accuracy: 0.6648\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.665 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6908 - accuracy: 0.5275\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6765 - accuracy: 0.6236\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.624 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6798 - accuracy: 0.5680\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6411 - accuracy: 0.6686\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.669 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6870 - accuracy: 0.5422\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6469 - accuracy: 0.6668\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.667 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6904 - accuracy: 0.5339\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6469\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.647 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6820 - accuracy: 0.5630\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6508 - accuracy: 0.6343\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.634 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6857 - accuracy: 0.5427\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6271\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.627 total time=   7.5s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6866 - accuracy: 0.5423\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6480\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.648 total time=   8.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6830 - accuracy: 0.5540\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6468 - accuracy: 0.6447\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.645 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6850 - accuracy: 0.5495\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6479 - accuracy: 0.6561\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.656 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6890 - accuracy: 0.5350\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6226\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.623 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6892 - accuracy: 0.5281\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6638 - accuracy: 0.6460\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.646 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6851 - accuracy: 0.5494\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6583\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.658 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6738 - accuracy: 0.5760\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6270 - accuracy: 0.6653\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.665 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6860 - accuracy: 0.5470\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6572\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.657 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6834 - accuracy: 0.5561\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6420 - accuracy: 0.6579\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.658 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6723 - accuracy: 0.5894\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6285 - accuracy: 0.6711\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6827 - accuracy: 0.5530\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6423 - accuracy: 0.6524\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.652 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6907 - accuracy: 0.5305\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6681 - accuracy: 0.5995\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.599 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6871 - accuracy: 0.5388\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6489 - accuracy: 0.6491\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.649 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6846 - accuracy: 0.5550\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6419 - accuracy: 0.6675\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.667 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6835 - accuracy: 0.5539\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6367 - accuracy: 0.6680\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6861 - accuracy: 0.5478\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6517 - accuracy: 0.6454\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.645 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6845 - accuracy: 0.5574\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6496 - accuracy: 0.6444\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.644 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6812 - accuracy: 0.5587\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6338 - accuracy: 0.6680\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.668 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6815 - accuracy: 0.5591\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6377 - accuracy: 0.6688\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.669 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6782 - accuracy: 0.5689\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6352 - accuracy: 0.6601\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.660 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6808 - accuracy: 0.5676\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6362 - accuracy: 0.6834\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.683 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6881 - accuracy: 0.5381\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6645 - accuracy: 0.6026\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.603 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6806 - accuracy: 0.5621\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6473 - accuracy: 0.6574\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.657 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6886 - accuracy: 0.5433\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6591 - accuracy: 0.6528\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.653 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6776 - accuracy: 0.5694\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6345 - accuracy: 0.6635\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.664 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6810 - accuracy: 0.5577\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.6584\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.658 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6860 - accuracy: 0.5453\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6524 - accuracy: 0.6558\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.656 total time=   8.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6881 - accuracy: 0.5383\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6574 - accuracy: 0.6514\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.651 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6875 - accuracy: 0.5363\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6500\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.650 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6810 - accuracy: 0.5631\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6738\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.674 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6779 - accuracy: 0.5703\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6656\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.666 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6718 - accuracy: 0.5866\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6345 - accuracy: 0.6608\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.661 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6904 - accuracy: 0.5256\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6791 - accuracy: 0.5562\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.556 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6898 - accuracy: 0.5334\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6618 - accuracy: 0.6471\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.647 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6781 - accuracy: 0.5640\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6409 - accuracy: 0.6613\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.661 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6815 - accuracy: 0.5617\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6331 - accuracy: 0.6766\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.677 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6773 - accuracy: 0.5727\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6617\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.662 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6794 - accuracy: 0.5653\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6408 - accuracy: 0.6590\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.659 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6824 - accuracy: 0.5578\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6354 - accuracy: 0.6645\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.664 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6819 - accuracy: 0.5548\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6400 - accuracy: 0.6577\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.658 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6657 - accuracy: 0.5944\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6325 - accuracy: 0.6583\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.658 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6768 - accuracy: 0.5750\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6405 - accuracy: 0.6392\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.639 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6765 - accuracy: 0.5726\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6474 - accuracy: 0.6339\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.634 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6722 - accuracy: 0.5854\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6388 - accuracy: 0.6492\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.649 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6840 - accuracy: 0.5590\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6752\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.675 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6864 - accuracy: 0.5405\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6521\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.652 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6775 - accuracy: 0.5748\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6363 - accuracy: 0.6581\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.658 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6838 - accuracy: 0.5441\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6511 - accuracy: 0.6243\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.624 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6872 - accuracy: 0.5414\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6630 - accuracy: 0.5989\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.599 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6807 - accuracy: 0.5644\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6488 - accuracy: 0.6338\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.634 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6809 - accuracy: 0.5623\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6401 - accuracy: 0.6641\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.664 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6855 - accuracy: 0.5380\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6337\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.634 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6689 - accuracy: 0.5921\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6564\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.656 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6903 - accuracy: 0.5255\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6580 - accuracy: 0.6563\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.656 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6836 - accuracy: 0.5494\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6510\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.651 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6827 - accuracy: 0.5510\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6345 - accuracy: 0.6605\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.660 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6797 - accuracy: 0.5546\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6310 - accuracy: 0.6652\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.665 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6752 - accuracy: 0.5730\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6292 - accuracy: 0.6677\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6654 - accuracy: 0.6015\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6294 - accuracy: 0.6631\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.663 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6826 - accuracy: 0.5563\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6393 - accuracy: 0.6648\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.665 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6770 - accuracy: 0.5687\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6508 - accuracy: 0.6390\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.639 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6731 - accuracy: 0.5755\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6456 - accuracy: 0.6340\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.634 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6747 - accuracy: 0.5768\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6258 - accuracy: 0.6780\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.678 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6832 - accuracy: 0.5505\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6447 - accuracy: 0.6374\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.637 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6648 - accuracy: 0.6012\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6321 - accuracy: 0.6538\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.654 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6765 - accuracy: 0.5748\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6367 - accuracy: 0.6468\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.647 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6757 - accuracy: 0.5716\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6727\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.673 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6914 - accuracy: 0.5247\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6795 - accuracy: 0.5907\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.591 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6903 - accuracy: 0.5380\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6662 - accuracy: 0.6676\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6926 - accuracy: 0.5158\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6832 - accuracy: 0.6411\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.641 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6935 - accuracy: 0.5161\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6810 - accuracy: 0.6149\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.615 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6922 - accuracy: 0.5241\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6731 - accuracy: 0.6376\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.638 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6916 - accuracy: 0.5314\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6768 - accuracy: 0.6509\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.651 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6887 - accuracy: 0.5381\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6584 - accuracy: 0.6565\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.657 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6861 - accuracy: 0.5452\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6714 - accuracy: 0.5695\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.570 total time=   7.5s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6948 - accuracy: 0.5089\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6883 - accuracy: 0.5748\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.575 total time=   8.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6907 - accuracy: 0.5296\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6676 - accuracy: 0.6578\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.658 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6887 - accuracy: 0.5426\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6657 - accuracy: 0.6154\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.615 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6915 - accuracy: 0.5269\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6735 - accuracy: 0.6425\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.643 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6827 - accuracy: 0.5610\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6430 - accuracy: 0.6540\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.654 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6891 - accuracy: 0.5434\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6628 - accuracy: 0.6570\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.657 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6936 - accuracy: 0.5129\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6857 - accuracy: 0.5051\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.505 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6888 - accuracy: 0.5385\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6644 - accuracy: 0.5973\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.597 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6859 - accuracy: 0.5558\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6466 - accuracy: 0.6592\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.659 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6845 - accuracy: 0.5473\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6584\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.658 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6860 - accuracy: 0.5467\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6491 - accuracy: 0.6518\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.652 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6881 - accuracy: 0.5381\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6636 - accuracy: 0.6412\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.641 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6910 - accuracy: 0.5338\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6687 - accuracy: 0.6577\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.658 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6928 - accuracy: 0.5176\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6795 - accuracy: 0.6363\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.636 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6861 - accuracy: 0.5494\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6558 - accuracy: 0.6174\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.617 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6874 - accuracy: 0.5367\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6589\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.659 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6847 - accuracy: 0.5531\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6512 - accuracy: 0.6316\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.632 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6880 - accuracy: 0.5397\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6005\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.601 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6853 - accuracy: 0.5443\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6576 - accuracy: 0.6015\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.602 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6851 - accuracy: 0.5521\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6551 - accuracy: 0.6509\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.651 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6892 - accuracy: 0.5334\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6568 - accuracy: 0.6707\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.671 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6866 - accuracy: 0.5493\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6341\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.634 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6888 - accuracy: 0.5380\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6558 - accuracy: 0.6576\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.658 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6845 - accuracy: 0.5512\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6445 - accuracy: 0.6695\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.669 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6782 - accuracy: 0.5751\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6328 - accuracy: 0.6632\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.663 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6842 - accuracy: 0.5604\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6419 - accuracy: 0.6622\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.662 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6866 - accuracy: 0.5414\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6427 - accuracy: 0.6654\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.665 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6869 - accuracy: 0.5477\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6567 - accuracy: 0.6326\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.633 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6883 - accuracy: 0.5407\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6611 - accuracy: 0.6570\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.657 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6866 - accuracy: 0.5482\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6538 - accuracy: 0.6585\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.658 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6909 - accuracy: 0.5328\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6699 - accuracy: 0.6384\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.638 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6861 - accuracy: 0.5382\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6513 - accuracy: 0.6452\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.645 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6875 - accuracy: 0.5485\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6750\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.675 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6899 - accuracy: 0.5366\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6619 - accuracy: 0.6416\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.642 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6781 - accuracy: 0.5675\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6387 - accuracy: 0.6479\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.648 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6900 - accuracy: 0.5306\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6601 - accuracy: 0.6609\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.661 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6860 - accuracy: 0.5496\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6492 - accuracy: 0.6496\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.650 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6773 - accuracy: 0.5727\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6413 - accuracy: 0.6482\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.648 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6750 - accuracy: 0.5819\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6366 - accuracy: 0.6613\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.661 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6808 - accuracy: 0.5579\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6485 - accuracy: 0.6222\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.622 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6820 - accuracy: 0.5576\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6441 - accuracy: 0.6534\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.653 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6881 - accuracy: 0.5450\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6257\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.626 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6839 - accuracy: 0.5534\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6390 - accuracy: 0.6645\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.665 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6880 - accuracy: 0.5380\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6591\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.659 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6837 - accuracy: 0.5509\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6377 - accuracy: 0.6569\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.657 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6894 - accuracy: 0.5356\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6638 - accuracy: 0.6567\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.657 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6861 - accuracy: 0.5475\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6541 - accuracy: 0.6222\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.622 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6835 - accuracy: 0.5545\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6493 - accuracy: 0.6570\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.657 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6800 - accuracy: 0.5618\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6426 - accuracy: 0.6539\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.654 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6884 - accuracy: 0.5367\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6562\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.656 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6801 - accuracy: 0.5623\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6355 - accuracy: 0.6794\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.679 total time=   7.6s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6840 - accuracy: 0.5519\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6411 - accuracy: 0.6688\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.669 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6906 - accuracy: 0.5265\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6692 - accuracy: 0.6520\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.652 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6878 - accuracy: 0.5363\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6486 - accuracy: 0.6688\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.669 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6772 - accuracy: 0.5697\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6295 - accuracy: 0.6671\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.667 total time=   7.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6826 - accuracy: 0.5602\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6445 - accuracy: 0.6595\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.660 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6865 - accuracy: 0.5448\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6506 - accuracy: 0.6543\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.654 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6836 - accuracy: 0.5558\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6401 - accuracy: 0.6696\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.670 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6801 - accuracy: 0.5634\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6381 - accuracy: 0.6561\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.656 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6793 - accuracy: 0.5636\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6694\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.669 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6870 - accuracy: 0.5447\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6467 - accuracy: 0.6631\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.663 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6813 - accuracy: 0.5603\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6614\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.661 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6783 - accuracy: 0.5653\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6656\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.666 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6794 - accuracy: 0.5645\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6368 - accuracy: 0.6518\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.652 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6839 - accuracy: 0.5514\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6444 - accuracy: 0.6486\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.649 total time=   6.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6807 - accuracy: 0.5619\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6751\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   5.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6808 - accuracy: 0.5561\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6335 - accuracy: 0.6699\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.670 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6741 - accuracy: 0.5776\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6352 - accuracy: 0.6603\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.660 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6698 - accuracy: 0.5913\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6616\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.662 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6813 - accuracy: 0.5596\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6716\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.672 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6762 - accuracy: 0.5736\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6338 - accuracy: 0.6561\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.656 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6806 - accuracy: 0.5599\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6649\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.665 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6742 - accuracy: 0.5780\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6724\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.672 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6907 - accuracy: 0.5325\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6732 - accuracy: 0.6158\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.616 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6923 - accuracy: 0.5251\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6745 - accuracy: 0.6702\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.670 total time=   6.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6880 - accuracy: 0.5488\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6575 - accuracy: 0.6667\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.667 total time=   7.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6913 - accuracy: 0.5333\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6713 - accuracy: 0.6647\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.665 total time=   8.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6849 - accuracy: 0.5544\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6444 - accuracy: 0.6502\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.650 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6869 - accuracy: 0.5380\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6558 - accuracy: 0.6590\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.659 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6884 - accuracy: 0.5410\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6562 - accuracy: 0.6488\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.649 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6869 - accuracy: 0.5402\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6341\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.634 total time=   7.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6896 - accuracy: 0.5384\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6603 - accuracy: 0.6602\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.660 total time=   6.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6866 - accuracy: 0.5513\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6531 - accuracy: 0.6606\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.661 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6913 - accuracy: 0.5290\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6761 - accuracy: 0.5753\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.575 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6864 - accuracy: 0.5474\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6690\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.669 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6903 - accuracy: 0.5350\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6603 - accuracy: 0.6609\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.661 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6836 - accuracy: 0.5531\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6405 - accuracy: 0.6765\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.676 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6784 - accuracy: 0.5687\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6607\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.661 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6807 - accuracy: 0.5660\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6430 - accuracy: 0.6460\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.646 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6827 - accuracy: 0.5535\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6369 - accuracy: 0.6730\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.673 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6842 - accuracy: 0.5562\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6448 - accuracy: 0.6644\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.664 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6876 - accuracy: 0.5420\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6429\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.643 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6919 - accuracy: 0.5259\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6723 - accuracy: 0.6316\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.632 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6900 - accuracy: 0.5284\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6681\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.668 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6889 - accuracy: 0.5398\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6074\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.607 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6843 - accuracy: 0.5588\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6483 - accuracy: 0.6387\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.639 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6860 - accuracy: 0.5486\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6397 - accuracy: 0.6643\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.664 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6823 - accuracy: 0.5587\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6162\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.616 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6777 - accuracy: 0.5710\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6718\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.672 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6754 - accuracy: 0.5741\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6283 - accuracy: 0.6685\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.668 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6871 - accuracy: 0.5465\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6644 - accuracy: 0.5997\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.600 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6819 - accuracy: 0.5649\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6617\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.662 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6872 - accuracy: 0.5534\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6570 - accuracy: 0.6203\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.620 total time=   8.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6801 - accuracy: 0.5678\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6380 - accuracy: 0.6543\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.654 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6848 - accuracy: 0.5568\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6423 - accuracy: 0.6541\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.654 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6860 - accuracy: 0.5426\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6490 - accuracy: 0.6624\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.662 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6861 - accuracy: 0.5471\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6610 - accuracy: 0.5991\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.599 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6843 - accuracy: 0.5531\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6527\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.653 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6884 - accuracy: 0.5389\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6518 - accuracy: 0.6560\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.656 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6808 - accuracy: 0.5671\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6429 - accuracy: 0.6476\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.648 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6852 - accuracy: 0.5512\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6397 - accuracy: 0.6724\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6870 - accuracy: 0.5478\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6534 - accuracy: 0.6304\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.630 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6826 - accuracy: 0.5574\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6426 - accuracy: 0.6461\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.646 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6867 - accuracy: 0.5474\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6526 - accuracy: 0.6449\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.645 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6872 - accuracy: 0.5406\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6510 - accuracy: 0.6484\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.648 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6896 - accuracy: 0.5309\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6584 - accuracy: 0.6315\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.632 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6853 - accuracy: 0.5433\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6419 - accuracy: 0.6501\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.650 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6769 - accuracy: 0.5697\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6747\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.675 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6814 - accuracy: 0.5636\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6437 - accuracy: 0.6479\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.648 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6787 - accuracy: 0.5680\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6424 - accuracy: 0.6641\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.664 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6791 - accuracy: 0.5689\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6350 - accuracy: 0.6614\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.661 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6713 - accuracy: 0.5890\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6312 - accuracy: 0.6634\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.663 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6802 - accuracy: 0.5567\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6267 - accuracy: 0.6823\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.682 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6767 - accuracy: 0.5736\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6329 - accuracy: 0.6730\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.673 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6820 - accuracy: 0.5569\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6406 - accuracy: 0.6539\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.654 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6795 - accuracy: 0.5670\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6356 - accuracy: 0.6512\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.651 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6802 - accuracy: 0.5610\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6489 - accuracy: 0.6279\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.628 total time=   7.5s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6726 - accuracy: 0.5836\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6294 - accuracy: 0.6645\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.665 total time=   8.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6803 - accuracy: 0.5701\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6411 - accuracy: 0.6574\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.657 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6790 - accuracy: 0.5667\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6322 - accuracy: 0.6645\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.665 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6843 - accuracy: 0.5517\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6489 - accuracy: 0.6411\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.641 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6816 - accuracy: 0.5638\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6356 - accuracy: 0.6494\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.649 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6839 - accuracy: 0.5488\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6440 - accuracy: 0.6410\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.641 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6847 - accuracy: 0.5533\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6446 - accuracy: 0.6496\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.650 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6824 - accuracy: 0.5570\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6703\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.670 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6795 - accuracy: 0.5636\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6354 - accuracy: 0.6635\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.663 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6881 - accuracy: 0.5411\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6516 - accuracy: 0.6548\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.655 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6844 - accuracy: 0.5505\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6419 - accuracy: 0.6633\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.663 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6771 - accuracy: 0.5762\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6296 - accuracy: 0.6673\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.667 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6824 - accuracy: 0.5611\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6532 - accuracy: 0.6179\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.618 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6742 - accuracy: 0.5774\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6627\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.663 total time=   7.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6703 - accuracy: 0.5849\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6716\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.672 total time=   6.3s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6747 - accuracy: 0.5791\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6281 - accuracy: 0.6643\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.664 total time=   5.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6736 - accuracy: 0.5759\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6730\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.673 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6828 - accuracy: 0.5490\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6544\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.654 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6715 - accuracy: 0.5850\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6548\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.655 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6748 - accuracy: 0.5746\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6609\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.661 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6818 - accuracy: 0.5556\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6416 - accuracy: 0.6476\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.648 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6702 - accuracy: 0.5878\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6313 - accuracy: 0.6607\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.661 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6747 - accuracy: 0.5754\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6704\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6815 - accuracy: 0.5581\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6440 - accuracy: 0.6547\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.655 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6689 - accuracy: 0.5891\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6635\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.663 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6800 - accuracy: 0.5603\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6812\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.681 total time=   7.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6781 - accuracy: 0.5657\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.6629\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.663 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6867 - accuracy: 0.5417\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.6686\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.669 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6823 - accuracy: 0.5597\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6394 - accuracy: 0.6739\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.674 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6868 - accuracy: 0.5461\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6730\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.673 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6829 - accuracy: 0.5572\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6479\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.648 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6920 - accuracy: 0.5272\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6750 - accuracy: 0.6289\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.629 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6903 - accuracy: 0.5368\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6509\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.651 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6860 - accuracy: 0.5418\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6587\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.659 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6819 - accuracy: 0.5535\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6541\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.654 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6816 - accuracy: 0.5585\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6329 - accuracy: 0.6688\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.669 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6824 - accuracy: 0.5548\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6387 - accuracy: 0.6686\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.669 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6906 - accuracy: 0.5306\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6655 - accuracy: 0.6556\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.656 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6831 - accuracy: 0.5587\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6339 - accuracy: 0.6679\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.668 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6855 - accuracy: 0.5509\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6560\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.656 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6750 - accuracy: 0.5766\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6209 - accuracy: 0.6796\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.680 total time=   7.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6851 - accuracy: 0.5534\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6449 - accuracy: 0.6651\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.665 total time=   5.8s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6820 - accuracy: 0.5556\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6475 - accuracy: 0.6268\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.627 total time=   5.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6714 - accuracy: 0.5900\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6232 - accuracy: 0.6729\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.673 total time=   6.5s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6802 - accuracy: 0.5660\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6275 - accuracy: 0.6704\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.670 total time=   5.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6819 - accuracy: 0.5546\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6407 - accuracy: 0.6563\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.656 total time=   6.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6906 - accuracy: 0.5292\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6690 - accuracy: 0.5875\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.588 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6863 - accuracy: 0.5476\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6331\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.633 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6789 - accuracy: 0.5665\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6621\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.662 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6792 - accuracy: 0.5690\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6731\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.673 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6809 - accuracy: 0.5603\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6628\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.663 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6679 - accuracy: 0.5910\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6671\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   7.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6703 - accuracy: 0.5902\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6250 - accuracy: 0.6738\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6740 - accuracy: 0.5833\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6657\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.666 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6797 - accuracy: 0.5720\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6423 - accuracy: 0.6450\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.645 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6804 - accuracy: 0.5634\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6390 - accuracy: 0.6508\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.651 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6788 - accuracy: 0.5674\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.6701\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.670 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6894 - accuracy: 0.5329\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6611 - accuracy: 0.6490\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.649 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6822 - accuracy: 0.5591\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6519\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.652 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6790 - accuracy: 0.5715\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6407 - accuracy: 0.6598\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.660 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6708 - accuracy: 0.5864\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6358 - accuracy: 0.6511\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.651 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6835 - accuracy: 0.5502\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6343 - accuracy: 0.6623\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.662 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6759 - accuracy: 0.5764\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6253 - accuracy: 0.6718\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.672 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6855 - accuracy: 0.5485\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6629\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.663 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6859 - accuracy: 0.5465\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6478 - accuracy: 0.6462\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.646 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6832 - accuracy: 0.5563\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6359 - accuracy: 0.6644\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.664 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6765 - accuracy: 0.5699\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6644\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.664 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6798 - accuracy: 0.5686\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6252 - accuracy: 0.6751\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.675 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6721 - accuracy: 0.5819\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6227 - accuracy: 0.6743\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.674 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6860 - accuracy: 0.5437\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6426 - accuracy: 0.6590\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.659 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6812 - accuracy: 0.5615\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6305 - accuracy: 0.6753\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.675 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6746 - accuracy: 0.5777\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6301 - accuracy: 0.6638\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.664 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6742 - accuracy: 0.5768\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6310 - accuracy: 0.6593\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.659 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6822 - accuracy: 0.5574\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6691\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6810 - accuracy: 0.5635\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.6698\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.670 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6682 - accuracy: 0.5899\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6268 - accuracy: 0.6691\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.669 total time=   7.6s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6792 - accuracy: 0.5641\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6316 - accuracy: 0.6615\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.661 total time=   8.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6731 - accuracy: 0.5814\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6735\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6724 - accuracy: 0.5852\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6310 - accuracy: 0.6653\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.665 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6746 - accuracy: 0.5800\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6252 - accuracy: 0.6746\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.675 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6787 - accuracy: 0.5664\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6423 - accuracy: 0.6393\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.639 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6812 - accuracy: 0.5613\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6395 - accuracy: 0.6548\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.655 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6749 - accuracy: 0.5765\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6284 - accuracy: 0.6722\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.672 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6751 - accuracy: 0.5779\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6250 - accuracy: 0.6759\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.676 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6800 - accuracy: 0.5638\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6366 - accuracy: 0.6618\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.662 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6790 - accuracy: 0.5634\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6714\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.671 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6809 - accuracy: 0.5614\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6361 - accuracy: 0.6641\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.664 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6695 - accuracy: 0.5873\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6314 - accuracy: 0.6623\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.662 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6704 - accuracy: 0.5855\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6292 - accuracy: 0.6726\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6805 - accuracy: 0.5587\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6298 - accuracy: 0.6650\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.665 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6758 - accuracy: 0.5796\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6612\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.661 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6840 - accuracy: 0.5487\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6472\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.647 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6816 - accuracy: 0.5606\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6379 - accuracy: 0.6608\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.661 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6732 - accuracy: 0.5796\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6393 - accuracy: 0.6533\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.653 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6755 - accuracy: 0.5783\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6793\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.679 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6769 - accuracy: 0.5693\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6335 - accuracy: 0.6720\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.672 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6652 - accuracy: 0.6003\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6292 - accuracy: 0.6647\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.665 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6621 - accuracy: 0.6054\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6216 - accuracy: 0.6692\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.669 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6761 - accuracy: 0.5750\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6697\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.670 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6773 - accuracy: 0.5703\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6474\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.647 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6671 - accuracy: 0.5951\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6219 - accuracy: 0.6754\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6647 - accuracy: 0.5972\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6247 - accuracy: 0.6697\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.670 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6747 - accuracy: 0.5737\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6301 - accuracy: 0.6586\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.659 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6725 - accuracy: 0.5836\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6723\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.672 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6684 - accuracy: 0.5894\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6251 - accuracy: 0.6695\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.669 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6678 - accuracy: 0.5880\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6693\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.669 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6744 - accuracy: 0.5736\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6191 - accuracy: 0.6757\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.676 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6700 - accuracy: 0.5825\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6253 - accuracy: 0.6730\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.673 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6867 - accuracy: 0.5444\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6587 - accuracy: 0.6173\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.617 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6909 - accuracy: 0.5354\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6677 - accuracy: 0.6584\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.658 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6904 - accuracy: 0.5375\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6673 - accuracy: 0.6376\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.638 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6875 - accuracy: 0.5452\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6576 - accuracy: 0.6536\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.654 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6871 - accuracy: 0.5437\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6514 - accuracy: 0.6709\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.671 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6878 - accuracy: 0.5446\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6579 - accuracy: 0.6370\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.637 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6862 - accuracy: 0.5479\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6497 - accuracy: 0.6475\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.648 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6905 - accuracy: 0.5289\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6583 - accuracy: 0.6736\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.674 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6849 - accuracy: 0.5461\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6411 - accuracy: 0.6685\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.668 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6819 - accuracy: 0.5581\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6384 - accuracy: 0.6662\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.666 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6872 - accuracy: 0.5415\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6492 - accuracy: 0.6731\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.673 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6833 - accuracy: 0.5613\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6432 - accuracy: 0.6637\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.664 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6802 - accuracy: 0.5663\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6381 - accuracy: 0.6594\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.659 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6837 - accuracy: 0.5544\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6363 - accuracy: 0.6711\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.671 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6816 - accuracy: 0.5632\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6334 - accuracy: 0.6677\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.668 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6764 - accuracy: 0.5762\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6664\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.666 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6844 - accuracy: 0.5554\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6435 - accuracy: 0.6669\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.667 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6878 - accuracy: 0.5406\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6555 - accuracy: 0.6323\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.632 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6827 - accuracy: 0.5565\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6392 - accuracy: 0.6566\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.657 total time=   7.5s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6856 - accuracy: 0.5465\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6465 - accuracy: 0.6726\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.673 total time=   8.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6844 - accuracy: 0.5554\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6447 - accuracy: 0.6689\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6854 - accuracy: 0.5504\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6454 - accuracy: 0.6624\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.662 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6860 - accuracy: 0.5461\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6350 - accuracy: 0.6703\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6751 - accuracy: 0.5795\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6328 - accuracy: 0.6670\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.667 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6859 - accuracy: 0.5439\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6461 - accuracy: 0.6570\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.657 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6807 - accuracy: 0.5603\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6295 - accuracy: 0.6690\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.669 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6819 - accuracy: 0.5568\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6370 - accuracy: 0.6592\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.659 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6893 - accuracy: 0.5348\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6606 - accuracy: 0.6237\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.624 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6909 - accuracy: 0.5248\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6662 - accuracy: 0.6449\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.645 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6882 - accuracy: 0.5386\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6575 - accuracy: 0.6348\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.635 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6892 - accuracy: 0.5373\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6643 - accuracy: 0.6159\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.616 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6819 - accuracy: 0.5617\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6420 - accuracy: 0.6452\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.645 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6884 - accuracy: 0.5447\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6589 - accuracy: 0.6520\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.652 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6732 - accuracy: 0.5815\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6324 - accuracy: 0.6617\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.662 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6864 - accuracy: 0.5446\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6530 - accuracy: 0.6364\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.636 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6819 - accuracy: 0.5585\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6387 - accuracy: 0.6614\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.661 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6888 - accuracy: 0.5371\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6566 - accuracy: 0.6612\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.661 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6903 - accuracy: 0.5339\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6579 - accuracy: 0.6656\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.666 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6815 - accuracy: 0.5614\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6379 - accuracy: 0.6547\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.655 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6832 - accuracy: 0.5582\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6623\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.662 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6814 - accuracy: 0.5593\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6308 - accuracy: 0.6711\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.671 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6791 - accuracy: 0.5662\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6330 - accuracy: 0.6690\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.669 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6768 - accuracy: 0.5778\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6298 - accuracy: 0.6679\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6851 - accuracy: 0.5455\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6518 - accuracy: 0.6405\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.641 total time=   7.5s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6827 - accuracy: 0.5582\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6354 - accuracy: 0.6705\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.671 total time=   8.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6805 - accuracy: 0.5621\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6376 - accuracy: 0.6575\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.657 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6805 - accuracy: 0.5595\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6720\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.672 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6790 - accuracy: 0.5720\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6338 - accuracy: 0.6692\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6725 - accuracy: 0.5851\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6350 - accuracy: 0.6562\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.656 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6836 - accuracy: 0.5551\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6367 - accuracy: 0.6705\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6847 - accuracy: 0.5474\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6496 - accuracy: 0.6266\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.627 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6779 - accuracy: 0.5721\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6392 - accuracy: 0.6487\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.649 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6807 - accuracy: 0.5537\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6455 - accuracy: 0.6363\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.636 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6791 - accuracy: 0.5651\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6283 - accuracy: 0.6654\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.665 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6774 - accuracy: 0.5725\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6403 - accuracy: 0.6562\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.656 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6815 - accuracy: 0.5635\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6385 - accuracy: 0.6451\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.645 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6847 - accuracy: 0.5499\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6372 - accuracy: 0.6702\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.670 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6812 - accuracy: 0.5658\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6373 - accuracy: 0.6608\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.661 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6749 - accuracy: 0.5784\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6221 - accuracy: 0.6849\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.685 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6780 - accuracy: 0.5724\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6281 - accuracy: 0.6734\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.673 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6775 - accuracy: 0.5711\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6651\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.665 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6773 - accuracy: 0.5660\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6804\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.680 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6773 - accuracy: 0.5712\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6711\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.671 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6821 - accuracy: 0.5542\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6394 - accuracy: 0.6702\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.670 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6769 - accuracy: 0.5767\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6753\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.675 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6804 - accuracy: 0.5638\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6422 - accuracy: 0.6571\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.657 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6709 - accuracy: 0.5844\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6586\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.659 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6801 - accuracy: 0.5646\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6718\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.672 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6852 - accuracy: 0.5484\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6435 - accuracy: 0.6529\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.653 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6817 - accuracy: 0.5622\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6351 - accuracy: 0.6679\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   7.5s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6781 - accuracy: 0.5671\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6241 - accuracy: 0.6778\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.678 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6753 - accuracy: 0.5776\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6313 - accuracy: 0.6643\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.664 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6788 - accuracy: 0.5650\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6409 - accuracy: 0.6505\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.651 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6744 - accuracy: 0.5755\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6241 - accuracy: 0.6686\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6802 - accuracy: 0.5609\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6329 - accuracy: 0.6707\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.671 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6788 - accuracy: 0.5612\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6434 - accuracy: 0.6389\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.639 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6789 - accuracy: 0.5682\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6307 - accuracy: 0.6635\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.663 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6793 - accuracy: 0.5632\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6316 - accuracy: 0.6651\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.665 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6728 - accuracy: 0.5849\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6331 - accuracy: 0.6606\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.661 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6734 - accuracy: 0.5764\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6192 - accuracy: 0.6743\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6721 - accuracy: 0.5817\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6230 - accuracy: 0.6749\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.675 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6858 - accuracy: 0.5522\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6506 - accuracy: 0.6436\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.644 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6867 - accuracy: 0.5438\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6603 - accuracy: 0.6436\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.644 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6814 - accuracy: 0.5671\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6407 - accuracy: 0.6635\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.664 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6803 - accuracy: 0.5646\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6354 - accuracy: 0.6721\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.672 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6867 - accuracy: 0.5475\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6503 - accuracy: 0.6411\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.641 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6854 - accuracy: 0.5565\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6470 - accuracy: 0.6509\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.651 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6756 - accuracy: 0.5758\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6360 - accuracy: 0.6572\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.657 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6891 - accuracy: 0.5392\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6538 - accuracy: 0.6639\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.664 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6792 - accuracy: 0.5697\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.6696\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.670 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6831 - accuracy: 0.5595\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6403 - accuracy: 0.6554\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.655 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6834 - accuracy: 0.5537\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6368 - accuracy: 0.6719\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6863 - accuracy: 0.5431\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6465 - accuracy: 0.6501\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.650 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6779 - accuracy: 0.5683\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6326 - accuracy: 0.6675\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.667 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6845 - accuracy: 0.5518\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6383 - accuracy: 0.6702\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.670 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6841 - accuracy: 0.5548\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6366 - accuracy: 0.6688\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.669 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6805 - accuracy: 0.5664\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6404 - accuracy: 0.6544\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.654 total time=   8.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6807 - accuracy: 0.5628\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6373 - accuracy: 0.6478\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.648 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6774 - accuracy: 0.5653\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6304 - accuracy: 0.6676\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6744 - accuracy: 0.5759\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6324 - accuracy: 0.6606\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.661 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6761 - accuracy: 0.5762\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6336 - accuracy: 0.6635\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.663 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6783 - accuracy: 0.5716\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6365 - accuracy: 0.6529\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.653 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6747 - accuracy: 0.5777\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6346 - accuracy: 0.6557\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.656 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6801 - accuracy: 0.5598\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6292 - accuracy: 0.6700\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6764 - accuracy: 0.5697\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6301 - accuracy: 0.6618\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.662 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6738 - accuracy: 0.5789\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6335 - accuracy: 0.6568\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.657 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6778 - accuracy: 0.5713\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6293 - accuracy: 0.6656\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.666 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6793 - accuracy: 0.5664\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6320 - accuracy: 0.6668\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6849 - accuracy: 0.5498\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6451 - accuracy: 0.6550\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.655 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6835 - accuracy: 0.5524\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6532 - accuracy: 0.6233\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.623 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6887 - accuracy: 0.5397\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6578 - accuracy: 0.6357\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.636 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6734 - accuracy: 0.5726\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6307 - accuracy: 0.6642\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.664 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6890 - accuracy: 0.5351\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6493 - accuracy: 0.6746\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.675 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6825 - accuracy: 0.5559\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6333 - accuracy: 0.6720\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.672 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6797 - accuracy: 0.5642\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6360 - accuracy: 0.6591\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.659 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6815 - accuracy: 0.5535\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6402 - accuracy: 0.6618\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.662 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6751 - accuracy: 0.5777\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6311 - accuracy: 0.6678\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.668 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6802 - accuracy: 0.5713\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6368 - accuracy: 0.6654\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.665 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6852 - accuracy: 0.5487\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6410 - accuracy: 0.6728\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.673 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6804 - accuracy: 0.5732\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6378 - accuracy: 0.6601\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.660 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6767 - accuracy: 0.5740\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6346 - accuracy: 0.6595\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.660 total time=   7.5s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6846 - accuracy: 0.5482\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6388 - accuracy: 0.6755\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.675 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6845 - accuracy: 0.5515\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6379 - accuracy: 0.6738\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.674 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6781 - accuracy: 0.5630\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6454 - accuracy: 0.6364\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.636 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6776 - accuracy: 0.5694\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.6778\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.678 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6721 - accuracy: 0.5820\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6297 - accuracy: 0.6632\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.663 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6724 - accuracy: 0.5822\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6323 - accuracy: 0.6642\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.664 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6775 - accuracy: 0.5662\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6244 - accuracy: 0.6778\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.678 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6724 - accuracy: 0.5826\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6264 - accuracy: 0.6754\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6746 - accuracy: 0.5733\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6322 - accuracy: 0.6624\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.662 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6787 - accuracy: 0.5653\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6331 - accuracy: 0.6607\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.661 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6741 - accuracy: 0.5807\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6246 - accuracy: 0.6753\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.675 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6719 - accuracy: 0.5851\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6291 - accuracy: 0.6630\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.663 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6716 - accuracy: 0.5806\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6226 - accuracy: 0.6740\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6703 - accuracy: 0.5857\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6530\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.653 total time=   7.4s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6771 - accuracy: 0.5745\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6395 - accuracy: 0.6489\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.649 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6855 - accuracy: 0.5443\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6359 - accuracy: 0.6723\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.672 total time=   7.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6739 - accuracy: 0.5794\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6703\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.670 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6726 - accuracy: 0.5858\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6673\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.667 total time=   6.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6907 - accuracy: 0.5271\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6594 - accuracy: 0.6625\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.663 total time=   7.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6838 - accuracy: 0.5538\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6419 - accuracy: 0.6554\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.655 total time=   6.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6732 - accuracy: 0.5774\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6269 - accuracy: 0.6674\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.667 total time=   7.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6765 - accuracy: 0.5735\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6233 - accuracy: 0.6781\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.678 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6726 - accuracy: 0.5796\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6260 - accuracy: 0.6716\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.672 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6662 - accuracy: 0.5985\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6692\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.669 total time=   7.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6831 - accuracy: 0.5577\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6612\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.661 total time=   6.8s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6786 - accuracy: 0.5706\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6304 - accuracy: 0.6720\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   6.5s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6745 - accuracy: 0.5795\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6558\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.656 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6839 - accuracy: 0.5489\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6668\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.667 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6762 - accuracy: 0.5723\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6748\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.675 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6681 - accuracy: 0.5902\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6454 - accuracy: 0.6345\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.635 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6737 - accuracy: 0.5767\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6253 - accuracy: 0.6623\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.662 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6733 - accuracy: 0.5798\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6344 - accuracy: 0.6523\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.652 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6720 - accuracy: 0.5806\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6313 - accuracy: 0.6675\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.668 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6651 - accuracy: 0.6003\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6213 - accuracy: 0.6769\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.677 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6686 - accuracy: 0.5932\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6253 - accuracy: 0.6764\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.676 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6754 - accuracy: 0.5682\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6683\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.668 total time=   6.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6804 - accuracy: 0.5579\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6404 - accuracy: 0.6473\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.647 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6676 - accuracy: 0.5916\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6230 - accuracy: 0.6745\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6702 - accuracy: 0.5804\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6298 - accuracy: 0.6608\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.661 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6758 - accuracy: 0.5706\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6216 - accuracy: 0.6796\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.680 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6660 - accuracy: 0.5930\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6224 - accuracy: 0.6728\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.673 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6880 - accuracy: 0.5419\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6590 - accuracy: 0.6224\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.622 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6833 - accuracy: 0.5570\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6359 - accuracy: 0.6744\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.674 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6790 - accuracy: 0.5620\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6320 - accuracy: 0.6575\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.658 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6871 - accuracy: 0.5485\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6471 - accuracy: 0.6583\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.658 total time=   7.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6799 - accuracy: 0.5664\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6393 - accuracy: 0.6505\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.651 total time=   6.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6746 - accuracy: 0.5792\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6384 - accuracy: 0.6447\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.645 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6734 - accuracy: 0.5795\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6296 - accuracy: 0.6633\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.663 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6831 - accuracy: 0.5570\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6728\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6839 - accuracy: 0.5501\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6597\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.660 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6807 - accuracy: 0.5618\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6599\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.660 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6758 - accuracy: 0.5735\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6467 - accuracy: 0.6291\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.629 total time=   7.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6783 - accuracy: 0.5671\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.6662\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.666 total time=   6.7s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6734 - accuracy: 0.5838\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6354 - accuracy: 0.6549\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.655 total time=   5.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6813 - accuracy: 0.5640\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6582\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.658 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6794 - accuracy: 0.5696\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6680\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.668 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6733 - accuracy: 0.5806\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.6578\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.658 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6746 - accuracy: 0.5772\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6229 - accuracy: 0.6775\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.678 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6833 - accuracy: 0.5527\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6418 - accuracy: 0.6456\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.646 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6704 - accuracy: 0.5918\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6662\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.666 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6761 - accuracy: 0.5709\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6371 - accuracy: 0.6498\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.650 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6730 - accuracy: 0.5822\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6297 - accuracy: 0.6694\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6709 - accuracy: 0.5869\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6286 - accuracy: 0.6644\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.664 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6726 - accuracy: 0.5862\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6686\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.669 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6796 - accuracy: 0.5636\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6629\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.663 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6739 - accuracy: 0.5799\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6674\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6725 - accuracy: 0.5832\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6183 - accuracy: 0.6834\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.683 total time=   6.8s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6715 - accuracy: 0.5830\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6269 - accuracy: 0.6694\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.669 total time=   5.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6725 - accuracy: 0.5891\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6591\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.659 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6863 - accuracy: 0.5466\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6434 - accuracy: 0.6725\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.672 total time=   6.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6768 - accuracy: 0.5688\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6671\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.667 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6733 - accuracy: 0.5820\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6299 - accuracy: 0.6619\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.662 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6788 - accuracy: 0.5728\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6289 - accuracy: 0.6724\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.672 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6746 - accuracy: 0.5789\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6336 - accuracy: 0.6536\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.654 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6733 - accuracy: 0.5771\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6676\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.668 total time=   7.2s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6743 - accuracy: 0.5754\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6264 - accuracy: 0.6728\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   5.5s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6707 - accuracy: 0.5853\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6714\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.671 total time=   7.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6706 - accuracy: 0.5880\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6339\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.634 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6729 - accuracy: 0.5813\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6216 - accuracy: 0.6765\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.676 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6754 - accuracy: 0.5723\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6773\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.677 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6746 - accuracy: 0.5741\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6691\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.669 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6742 - accuracy: 0.5758\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6334\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.633 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6791 - accuracy: 0.5656\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6731\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.673 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6686 - accuracy: 0.5896\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6663\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.666 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6718 - accuracy: 0.5824\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6859\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.686 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6711 - accuracy: 0.5807\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6343 - accuracy: 0.6556\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.656 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6732 - accuracy: 0.5772\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6575\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.657 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6808 - accuracy: 0.5577\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6772\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.677 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6729 - accuracy: 0.5786\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6756\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.676 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6721 - accuracy: 0.5812\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6718\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.672 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6731 - accuracy: 0.5765\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6699\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6755 - accuracy: 0.5712\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6632\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.663 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6710 - accuracy: 0.5812\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6665\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.666 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6736 - accuracy: 0.5787\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6611\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.661 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6758 - accuracy: 0.5721\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6297 - accuracy: 0.6701\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.670 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6748 - accuracy: 0.5761\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6384 - accuracy: 0.6552\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.655 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6720 - accuracy: 0.5861\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6233 - accuracy: 0.6772\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.677 total time=   6.4s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6745 - accuracy: 0.5804\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6268 - accuracy: 0.6773\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.677 total time=   5.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6705 - accuracy: 0.5865\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6664\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.666 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6713 - accuracy: 0.5808\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6686\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.669 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6665 - accuracy: 0.5953\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6692\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.669 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6740 - accuracy: 0.5739\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6323 - accuracy: 0.6582\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.658 total time=   7.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6829 - accuracy: 0.5514\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6269 - accuracy: 0.6744\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.674 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6713 - accuracy: 0.5810\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6229 - accuracy: 0.6751\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.675 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6620 - accuracy: 0.6076\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6289 - accuracy: 0.6630\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.663 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6734 - accuracy: 0.5751\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6774\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.677 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6754 - accuracy: 0.5740\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6649\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.665 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6612 - accuracy: 0.6076\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6610\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.661 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6683 - accuracy: 0.5940\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6132 - accuracy: 0.6834\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.683 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6685 - accuracy: 0.5876\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6227 - accuracy: 0.6730\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.673 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6676 - accuracy: 0.5915\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6650\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.665 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6657 - accuracy: 0.5958\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6162 - accuracy: 0.6745\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.675 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6664 - accuracy: 0.5936\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6606\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.661 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6736 - accuracy: 0.5761\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6323 - accuracy: 0.6696\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.670 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6664 - accuracy: 0.6010\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6739\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.674 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6649 - accuracy: 0.5956\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6338 - accuracy: 0.6557\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.656 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6688 - accuracy: 0.5950\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6312 - accuracy: 0.6600\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.660 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6721 - accuracy: 0.5831\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.6614\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.661 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6641 - accuracy: 0.6014\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6238 - accuracy: 0.6719\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.672 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6638 - accuracy: 0.6016\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6695\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.669 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6659 - accuracy: 0.5952\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6164 - accuracy: 0.6802\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.680 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6633 - accuracy: 0.6044\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6241 - accuracy: 0.6695\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.669 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6560 - accuracy: 0.6209\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6353\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.635 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6606 - accuracy: 0.6086\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6204 - accuracy: 0.6813\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.681 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6619 - accuracy: 0.6095\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6735\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.674 total time=   6.9s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6507 - accuracy: 0.6294\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6615\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.662 total time=   6.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6512 - accuracy: 0.6263\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6246 - accuracy: 0.6608\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.661 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6482 - accuracy: 0.6314\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6293 - accuracy: 0.6708\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6530 - accuracy: 0.6265\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6566\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.657 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6568 - accuracy: 0.6148\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6781\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.678 total time=   6.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6525 - accuracy: 0.6235\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6292 - accuracy: 0.6620\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.662 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6534 - accuracy: 0.6274\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6268 - accuracy: 0.6707\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.671 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6542 - accuracy: 0.6254\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6212 - accuracy: 0.6721\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6497 - accuracy: 0.6279\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6667\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.667 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6548 - accuracy: 0.6230\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6462 - accuracy: 0.6439\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.644 total time=   6.5s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6570 - accuracy: 0.6247\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6250 - accuracy: 0.6768\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.677 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6506 - accuracy: 0.6296\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6201 - accuracy: 0.6741\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.674 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6560 - accuracy: 0.6182\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6249 - accuracy: 0.6699\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.670 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6513 - accuracy: 0.6290\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6200 - accuracy: 0.6825\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.682 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6524 - accuracy: 0.6209\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6779\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.678 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6494 - accuracy: 0.6285\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6309 - accuracy: 0.6685\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   6.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6538 - accuracy: 0.6250\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6171 - accuracy: 0.6804\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.680 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6512 - accuracy: 0.6262\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6229 - accuracy: 0.6695\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6454 - accuracy: 0.6360\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6721\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.672 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6553 - accuracy: 0.6227\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6653\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.665 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6567 - accuracy: 0.6097\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6348 - accuracy: 0.6635\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.664 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6495 - accuracy: 0.6286\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6286 - accuracy: 0.6645\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.665 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6553 - accuracy: 0.6184\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6160 - accuracy: 0.6798\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.680 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6473 - accuracy: 0.6320\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6237 - accuracy: 0.6768\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.677 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6484 - accuracy: 0.6378\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6267 - accuracy: 0.6722\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.672 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6489 - accuracy: 0.6372\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6175 - accuracy: 0.6803\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.680 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6504 - accuracy: 0.6313\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6782\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.678 total time=   6.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6537 - accuracy: 0.6230\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6660\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.666 total time=   6.9s\n",
      "708/708 [==============================] - 6s 6ms/step - loss: 0.6519 - accuracy: 0.6285\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6303 - accuracy: 0.6719\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.672 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6515 - accuracy: 0.6229\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6657\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.666 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6499 - accuracy: 0.6292\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6691\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.669 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6548 - accuracy: 0.6192\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6847\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.685 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6506 - accuracy: 0.6248\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6220 - accuracy: 0.6719\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.672 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6471 - accuracy: 0.6384\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6245 - accuracy: 0.6739\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.674 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6565 - accuracy: 0.6204\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6312 - accuracy: 0.6715\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6533 - accuracy: 0.6207\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6227 - accuracy: 0.6768\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.677 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6543 - accuracy: 0.6195\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6378 - accuracy: 0.6600\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.660 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6547 - accuracy: 0.6249\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6152 - accuracy: 0.6818\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.682 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6537 - accuracy: 0.6171\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6710\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.671 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6472 - accuracy: 0.6335\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6634\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.663 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6492 - accuracy: 0.6281\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6748\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.675 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6578 - accuracy: 0.6117\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6220 - accuracy: 0.6753\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.675 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6477 - accuracy: 0.6337\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6248 - accuracy: 0.6696\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.670 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6541 - accuracy: 0.6249\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6668\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.667 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6542 - accuracy: 0.6207\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6201 - accuracy: 0.6726\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.673 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6440 - accuracy: 0.6382\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6690\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.669 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6524 - accuracy: 0.6271\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6129 - accuracy: 0.6842\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.684 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6516 - accuracy: 0.6218\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6198 - accuracy: 0.6751\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.675 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6464 - accuracy: 0.6390\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6477\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.648 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6493 - accuracy: 0.6280\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6165 - accuracy: 0.6775\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.678 total time=   6.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6484 - accuracy: 0.6362\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6707\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.671 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6509 - accuracy: 0.6294\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6338 - accuracy: 0.6615\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.662 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6576 - accuracy: 0.6174\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6284 - accuracy: 0.6723\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.672 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6506 - accuracy: 0.6280\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6163 - accuracy: 0.6754\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.675 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6537 - accuracy: 0.6217\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6585\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.659 total time=   7.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6570 - accuracy: 0.6128\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.6510\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.651 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6505 - accuracy: 0.6290\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6679\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.668 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6470 - accuracy: 0.6359\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6319 - accuracy: 0.6596\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.660 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6551 - accuracy: 0.6196\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6204 - accuracy: 0.6852\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.685 total time=   6.5s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6483 - accuracy: 0.6305\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6740\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.674 total time=   5.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6472 - accuracy: 0.6314\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6595\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.660 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6537 - accuracy: 0.6256\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6162 - accuracy: 0.6820\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.682 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6490 - accuracy: 0.6309\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6479\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.648 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6482 - accuracy: 0.6332\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6268 - accuracy: 0.6721\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.672 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6519 - accuracy: 0.6284\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6138 - accuracy: 0.6827\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.683 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6514 - accuracy: 0.6237\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6251 - accuracy: 0.6782\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.678 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6511 - accuracy: 0.6269\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6626\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.663 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6505 - accuracy: 0.6296\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6763\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.676 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6529 - accuracy: 0.6240\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6292 - accuracy: 0.6718\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.672 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6473 - accuracy: 0.6304\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6403\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.640 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6555 - accuracy: 0.6242\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6192 - accuracy: 0.6759\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.676 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6597 - accuracy: 0.6142\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6728\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.673 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6470 - accuracy: 0.6330\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6224 - accuracy: 0.6724\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.672 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6497 - accuracy: 0.6337\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6675\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.667 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6515 - accuracy: 0.6313\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6804\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.680 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6522 - accuracy: 0.6232\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6370\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.637 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6498 - accuracy: 0.6323\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6368\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.637 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6536 - accuracy: 0.6205\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6292 - accuracy: 0.6656\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.666 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6506 - accuracy: 0.6331\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6215 - accuracy: 0.6754\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.675 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6584 - accuracy: 0.6162\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6640\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.664 total time=   6.8s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6553 - accuracy: 0.6189\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6221 - accuracy: 0.6739\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.674 total time=   8.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6534 - accuracy: 0.6223\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6639\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.664 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6535 - accuracy: 0.6289\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6237 - accuracy: 0.6814\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.681 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6531 - accuracy: 0.6218\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6208 - accuracy: 0.6767\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.677 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6437 - accuracy: 0.6393\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6201 - accuracy: 0.6782\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.678 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6543 - accuracy: 0.6208\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6429 - accuracy: 0.6364\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.636 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6456 - accuracy: 0.6357\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6773\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.677 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6478 - accuracy: 0.6322\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6229 - accuracy: 0.6684\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.668 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6559 - accuracy: 0.6186\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6203 - accuracy: 0.6837\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.684 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6476 - accuracy: 0.6359\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6712\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.671 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6473 - accuracy: 0.6383\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6249 - accuracy: 0.6650\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.665 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6568 - accuracy: 0.6159\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6810\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.681 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6553 - accuracy: 0.6192\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6656\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.666 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6436 - accuracy: 0.6410\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6250 - accuracy: 0.6663\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.666 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6542 - accuracy: 0.6198\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6159 - accuracy: 0.6824\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.682 total time=   6.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6515 - accuracy: 0.6291\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6290 - accuracy: 0.6708\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.671 total time=   5.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6485 - accuracy: 0.6322\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6312 - accuracy: 0.6728\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.673 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6574 - accuracy: 0.6155\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6267 - accuracy: 0.6807\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.681 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6546 - accuracy: 0.6176\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6317 - accuracy: 0.6650\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.665 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6469 - accuracy: 0.6372\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6667\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.667 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6477 - accuracy: 0.6312\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6115 - accuracy: 0.6807\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.681 total time=   7.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6502 - accuracy: 0.6277\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6265 - accuracy: 0.6652\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.665 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6455 - accuracy: 0.6390\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6674\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6513 - accuracy: 0.6300\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6123 - accuracy: 0.6815\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.682 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6451 - accuracy: 0.6354\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6768\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.677 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6493 - accuracy: 0.6318\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6383 - accuracy: 0.6490\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.649 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6557 - accuracy: 0.6225\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6195 - accuracy: 0.6812\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.681 total time=   6.9s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6535 - accuracy: 0.6312\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6401\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.640 total time=   6.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6513 - accuracy: 0.6269\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6680 - accuracy: 0.5760\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.576 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6533 - accuracy: 0.6276\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6781\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.678 total time=   6.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6506 - accuracy: 0.6325\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6745\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6464 - accuracy: 0.6356\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6304 - accuracy: 0.6707\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.671 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6565 - accuracy: 0.6138\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.6757\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.676 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6495 - accuracy: 0.6290\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6308 - accuracy: 0.6664\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.666 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6515 - accuracy: 0.6278\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6321 - accuracy: 0.6619\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.662 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6526 - accuracy: 0.6279\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6127 - accuracy: 0.6829\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.683 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6561 - accuracy: 0.6199\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6228 - accuracy: 0.6769\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.677 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6537 - accuracy: 0.6237\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6271 - accuracy: 0.6683\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.668 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6507 - accuracy: 0.6297\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6181 - accuracy: 0.6781\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.678 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6471 - accuracy: 0.6325\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6307 - accuracy: 0.6800\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.680 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6499 - accuracy: 0.6290\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6250 - accuracy: 0.6668\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.667 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6468 - accuracy: 0.6355\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6119 - accuracy: 0.6804\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.680 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6483 - accuracy: 0.6339\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6233 - accuracy: 0.6733\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.673 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6521 - accuracy: 0.6266\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6317 - accuracy: 0.6617\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.662 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6525 - accuracy: 0.6247\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6316 - accuracy: 0.6779\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.678 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6521 - accuracy: 0.6277\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6210 - accuracy: 0.6767\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.677 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6486 - accuracy: 0.6329\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6351 - accuracy: 0.6551\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.655 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6578 - accuracy: 0.6121\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6235 - accuracy: 0.6790\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.679 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6498 - accuracy: 0.6303\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6240 - accuracy: 0.6698\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6447 - accuracy: 0.6348\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6253 - accuracy: 0.6682\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.668 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6501 - accuracy: 0.6296\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6168 - accuracy: 0.6858\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.686 total time=   7.8s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6460 - accuracy: 0.6372\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6324 - accuracy: 0.6785\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.678 total time=   8.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6452 - accuracy: 0.6408\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6308 - accuracy: 0.6653\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.665 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6565 - accuracy: 0.6189\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6139 - accuracy: 0.6812\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.681 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6504 - accuracy: 0.6313\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6220 - accuracy: 0.6774\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.677 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6467 - accuracy: 0.6355\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6635\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.663 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6529 - accuracy: 0.6257\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6224 - accuracy: 0.6714\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.671 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6565 - accuracy: 0.6141\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6460 - accuracy: 0.6483\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.648 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6480 - accuracy: 0.6318\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6463 - accuracy: 0.6365\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.636 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6541 - accuracy: 0.6222\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6200 - accuracy: 0.6759\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.676 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6524 - accuracy: 0.6239\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6265 - accuracy: 0.6696\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.670 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6468 - accuracy: 0.6371\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6375 - accuracy: 0.6520\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.652 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6523 - accuracy: 0.6309\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6223 - accuracy: 0.6819\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.682 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6553 - accuracy: 0.6218\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6228 - accuracy: 0.6745\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.674 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6485 - accuracy: 0.6341\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6556 - accuracy: 0.6345\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.635 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6519 - accuracy: 0.6267\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6281 - accuracy: 0.6671\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.667 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6507 - accuracy: 0.6263\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6266 - accuracy: 0.6670\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.667 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6492 - accuracy: 0.6247\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6265 - accuracy: 0.6658\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.666 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6536 - accuracy: 0.6256\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6133 - accuracy: 0.6804\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.680 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6486 - accuracy: 0.6288\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6593\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.659 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6485 - accuracy: 0.6362\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6248 - accuracy: 0.6751\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6506 - accuracy: 0.6295\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6248 - accuracy: 0.6795\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.679 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6496 - accuracy: 0.6315\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6209 - accuracy: 0.6783\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.678 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6487 - accuracy: 0.6331\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6356 - accuracy: 0.6531\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.653 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6507 - accuracy: 0.6340\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6209 - accuracy: 0.6823\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.682 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6441 - accuracy: 0.6400\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6261 - accuracy: 0.6763\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.676 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6426 - accuracy: 0.6390\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6222 - accuracy: 0.6668\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   7.5s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6525 - accuracy: 0.6288\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6120 - accuracy: 0.6836\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.684 total time=   8.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6471 - accuracy: 0.6346\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6421\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.642 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6518 - accuracy: 0.6306\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6238 - accuracy: 0.6760\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.676 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6516 - accuracy: 0.6312\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6213 - accuracy: 0.6758\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.676 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6502 - accuracy: 0.6323\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6246 - accuracy: 0.6735\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.674 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6467 - accuracy: 0.6330\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6388 - accuracy: 0.6686\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.669 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6488 - accuracy: 0.6363\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6198 - accuracy: 0.6794\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.679 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6486 - accuracy: 0.6302\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6259 - accuracy: 0.6694\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.669 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6486 - accuracy: 0.6330\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6267 - accuracy: 0.6662\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.666 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6538 - accuracy: 0.6201\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6159 - accuracy: 0.6805\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.681 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6467 - accuracy: 0.6360\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6205 - accuracy: 0.6717\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.672 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6621 - accuracy: 0.6045\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6614\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.661 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6533 - accuracy: 0.6230\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6133 - accuracy: 0.6836\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.684 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6495 - accuracy: 0.6334\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6312 - accuracy: 0.6650\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.665 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6482 - accuracy: 0.6310\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6570\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.657 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6519 - accuracy: 0.6216\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6122 - accuracy: 0.6822\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.682 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6503 - accuracy: 0.6284\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6619\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.662 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6455 - accuracy: 0.6351\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6185 - accuracy: 0.6769\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.677 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6531 - accuracy: 0.6277\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6181 - accuracy: 0.6758\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.676 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6506 - accuracy: 0.6206\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6684\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6472 - accuracy: 0.6342\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6208 - accuracy: 0.6766\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.677 total time=   6.9s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6567 - accuracy: 0.6159\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6740\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.674 total time=   5.8s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6472 - accuracy: 0.6322\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6383 - accuracy: 0.6432\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.643 total time=   5.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6493 - accuracy: 0.6349\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6331 - accuracy: 0.6545\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.654 total time=   6.4s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6565 - accuracy: 0.6214\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6154 - accuracy: 0.6803\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.680 total time=   6.4s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6446 - accuracy: 0.6418\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6191 - accuracy: 0.6797\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.680 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6514 - accuracy: 0.6287\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6340 - accuracy: 0.6587\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.659 total time=   7.4s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6549 - accuracy: 0.6173\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6463 - accuracy: 0.6396\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.640 total time=   6.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6457 - accuracy: 0.6347\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6247 - accuracy: 0.6713\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.671 total time=   7.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6502 - accuracy: 0.6305\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6297 - accuracy: 0.6629\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.663 total time=   8.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6572 - accuracy: 0.6210\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6744\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.674 total time=   5.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6510 - accuracy: 0.6289\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6360 - accuracy: 0.6571\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.657 total time=   6.5s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6495 - accuracy: 0.6303\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6356 - accuracy: 0.6538\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.654 total time=   5.6s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6529 - accuracy: 0.6247\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6368 - accuracy: 0.6494\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.649 total time=   6.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6483 - accuracy: 0.6334\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6201\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.620 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6473 - accuracy: 0.6351\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6500 - accuracy: 0.6294\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.629 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6468 - accuracy: 0.6340\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6180 - accuracy: 0.6819\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.682 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6522 - accuracy: 0.6293\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6369\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.637 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6516 - accuracy: 0.6297\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6287 - accuracy: 0.6656\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.666 total time=   7.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6514 - accuracy: 0.6265\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6144 - accuracy: 0.6813\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.681 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6498 - accuracy: 0.6305\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6510\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.651 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6450 - accuracy: 0.6435\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6295 - accuracy: 0.6622\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.662 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6495 - accuracy: 0.6304\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6180 - accuracy: 0.6746\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.675 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6454 - accuracy: 0.6392\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6207 - accuracy: 0.6727\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.673 total time=   6.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6418 - accuracy: 0.6437\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6221 - accuracy: 0.6675\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.667 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6507 - accuracy: 0.6277\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6185 - accuracy: 0.6749\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.675 total time=   7.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6444 - accuracy: 0.6380\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6182 - accuracy: 0.6764\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.676 total time=   5.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6513 - accuracy: 0.6271\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6398 - accuracy: 0.6432\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.643 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6552 - accuracy: 0.6235\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6750\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6500 - accuracy: 0.6366\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6249 - accuracy: 0.6759\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.676 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6449 - accuracy: 0.6379\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6571\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.657 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6536 - accuracy: 0.6229\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6264 - accuracy: 0.6607\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.661 total time=   6.8s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6477 - accuracy: 0.6325\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6248 - accuracy: 0.6790\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.679 total time=   7.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6449 - accuracy: 0.6388\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6297 - accuracy: 0.6704\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.670 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6500 - accuracy: 0.6294\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6661\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.666 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6487 - accuracy: 0.6312\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6227 - accuracy: 0.6736\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   6.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6501 - accuracy: 0.6349\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6681\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6524 - accuracy: 0.6298\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.6681\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6476 - accuracy: 0.6405\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6252 - accuracy: 0.6754\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.675 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6467 - accuracy: 0.6340\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6248 - accuracy: 0.6716\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.672 total time=   6.4s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6565 - accuracy: 0.6148\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6135 - accuracy: 0.6859\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.686 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6479 - accuracy: 0.6338\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6601\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.660 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6472 - accuracy: 0.6371\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6352 - accuracy: 0.6744\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.674 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6524 - accuracy: 0.6260\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6570\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.657 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6447 - accuracy: 0.6365\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6206 - accuracy: 0.6749\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.675 total time=   7.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6482 - accuracy: 0.6341\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6695\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.669 total time=   6.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6482 - accuracy: 0.6405\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6149 - accuracy: 0.6854\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.685 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6451 - accuracy: 0.6395\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6216 - accuracy: 0.6779\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.678 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6410 - accuracy: 0.6473\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6652\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.665 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6485 - accuracy: 0.6347\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6232 - accuracy: 0.6761\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.676 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6561 - accuracy: 0.6135\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6201 - accuracy: 0.6772\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.677 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6492 - accuracy: 0.6305\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6684\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6479 - accuracy: 0.6345\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6212 - accuracy: 0.6824\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.682 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6522 - accuracy: 0.6270\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6722\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.672 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6450 - accuracy: 0.6413\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6345 - accuracy: 0.6502\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.650 total time=   6.9s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6477 - accuracy: 0.6368\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6203 - accuracy: 0.6749\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6444 - accuracy: 0.6418\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6234 - accuracy: 0.6794\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.679 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6462 - accuracy: 0.6354\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6711\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   6.3s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6558 - accuracy: 0.6180\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6813\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.681 total time=   7.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6453 - accuracy: 0.6395\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6725\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.672 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6450 - accuracy: 0.6415\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6706\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.671 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6458 - accuracy: 0.6363\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6158 - accuracy: 0.6693\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.669 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6434 - accuracy: 0.6384\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6241 - accuracy: 0.6694\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.669 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6526 - accuracy: 0.6261\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6620\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.662 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6554 - accuracy: 0.6177\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6123 - accuracy: 0.6843\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.684 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6558 - accuracy: 0.6170\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6252 - accuracy: 0.6779\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.678 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6517 - accuracy: 0.6260\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6253 - accuracy: 0.6745\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.675 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6572 - accuracy: 0.6123\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6156 - accuracy: 0.6804\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.680 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6485 - accuracy: 0.6287\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6286 - accuracy: 0.6628\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.663 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6469 - accuracy: 0.6343\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6284 - accuracy: 0.6653\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.665 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6547 - accuracy: 0.6196\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6129 - accuracy: 0.6843\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.684 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6530 - accuracy: 0.6226\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6654\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.665 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6510 - accuracy: 0.6325\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6620\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.662 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6546 - accuracy: 0.6254\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6494\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.649 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6553 - accuracy: 0.6166\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6202 - accuracy: 0.6752\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.675 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6528 - accuracy: 0.6245\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6336 - accuracy: 0.6601\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.660 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6542 - accuracy: 0.6242\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6168 - accuracy: 0.6835\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.684 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6497 - accuracy: 0.6316\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6761\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.676 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6510 - accuracy: 0.6269\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6713\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.671 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6540 - accuracy: 0.6194\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6203 - accuracy: 0.6728\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.673 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6518 - accuracy: 0.6256\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6181\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.618 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6502 - accuracy: 0.6292\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6253 - accuracy: 0.6745\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6548 - accuracy: 0.6221\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6156 - accuracy: 0.6863\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.686 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6520 - accuracy: 0.6243\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6698\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.670 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6489 - accuracy: 0.6356\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6650\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.665 total time=   7.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6535 - accuracy: 0.6232\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6411\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.641 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6481 - accuracy: 0.6362\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6643\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.664 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6461 - accuracy: 0.6378\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6679\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.668 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6518 - accuracy: 0.6288\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6486 - accuracy: 0.6220\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.622 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6449 - accuracy: 0.6366\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6367 - accuracy: 0.6456\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.646 total time=   7.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6480 - accuracy: 0.6340\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6562\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.656 total time=   5.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6576 - accuracy: 0.6177\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6292 - accuracy: 0.6666\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.667 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6577 - accuracy: 0.6114\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6764\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.676 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6483 - accuracy: 0.6289\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6275 - accuracy: 0.6592\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.659 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6541 - accuracy: 0.6240\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6502\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.650 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6496 - accuracy: 0.6320\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6190 - accuracy: 0.6756\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.676 total time=   6.5s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6457 - accuracy: 0.6309\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6248 - accuracy: 0.6693\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.669 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6499 - accuracy: 0.6335\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6217 - accuracy: 0.6797\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.680 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6505 - accuracy: 0.6279\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6610\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.661 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6518 - accuracy: 0.6292\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6343 - accuracy: 0.6600\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.660 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6547 - accuracy: 0.6223\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6795\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.679 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6493 - accuracy: 0.6282\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6693\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.669 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6494 - accuracy: 0.6260\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6394 - accuracy: 0.6515\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.651 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6507 - accuracy: 0.6303\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6668\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.667 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6485 - accuracy: 0.6294\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6766\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.677 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6488 - accuracy: 0.6276\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6606\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.661 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6499 - accuracy: 0.6263\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6215 - accuracy: 0.6853\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.685 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6437 - accuracy: 0.6382\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6198 - accuracy: 0.6748\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.675 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6488 - accuracy: 0.6322\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6718\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.672 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6570 - accuracy: 0.6164\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6146 - accuracy: 0.6766\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.677 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6527 - accuracy: 0.6275\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6238 - accuracy: 0.6678\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6483 - accuracy: 0.6306\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6234 - accuracy: 0.6755\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.675 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6519 - accuracy: 0.6250\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6708\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6463 - accuracy: 0.6380\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6201 - accuracy: 0.6764\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.676 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6446 - accuracy: 0.6413\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6264 - accuracy: 0.6753\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.675 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6532 - accuracy: 0.6221\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6180 - accuracy: 0.6785\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.679 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6531 - accuracy: 0.6250\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6210 - accuracy: 0.6787\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.679 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6486 - accuracy: 0.6344\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6378 - accuracy: 0.6597\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.660 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6543 - accuracy: 0.6290\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6776\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.678 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6509 - accuracy: 0.6287\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6220 - accuracy: 0.6767\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.677 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6450 - accuracy: 0.6436\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6347\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.635 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6565 - accuracy: 0.6230\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6698\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.670 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6520 - accuracy: 0.6269\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6703\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.670 total time=   6.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6438 - accuracy: 0.6402\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6720\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.672 total time=   6.8s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6499 - accuracy: 0.6314\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6123 - accuracy: 0.6859\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.686 total time=   5.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6510 - accuracy: 0.6246\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6360 - accuracy: 0.6433\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.643 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6488 - accuracy: 0.6345\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6700\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.670 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6519 - accuracy: 0.6290\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6194 - accuracy: 0.6742\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.674 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6524 - accuracy: 0.6244\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6296 - accuracy: 0.6741\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.674 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6449 - accuracy: 0.6398\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6216 - accuracy: 0.6704\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.670 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6500 - accuracy: 0.6325\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6165 - accuracy: 0.6849\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.685 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6475 - accuracy: 0.6330\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6187 - accuracy: 0.6741\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.674 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6439 - accuracy: 0.6404\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6317 - accuracy: 0.6730\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.673 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6461 - accuracy: 0.6366\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6124 - accuracy: 0.6819\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.682 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6500 - accuracy: 0.6289\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6695 - accuracy: 0.5901\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.590 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6475 - accuracy: 0.6353\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6311 - accuracy: 0.6713\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.671 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6512 - accuracy: 0.6271\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6278 - accuracy: 0.6667\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.667 total time=   8.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6458 - accuracy: 0.6415\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6233 - accuracy: 0.6749\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6426 - accuracy: 0.6406\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6213 - accuracy: 0.6744\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6528 - accuracy: 0.6232\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6140 - accuracy: 0.6834\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.683 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6489 - accuracy: 0.6355\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.6743\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6448 - accuracy: 0.6382\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6241 - accuracy: 0.6723\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.672 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6527 - accuracy: 0.6254\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6175 - accuracy: 0.6781\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.678 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6469 - accuracy: 0.6352\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6260 - accuracy: 0.6646\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.665 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6547 - accuracy: 0.6231\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6275 - accuracy: 0.6637\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.664 total time=   7.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6594 - accuracy: 0.6081\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6147 - accuracy: 0.6837\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.684 total time=   5.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6515 - accuracy: 0.6275\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6729\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.673 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6488 - accuracy: 0.6316\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6243 - accuracy: 0.6724\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.672 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6559 - accuracy: 0.6194\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6139 - accuracy: 0.6821\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.682 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6470 - accuracy: 0.6340\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6452 - accuracy: 0.6556\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.656 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6445 - accuracy: 0.6390\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6317 - accuracy: 0.6624\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.662 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6522 - accuracy: 0.6269\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6238 - accuracy: 0.6819\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.682 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6502 - accuracy: 0.6261\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6718\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.672 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6493 - accuracy: 0.6257\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6226 - accuracy: 0.6721\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   6.0s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6516 - accuracy: 0.6281\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6153 - accuracy: 0.6831\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.683 total time=   5.4s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6534 - accuracy: 0.6214\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.6779\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.678 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6476 - accuracy: 0.6331\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6744\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.674 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6539 - accuracy: 0.6207\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6179 - accuracy: 0.6793\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.679 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6537 - accuracy: 0.6224\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6202 - accuracy: 0.6723\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.672 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6434 - accuracy: 0.6468\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6398 - accuracy: 0.6592\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.659 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6494 - accuracy: 0.6311\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6180 - accuracy: 0.6839\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.684 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6472 - accuracy: 0.6326\n",
      "354/354 [==============================] - 2s 3ms/step - loss: 0.6173 - accuracy: 0.6809\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.681 total time=   7.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6518 - accuracy: 0.6310\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6230 - accuracy: 0.6703\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.670 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6539 - accuracy: 0.6254\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6150 - accuracy: 0.6810\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.681 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6522 - accuracy: 0.6237\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6209 - accuracy: 0.6747\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6460 - accuracy: 0.6343\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6713\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6549 - accuracy: 0.6142\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6143 - accuracy: 0.6846\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.685 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6497 - accuracy: 0.6316\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6195 - accuracy: 0.6766\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.677 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6537 - accuracy: 0.6211\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6586\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.659 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6517 - accuracy: 0.6271\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6129 - accuracy: 0.6846\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.685 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6469 - accuracy: 0.6323\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6232 - accuracy: 0.6742\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6474 - accuracy: 0.6356\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6269 - accuracy: 0.6682\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6518 - accuracy: 0.6312\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6191 - accuracy: 0.6811\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.681 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6546 - accuracy: 0.6251\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6188 - accuracy: 0.6797\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.680 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6474 - accuracy: 0.6396\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6605\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.660 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6536 - accuracy: 0.6236\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6385\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.638 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6492 - accuracy: 0.6303\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6703\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.670 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6480 - accuracy: 0.6319\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6550\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.655 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6522 - accuracy: 0.6236\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6829\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.683 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6489 - accuracy: 0.6330\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6237 - accuracy: 0.6718\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.672 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6468 - accuracy: 0.6385\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6636\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.664 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6538 - accuracy: 0.6252\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6295 - accuracy: 0.6539\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.654 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6482 - accuracy: 0.6335\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6749\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.675 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6465 - accuracy: 0.6383\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6701\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.670 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6453 - accuracy: 0.6382\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6848\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.685 total time=   6.4s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6491 - accuracy: 0.6274\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6768\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.677 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6449 - accuracy: 0.6387\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6681\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6458 - accuracy: 0.6347\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6183 - accuracy: 0.6821\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.682 total time=   7.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6487 - accuracy: 0.6293\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6701\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.670 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6452 - accuracy: 0.6392\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6674\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.667 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6568 - accuracy: 0.6222\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6175 - accuracy: 0.6756\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.676 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6489 - accuracy: 0.6279\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6658\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.666 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6431 - accuracy: 0.6408\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6138\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.614 total time=   6.7s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6465 - accuracy: 0.6356\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6521\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.652 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6478 - accuracy: 0.6365\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.6752\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.675 total time=   5.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6453 - accuracy: 0.6366\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6592\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.659 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6486 - accuracy: 0.6323\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6190 - accuracy: 0.6761\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.676 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6445 - accuracy: 0.6363\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6296 - accuracy: 0.6711\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.671 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6459 - accuracy: 0.6408\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6667\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.667 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6564 - accuracy: 0.6198\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6260 - accuracy: 0.6612\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.661 total time=   6.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6509 - accuracy: 0.6278\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6476\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.648 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6482 - accuracy: 0.6379\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.6686\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.669 total time=   6.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6529 - accuracy: 0.6288\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6238 - accuracy: 0.6741\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6490 - accuracy: 0.6331\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6204 - accuracy: 0.6767\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.677 total time=   7.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6451 - accuracy: 0.6360\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6231 - accuracy: 0.6723\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.672 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6485 - accuracy: 0.6360\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6311 - accuracy: 0.6672\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.667 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6491 - accuracy: 0.6308\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6279 - accuracy: 0.6817\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.682 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6454 - accuracy: 0.6440\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6289 - accuracy: 0.6663\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.666 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6555 - accuracy: 0.6240\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6158 - accuracy: 0.6800\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.680 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6501 - accuracy: 0.6329\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6261 - accuracy: 0.6668\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.667 total time=   8.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6507 - accuracy: 0.6278\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6276 - accuracy: 0.6715\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.672 total time=   6.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6490 - accuracy: 0.6326\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6253 - accuracy: 0.6729\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.673 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6466 - accuracy: 0.6354\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6243 - accuracy: 0.6763\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.676 total time=   7.9s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6432 - accuracy: 0.6435\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6371 - accuracy: 0.6502\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.650 total time=   8.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6499 - accuracy: 0.6322\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6260 - accuracy: 0.6819\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.682 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6417 - accuracy: 0.6426\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6247 - accuracy: 0.6775\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.678 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6454 - accuracy: 0.6402\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6243 - accuracy: 0.6707\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.671 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6517 - accuracy: 0.6312\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6207 - accuracy: 0.6774\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.677 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6505 - accuracy: 0.6295\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6197 - accuracy: 0.6726\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.673 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6429 - accuracy: 0.6434\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6361 - accuracy: 0.6578\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.658 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6493 - accuracy: 0.6307\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6637\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.664 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6434 - accuracy: 0.6385\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6624\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.662 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6437 - accuracy: 0.6394\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.6509\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.651 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6490 - accuracy: 0.6337\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6182 - accuracy: 0.6745\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.675 total time=   7.5s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6483 - accuracy: 0.6344\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6639\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.664 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6482 - accuracy: 0.6349\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6278 - accuracy: 0.6687\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.669 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6521 - accuracy: 0.6311\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6117 - accuracy: 0.6824\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.682 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6470 - accuracy: 0.6403\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6292 - accuracy: 0.6637\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.664 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6456 - accuracy: 0.6368\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6624\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.662 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6508 - accuracy: 0.6307\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6144 - accuracy: 0.6797\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.680 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6530 - accuracy: 0.6217\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6233 - accuracy: 0.6756\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.676 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6520 - accuracy: 0.6221\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6363 - accuracy: 0.6703\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.670 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6492 - accuracy: 0.6353\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6312 - accuracy: 0.6645\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.665 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6454 - accuracy: 0.6354\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6306 - accuracy: 0.6716\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.672 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6463 - accuracy: 0.6349\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6250 - accuracy: 0.6630\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.663 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6561 - accuracy: 0.6207\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6294 - accuracy: 0.6600\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.660 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6468 - accuracy: 0.6366\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6215 - accuracy: 0.6690\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.669 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6506 - accuracy: 0.6299\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6232 - accuracy: 0.6701\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.670 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6556 - accuracy: 0.6213\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6157 - accuracy: 0.6784\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.678 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6457 - accuracy: 0.6362\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6302 - accuracy: 0.6641\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.664 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6468 - accuracy: 0.6368\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6423 - accuracy: 0.6473\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.647 total time=   8.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6514 - accuracy: 0.6248\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6088 - accuracy: 0.6809\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.681 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6463 - accuracy: 0.6333\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6245 - accuracy: 0.6658\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.666 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6445 - accuracy: 0.6432\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6200 - accuracy: 0.6756\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.676 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6508 - accuracy: 0.6353\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6456 - accuracy: 0.6090\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.609 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6504 - accuracy: 0.6298\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6205 - accuracy: 0.6793\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.679 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6439 - accuracy: 0.6413\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6218 - accuracy: 0.6710\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   7.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6492 - accuracy: 0.6327\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6251 - accuracy: 0.6688\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.669 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6440 - accuracy: 0.6395\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6224 - accuracy: 0.6714\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   6.3s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6426 - accuracy: 0.6425\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6372\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.637 total time=   6.0s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6488 - accuracy: 0.6339\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6147 - accuracy: 0.6846\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.685 total time=   5.4s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6461 - accuracy: 0.6355\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6633\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.663 total time=   5.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6480 - accuracy: 0.6365\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6149\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.615 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6538 - accuracy: 0.6253\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6220 - accuracy: 0.6782\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.678 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6464 - accuracy: 0.6376\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6201 - accuracy: 0.6752\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.675 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6504 - accuracy: 0.6268\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6733 - accuracy: 0.5560\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.556 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6468 - accuracy: 0.6382\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6145 - accuracy: 0.6847\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.685 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6465 - accuracy: 0.6366\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6331 - accuracy: 0.6559\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.656 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6434 - accuracy: 0.6437\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6595\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.660 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6525 - accuracy: 0.6242\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6817\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.682 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6489 - accuracy: 0.6355\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6203 - accuracy: 0.6761\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.676 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6441 - accuracy: 0.6422\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6746\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.675 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6497 - accuracy: 0.6321\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6785\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.679 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6495 - accuracy: 0.6361\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6210 - accuracy: 0.6757\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.676 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6458 - accuracy: 0.6385\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6250 - accuracy: 0.6737\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.674 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6532 - accuracy: 0.6274\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6146 - accuracy: 0.6819\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.682 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6528 - accuracy: 0.6266\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6250 - accuracy: 0.6749\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.675 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6447 - accuracy: 0.6353\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6275 - accuracy: 0.6607\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.661 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6487 - accuracy: 0.6319\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6145 - accuracy: 0.6783\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.678 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6430 - accuracy: 0.6400\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.6568\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.657 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6434 - accuracy: 0.6416\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6329 - accuracy: 0.6736\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.674 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6493 - accuracy: 0.6334\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6674\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.667 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6463 - accuracy: 0.6387\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6772\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.677 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6443 - accuracy: 0.6412\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6232 - accuracy: 0.6737\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6506 - accuracy: 0.6272\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6730\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.673 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6440 - accuracy: 0.6393\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6180 - accuracy: 0.6742\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6421 - accuracy: 0.6449\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.6763\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.676 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6528 - accuracy: 0.6243\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6650\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.665 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6460 - accuracy: 0.6366\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6197 - accuracy: 0.6680\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.668 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6456 - accuracy: 0.6424\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.6728\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.673 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6543 - accuracy: 0.6201\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6168 - accuracy: 0.6811\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.681 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6459 - accuracy: 0.6394\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6245 - accuracy: 0.6671\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.667 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6469 - accuracy: 0.6351\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6737\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6475 - accuracy: 0.6345\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6248 - accuracy: 0.6762\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.676 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6495 - accuracy: 0.6265\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6738\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6473 - accuracy: 0.6383\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6331 - accuracy: 0.6638\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.664 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6519 - accuracy: 0.6322\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6162 - accuracy: 0.6789\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.679 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6477 - accuracy: 0.6335\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6224 - accuracy: 0.6767\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.677 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6460 - accuracy: 0.6416\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6225 - accuracy: 0.6688\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.669 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6520 - accuracy: 0.6312\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6179 - accuracy: 0.6818\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.682 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6436 - accuracy: 0.6403\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6757\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.676 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6448 - accuracy: 0.6400\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6206 - accuracy: 0.6722\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.672 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6482 - accuracy: 0.6316\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6154 - accuracy: 0.6811\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.681 total time=   7.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6442 - accuracy: 0.6422\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6795\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.680 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6408 - accuracy: 0.6467\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6246 - accuracy: 0.6693\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.669 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6452 - accuracy: 0.6390\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6213 - accuracy: 0.6772\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.677 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6465 - accuracy: 0.6382\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6225 - accuracy: 0.6749\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.675 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6452 - accuracy: 0.6429\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6725\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.672 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6494 - accuracy: 0.6360\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6671\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.667 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6476 - accuracy: 0.6366\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6782\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.678 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6459 - accuracy: 0.6390\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6617\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.662 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6467 - accuracy: 0.6421\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6430 - accuracy: 0.6335\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.634 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6462 - accuracy: 0.6379\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6310 - accuracy: 0.6532\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.653 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6483 - accuracy: 0.6368\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6470 - accuracy: 0.6414\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.641 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6464 - accuracy: 0.6387\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6143 - accuracy: 0.6824\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.682 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6435 - accuracy: 0.6358\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6213 - accuracy: 0.6712\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.671 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6524 - accuracy: 0.6296\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6286 - accuracy: 0.6683\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6539 - accuracy: 0.6270\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6138 - accuracy: 0.6797\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.680 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6520 - accuracy: 0.6233\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6716\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.672 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6517 - accuracy: 0.6286\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6387 - accuracy: 0.6740\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6528 - accuracy: 0.6252\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6780\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.678 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6566 - accuracy: 0.6216\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6394 - accuracy: 0.6587\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.659 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6463 - accuracy: 0.6349\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6713\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.671 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6512 - accuracy: 0.6264\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6128 - accuracy: 0.6842\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.684 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6512 - accuracy: 0.6293\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6208 - accuracy: 0.6768\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.677 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6472 - accuracy: 0.6337\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6696\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.670 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6526 - accuracy: 0.6259\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6762\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.676 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6572 - accuracy: 0.6098\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6476 - accuracy: 0.6401\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.640 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6500 - accuracy: 0.6269\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6659\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.666 total time=   7.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6547 - accuracy: 0.6193\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6118 - accuracy: 0.6827\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.683 total time=   6.8s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6473 - accuracy: 0.6338\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6223 - accuracy: 0.6741\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.674 total time=   5.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6548 - accuracy: 0.6213\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6329 - accuracy: 0.6604\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.660 total time=   6.5s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6526 - accuracy: 0.6253\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6559\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.656 total time=   6.6s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6550 - accuracy: 0.6183\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6310 - accuracy: 0.6601\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.660 total time=   6.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6468 - accuracy: 0.6373\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6482\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.648 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6496 - accuracy: 0.6308\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6638\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.664 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6484 - accuracy: 0.6343\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6185 - accuracy: 0.6759\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.676 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6499 - accuracy: 0.6282\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6713\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6524 - accuracy: 0.6266\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6196 - accuracy: 0.6781\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.678 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6546 - accuracy: 0.6203\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6206 - accuracy: 0.6776\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.678 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6458 - accuracy: 0.6336\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6751\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.675 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6497 - accuracy: 0.6318\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6795\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.679 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6449 - accuracy: 0.6357\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6232 - accuracy: 0.6763\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.676 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6530 - accuracy: 0.6251\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6234 - accuracy: 0.6736\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.674 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6531 - accuracy: 0.6219\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6837\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.684 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6512 - accuracy: 0.6306\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6230 - accuracy: 0.6801\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.680 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6480 - accuracy: 0.6334\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6208 - accuracy: 0.6755\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.675 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6527 - accuracy: 0.6256\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6796\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.680 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6495 - accuracy: 0.6335\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6205 - accuracy: 0.6749\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.675 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6465 - accuracy: 0.6353\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6690\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.669 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6497 - accuracy: 0.6296\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6128 - accuracy: 0.6848\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.685 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6542 - accuracy: 0.6196\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6202 - accuracy: 0.6790\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.679 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6454 - accuracy: 0.6376\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6660\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.666 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6548 - accuracy: 0.6163\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6116 - accuracy: 0.6849\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.685 total time=   8.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6503 - accuracy: 0.6288\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6434 - accuracy: 0.6552\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.655 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6479 - accuracy: 0.6360\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6323 - accuracy: 0.6567\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.657 total time=   6.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6526 - accuracy: 0.6297\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6205 - accuracy: 0.6834\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.683 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6488 - accuracy: 0.6281\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6185 - accuracy: 0.6760\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.676 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6462 - accuracy: 0.6345\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6215 - accuracy: 0.6750\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.675 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6534 - accuracy: 0.6261\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6096 - accuracy: 0.6886\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.689 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6480 - accuracy: 0.6329\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6631\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.663 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6480 - accuracy: 0.6287\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6230 - accuracy: 0.6721\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.672 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6534 - accuracy: 0.6262\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6808\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.681 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6507 - accuracy: 0.6284\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6230 - accuracy: 0.6736\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.674 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6458 - accuracy: 0.6400\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6606\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.661 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6513 - accuracy: 0.6263\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6098 - accuracy: 0.6842\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.684 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6483 - accuracy: 0.6354\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6353 - accuracy: 0.6626\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.663 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6464 - accuracy: 0.6363\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6596\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.660 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6493 - accuracy: 0.6339\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6112 - accuracy: 0.6821\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.682 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6469 - accuracy: 0.6303\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6172 - accuracy: 0.6779\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.678 total time=   6.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6520 - accuracy: 0.6316\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6213 - accuracy: 0.6752\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.675 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6568 - accuracy: 0.6249\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6195 - accuracy: 0.6799\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.680 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6486 - accuracy: 0.6316\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6215 - accuracy: 0.6752\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.675 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6477 - accuracy: 0.6361\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6269 - accuracy: 0.6605\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.660 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6512 - accuracy: 0.6308\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6181 - accuracy: 0.6762\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.676 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6508 - accuracy: 0.6293\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6281 - accuracy: 0.6600\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.660 total time=   6.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6440 - accuracy: 0.6397\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6232 - accuracy: 0.6726\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6570 - accuracy: 0.6131\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6790\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.679 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6482 - accuracy: 0.6313\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6210 - accuracy: 0.6765\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.677 total time=   7.1s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6482 - accuracy: 0.6342\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6652\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.665 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6510 - accuracy: 0.6323\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6113 - accuracy: 0.6848\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.685 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6435 - accuracy: 0.6451\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6269 - accuracy: 0.6736\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.674 total time=   7.0s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6458 - accuracy: 0.6364\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6365 - accuracy: 0.6503\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.650 total time=   5.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6511 - accuracy: 0.6241\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6178 - accuracy: 0.6744\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.674 total time=   7.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6484 - accuracy: 0.6308\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6659\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.666 total time=   5.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6468 - accuracy: 0.6394\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6652\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.665 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6496 - accuracy: 0.6318\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6168 - accuracy: 0.6816\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.682 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6455 - accuracy: 0.6363\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6657\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.666 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6457 - accuracy: 0.6404\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6206 - accuracy: 0.6745\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6480 - accuracy: 0.6328\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6103 - accuracy: 0.6865\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.687 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6484 - accuracy: 0.6321\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6269 - accuracy: 0.6786\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.679 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6458 - accuracy: 0.6382\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6532\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.653 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6484 - accuracy: 0.6341\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6706\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6459 - accuracy: 0.6416\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6296 - accuracy: 0.6745\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6462 - accuracy: 0.6402\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6746\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.675 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6455 - accuracy: 0.6363\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6722\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.672 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6467 - accuracy: 0.6346\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6179 - accuracy: 0.6783\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.678 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6531 - accuracy: 0.6238\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6702\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.670 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6588 - accuracy: 0.6101\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6825\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.682 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6496 - accuracy: 0.6308\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6697\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.670 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6472 - accuracy: 0.6338\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6232 - accuracy: 0.6751\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.675 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6534 - accuracy: 0.6213\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6710\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.671 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6470 - accuracy: 0.6333\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6576\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.658 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6497 - accuracy: 0.6263\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6231 - accuracy: 0.6732\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6541 - accuracy: 0.6239\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6203 - accuracy: 0.6788\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.679 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6455 - accuracy: 0.6351\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6345 - accuracy: 0.6582\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.658 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6471 - accuracy: 0.6360\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6651\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.665 total time=   7.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6537 - accuracy: 0.6226\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6224 - accuracy: 0.6727\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.673 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6508 - accuracy: 0.6294\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6230 - accuracy: 0.6789\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.679 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6468 - accuracy: 0.6349\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6429\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.643 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6518 - accuracy: 0.6313\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6320 - accuracy: 0.6419\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.642 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6501 - accuracy: 0.6301\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6227 - accuracy: 0.6701\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.670 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6467 - accuracy: 0.6356\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6404 - accuracy: 0.6592\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.659 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6537 - accuracy: 0.6234\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6179 - accuracy: 0.6809\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.681 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6484 - accuracy: 0.6329\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6719\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.672 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6478 - accuracy: 0.6334\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6619\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.662 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6519 - accuracy: 0.6289\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6104 - accuracy: 0.6876\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.688 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6517 - accuracy: 0.6260\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6337 - accuracy: 0.6684\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.668 total time=   7.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6455 - accuracy: 0.6366\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6614\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.661 total time=   5.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6485 - accuracy: 0.6359\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6791\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.679 total time=   6.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6452 - accuracy: 0.6419\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6193 - accuracy: 0.6760\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.676 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6445 - accuracy: 0.6371\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6775\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.678 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6498 - accuracy: 0.6319\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6667\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6458 - accuracy: 0.6380\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6418 - accuracy: 0.6344\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.634 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6501 - accuracy: 0.6292\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6293 - accuracy: 0.6717\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.672 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6541 - accuracy: 0.6247\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6162 - accuracy: 0.6769\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.677 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6497 - accuracy: 0.6323\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6559\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.656 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6540 - accuracy: 0.6216\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6339 - accuracy: 0.6527\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.653 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6522 - accuracy: 0.6272\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6371 - accuracy: 0.6514\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.651 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6487 - accuracy: 0.6331\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6157 - accuracy: 0.6778\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.678 total time=   6.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6469 - accuracy: 0.6325\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6694\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.669 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6484 - accuracy: 0.6341\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6156 - accuracy: 0.6826\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.683 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6485 - accuracy: 0.6319\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6223 - accuracy: 0.6735\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.674 total time=   7.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6490 - accuracy: 0.6327\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6466 - accuracy: 0.6476\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.648 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6526 - accuracy: 0.6278\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6667\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.667 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6477 - accuracy: 0.6357\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6209 - accuracy: 0.6761\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.676 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6460 - accuracy: 0.6367\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6209 - accuracy: 0.6727\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.673 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6499 - accuracy: 0.6332\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6446\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.645 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6488 - accuracy: 0.6338\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6234 - accuracy: 0.6756\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.676 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6444 - accuracy: 0.6394\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6284 - accuracy: 0.6625\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.663 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6529 - accuracy: 0.6273\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6291\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.629 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6488 - accuracy: 0.6303\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6703\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.670 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6449 - accuracy: 0.6413\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6251 - accuracy: 0.6762\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.676 total time=   5.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6606 - accuracy: 0.6039\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6140 - accuracy: 0.6819\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.682 total time=   6.8s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6499 - accuracy: 0.6353\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6807\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.681 total time=   5.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6477 - accuracy: 0.6314\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.6700\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6489 - accuracy: 0.6342\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6156 - accuracy: 0.6853\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.685 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6477 - accuracy: 0.6379\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6654\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.665 total time=   6.9s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6431 - accuracy: 0.6416\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6201 - accuracy: 0.6724\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.672 total time=   5.5s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6510 - accuracy: 0.6292\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6815\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.682 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6450 - accuracy: 0.6375\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6734\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.673 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6487 - accuracy: 0.6332\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6394 - accuracy: 0.6526\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.653 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6504 - accuracy: 0.6332\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6111 - accuracy: 0.6842\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.684 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6474 - accuracy: 0.6351\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6678\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6458 - accuracy: 0.6388\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6612\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.661 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6472 - accuracy: 0.6374\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6137 - accuracy: 0.6847\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.685 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6480 - accuracy: 0.6317\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6771\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.677 total time=   6.8s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6414 - accuracy: 0.6463\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6310 - accuracy: 0.6603\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.660 total time=   5.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6464 - accuracy: 0.6385\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6560\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.656 total time=   7.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6499 - accuracy: 0.6264\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6354 - accuracy: 0.6633\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.663 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6487 - accuracy: 0.6365\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6676\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.668 total time=   6.5s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6508 - accuracy: 0.6327\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6195 - accuracy: 0.6819\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.682 total time=   5.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6475 - accuracy: 0.6373\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6725\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   5.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6444 - accuracy: 0.6421\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6625\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.663 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6506 - accuracy: 0.6341\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6195 - accuracy: 0.6848\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.685 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6490 - accuracy: 0.6319\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6432 - accuracy: 0.6401\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.640 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6417 - accuracy: 0.6432\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6346 - accuracy: 0.6561\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.656 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6439 - accuracy: 0.6395\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6459 - accuracy: 0.6320\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.632 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6464 - accuracy: 0.6369\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6216 - accuracy: 0.6738\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.674 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6493 - accuracy: 0.6296\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6312 - accuracy: 0.6688\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6481 - accuracy: 0.6376\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6252 - accuracy: 0.6811\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.681 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6483 - accuracy: 0.6342\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6212 - accuracy: 0.6728\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.673 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6492 - accuracy: 0.6339\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6284 - accuracy: 0.6740\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6521 - accuracy: 0.6271\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6134 - accuracy: 0.6827\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.683 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6460 - accuracy: 0.6351\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6764\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.676 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6442 - accuracy: 0.6415\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6268 - accuracy: 0.6662\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.666 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6463 - accuracy: 0.6388\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6208 - accuracy: 0.6743\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6453 - accuracy: 0.6376\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6595\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.659 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6504 - accuracy: 0.6317\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6365 - accuracy: 0.6513\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.651 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6531 - accuracy: 0.6291\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6846\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.685 total time=   6.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6451 - accuracy: 0.6408\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6183 - accuracy: 0.6779\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.678 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6451 - accuracy: 0.6396\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6714\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.671 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6525 - accuracy: 0.6264\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6101 - accuracy: 0.6851\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.685 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6442 - accuracy: 0.6373\n",
      "354/354 [==============================] - 2s 3ms/step - loss: 0.6206 - accuracy: 0.6774\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.677 total time=   7.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6444 - accuracy: 0.6392\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6241 - accuracy: 0.6710\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.671 total time=   6.3s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6486 - accuracy: 0.6325\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6226 - accuracy: 0.6772\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.677 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6448 - accuracy: 0.6361\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6789\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.679 total time=   6.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6505 - accuracy: 0.6313\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6349 - accuracy: 0.6704\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.670 total time=   7.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6538 - accuracy: 0.6250\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6114 - accuracy: 0.6855\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.685 total time=   5.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6489 - accuracy: 0.6316\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6228 - accuracy: 0.6722\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   7.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6460 - accuracy: 0.6375\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6229 - accuracy: 0.6736\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.674 total time=   6.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6478 - accuracy: 0.6384\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6291 - accuracy: 0.6608\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.661 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6488 - accuracy: 0.6321\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6723\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.672 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6470 - accuracy: 0.6335\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6673\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.667 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6486 - accuracy: 0.6319\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6132 - accuracy: 0.6810\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.681 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6424 - accuracy: 0.6455\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6434 - accuracy: 0.6430\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.643 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6457 - accuracy: 0.6458\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6629\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.663 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6515 - accuracy: 0.6294\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6192 - accuracy: 0.6683\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.668 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6501 - accuracy: 0.6293\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6204 - accuracy: 0.6746\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   7.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6432 - accuracy: 0.6414\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6338 - accuracy: 0.6492\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.649 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6454 - accuracy: 0.6381\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6619\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.662 total time=   6.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6458 - accuracy: 0.6362\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6215 - accuracy: 0.6785\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.678 total time=   5.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6437 - accuracy: 0.6393\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6730\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.673 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6462 - accuracy: 0.6356\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6158 - accuracy: 0.6738\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6471 - accuracy: 0.6308\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6739\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6462 - accuracy: 0.6409\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6641\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.664 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6514 - accuracy: 0.6325\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6692\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.669 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6452 - accuracy: 0.6400\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6168 - accuracy: 0.6762\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.676 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6464 - accuracy: 0.6375\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6747\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.675 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6509 - accuracy: 0.6312\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6163 - accuracy: 0.6840\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.684 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6470 - accuracy: 0.6431\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6204 - accuracy: 0.6801\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.680 total time=   7.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6458 - accuracy: 0.6372\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6772\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.677 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6507 - accuracy: 0.6336\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6241 - accuracy: 0.6730\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   6.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6485 - accuracy: 0.6358\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6345 - accuracy: 0.6665\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.666 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6447 - accuracy: 0.6419\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6250 - accuracy: 0.6729\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.673 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6494 - accuracy: 0.6341\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6387 - accuracy: 0.6463\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.646 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6458 - accuracy: 0.6381\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6156 - accuracy: 0.6789\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.679 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6410 - accuracy: 0.6445\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6679\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.668 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6482 - accuracy: 0.6388\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6849\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.685 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6437 - accuracy: 0.6416\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6718\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.672 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6440 - accuracy: 0.6429\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.6746\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.675 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6490 - accuracy: 0.6342\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6121 - accuracy: 0.6873\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.687 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6448 - accuracy: 0.6373\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6215 - accuracy: 0.6761\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.676 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6477 - accuracy: 0.6368\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6390 - accuracy: 0.6725\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.672 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6527 - accuracy: 0.6290\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6185 - accuracy: 0.6839\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.684 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6480 - accuracy: 0.6370\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6220 - accuracy: 0.6749\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6477 - accuracy: 0.6338\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6391 - accuracy: 0.6740\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6455 - accuracy: 0.6417\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6384\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.638 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6437 - accuracy: 0.6396\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6196 - accuracy: 0.6730\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.673 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6465 - accuracy: 0.6322\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6393\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.639 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6467 - accuracy: 0.6340\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6143 - accuracy: 0.6842\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.684 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6451 - accuracy: 0.6358\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6333 - accuracy: 0.6523\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.652 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6492 - accuracy: 0.6309\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6287 - accuracy: 0.6711\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.671 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6505 - accuracy: 0.6347\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6317 - accuracy: 0.6720\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.672 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6453 - accuracy: 0.6404\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6232 - accuracy: 0.6707\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.671 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6430 - accuracy: 0.6432\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6675\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.667 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6494 - accuracy: 0.6317\n",
      "354/354 [==============================] - 2s 3ms/step - loss: 0.6203 - accuracy: 0.6736\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   7.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6491 - accuracy: 0.6325\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6183 - accuracy: 0.6772\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.677 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6449 - accuracy: 0.6413\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6418 - accuracy: 0.6471\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.647 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6471 - accuracy: 0.6377\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6119 - accuracy: 0.6838\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.684 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6449 - accuracy: 0.6387\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6165 - accuracy: 0.6783\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.678 total time=   7.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6463 - accuracy: 0.6400\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6560 - accuracy: 0.6219\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.622 total time=   6.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6493 - accuracy: 0.6319\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6769\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.677 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6483 - accuracy: 0.6332\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6397 - accuracy: 0.6441\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.644 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6447 - accuracy: 0.6427\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6204 - accuracy: 0.6751\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.675 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6516 - accuracy: 0.6299\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6161 - accuracy: 0.6814\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.681 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6426 - accuracy: 0.6451\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6213 - accuracy: 0.6712\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.671 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6435 - accuracy: 0.6421\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6702\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.670 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6506 - accuracy: 0.6306\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6175 - accuracy: 0.6762\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.676 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6422 - accuracy: 0.6426\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6198 - accuracy: 0.6726\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.673 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6466 - accuracy: 0.6399\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6619\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.662 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6509 - accuracy: 0.6269\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6151 - accuracy: 0.6795\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.679 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6448 - accuracy: 0.6420\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6183 - accuracy: 0.6787\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.679 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6427 - accuracy: 0.6412\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6205 - accuracy: 0.6749\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.675 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6506 - accuracy: 0.6300\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6148 - accuracy: 0.6838\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.684 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6420 - accuracy: 0.6442\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6448\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.645 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6439 - accuracy: 0.6415\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6662\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.666 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6478 - accuracy: 0.6344\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6348 - accuracy: 0.6568\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.657 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6459 - accuracy: 0.6389\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6600 - accuracy: 0.6483\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.648 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6956 - accuracy: 0.5050\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6910 - accuracy: 0.5688\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.569 total time=   6.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6958 - accuracy: 0.4980\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5522\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.552 total time=   7.4s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6973 - accuracy: 0.4996\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5278\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.528 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6949 - accuracy: 0.5058\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6899 - accuracy: 0.5863\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.586 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6969 - accuracy: 0.5035\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6926 - accuracy: 0.5414\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.541 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6954 - accuracy: 0.5035\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6902 - accuracy: 0.6115\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.612 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6944 - accuracy: 0.5094\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6897 - accuracy: 0.6149\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.615 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6927 - accuracy: 0.5213\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6840 - accuracy: 0.6432\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.643 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6922 - accuracy: 0.5224\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6783 - accuracy: 0.6505\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.650 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6935 - accuracy: 0.5169\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6865 - accuracy: 0.6154\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.615 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6926 - accuracy: 0.5207\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6777 - accuracy: 0.6615\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.662 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6950 - accuracy: 0.5098\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6898 - accuracy: 0.6145\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.615 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6941 - accuracy: 0.5143\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6884 - accuracy: 0.6033\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.603 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6939 - accuracy: 0.5182\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6883 - accuracy: 0.5579\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.558 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6922 - accuracy: 0.5140\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6783 - accuracy: 0.6350\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.635 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6924 - accuracy: 0.5199\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6836 - accuracy: 0.6261\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.626 total time=   7.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6955 - accuracy: 0.5142\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6886 - accuracy: 0.5683\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.568 total time=   5.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6971 - accuracy: 0.5052\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6893 - accuracy: 0.5946\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.595 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6947 - accuracy: 0.5080\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6885 - accuracy: 0.6006\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.601 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6938 - accuracy: 0.5164\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6852 - accuracy: 0.6382\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.638 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6899 - accuracy: 0.5339\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6696 - accuracy: 0.6589\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.659 total time=   7.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6940 - accuracy: 0.5126\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6895 - accuracy: 0.5864\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.586 total time=   6.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6957 - accuracy: 0.5089\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6877 - accuracy: 0.6427\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.643 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6952 - accuracy: 0.5126\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6894 - accuracy: 0.5896\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.590 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6900 - accuracy: 0.5327\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6682 - accuracy: 0.6515\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.651 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6940 - accuracy: 0.5125\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6826 - accuracy: 0.6632\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.663 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6953 - accuracy: 0.5079\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6903 - accuracy: 0.5719\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.572 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6933 - accuracy: 0.5154\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6875 - accuracy: 0.6358\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.636 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6947 - accuracy: 0.5084\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6900 - accuracy: 0.6498\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.650 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6970 - accuracy: 0.5054\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5559\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.556 total time=   6.8s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6936 - accuracy: 0.5166\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6855 - accuracy: 0.5867\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.587 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6957 - accuracy: 0.5091\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5040\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.504 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6946 - accuracy: 0.5108\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6892 - accuracy: 0.5641\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.564 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6938 - accuracy: 0.5092\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.5027\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.503 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6939 - accuracy: 0.5073\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6904 - accuracy: 0.5014\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.501 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6945 - accuracy: 0.5063\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5796\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.580 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6932 - accuracy: 0.5167\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6814 - accuracy: 0.6208\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.621 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6951 - accuracy: 0.5074\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6891 - accuracy: 0.6398\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.640 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6930 - accuracy: 0.5194\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6816 - accuracy: 0.6352\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.635 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6954 - accuracy: 0.5082\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5491\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.549 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6950 - accuracy: 0.5118\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5401\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.540 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6955 - accuracy: 0.5135\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6874 - accuracy: 0.5836\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.584 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6930 - accuracy: 0.5242\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6821 - accuracy: 0.6432\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.643 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6946 - accuracy: 0.5170\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6846 - accuracy: 0.6447\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.645 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6920 - accuracy: 0.5208\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6788 - accuracy: 0.6060\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.606 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6927 - accuracy: 0.5181\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6773 - accuracy: 0.6382\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.638 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6931 - accuracy: 0.5197\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6825 - accuracy: 0.6403\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.640 total time=   6.5s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6938 - accuracy: 0.5138\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6829 - accuracy: 0.6296\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.630 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6938 - accuracy: 0.5187\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6811 - accuracy: 0.5704\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.570 total time=   7.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6924 - accuracy: 0.5241\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6785 - accuracy: 0.6061\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.606 total time=   6.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6971 - accuracy: 0.5091\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6906 - accuracy: 0.6255\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.625 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6917 - accuracy: 0.5281\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6752 - accuracy: 0.6342\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.634 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6942 - accuracy: 0.5107\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6894 - accuracy: 0.6161\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.616 total time=   6.3s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6943 - accuracy: 0.5140\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6884 - accuracy: 0.6427\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.643 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6918 - accuracy: 0.5251\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6844 - accuracy: 0.6070\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.607 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6936 - accuracy: 0.5190\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6851 - accuracy: 0.6198\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.620 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6911 - accuracy: 0.5339\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6768 - accuracy: 0.6026\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.603 total time=   7.0s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6941 - accuracy: 0.5106\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6867 - accuracy: 0.5753\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.575 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6910 - accuracy: 0.5274\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6715 - accuracy: 0.6674\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.667 total time=   7.0s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6918 - accuracy: 0.5198\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6794 - accuracy: 0.6352\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.635 total time=   5.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6906 - accuracy: 0.5342\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6687 - accuracy: 0.6574\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.657 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6907 - accuracy: 0.5344\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.6518\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.652 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6915 - accuracy: 0.5248\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6785 - accuracy: 0.5874\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.587 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6939 - accuracy: 0.5055\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.6098\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.610 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6900 - accuracy: 0.5330\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6657 - accuracy: 0.6397\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.640 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6935 - accuracy: 0.5115\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6859 - accuracy: 0.6364\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.636 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6949 - accuracy: 0.5100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6879 - accuracy: 0.5794\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.579 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6860 - accuracy: 0.5484\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6588\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.659 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6825 - accuracy: 0.5551\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6451\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.645 total time=   6.9s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6891 - accuracy: 0.5343\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6627 - accuracy: 0.6404\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.640 total time=   5.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6923 - accuracy: 0.5224\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6797 - accuracy: 0.5889\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.589 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6909 - accuracy: 0.5312\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6741 - accuracy: 0.6617\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.662 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6915 - accuracy: 0.5226\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6710 - accuracy: 0.6348\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.635 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6945 - accuracy: 0.5028\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5030\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.503 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6925 - accuracy: 0.5151\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6819 - accuracy: 0.6030\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.603 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6908 - accuracy: 0.5279\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6714 - accuracy: 0.6380\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.638 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6913 - accuracy: 0.5301\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6736 - accuracy: 0.5830\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.583 total time=   6.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6784 - accuracy: 0.5677\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6448\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.645 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6909 - accuracy: 0.5300\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6704 - accuracy: 0.6067\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.607 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6833 - accuracy: 0.5520\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6451 - accuracy: 0.6559\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.656 total time=   6.5s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6907 - accuracy: 0.5310\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6707 - accuracy: 0.6265\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.627 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6967 - accuracy: 0.5107\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6865 - accuracy: 0.5712\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.571 total time=   6.2s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6961 - accuracy: 0.4989\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5465\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.546 total time=   6.1s\n",
      "708/708 [==============================] - 7s 7ms/step - loss: 0.6950 - accuracy: 0.5064\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6900 - accuracy: 0.5581\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.558 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6937 - accuracy: 0.5162\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6888 - accuracy: 0.5788\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.579 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6962 - accuracy: 0.5054\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.5937\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.594 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6967 - accuracy: 0.5117\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6896 - accuracy: 0.6193\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.619 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6928 - accuracy: 0.5154\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6835 - accuracy: 0.6411\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.641 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6942 - accuracy: 0.5121\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6890 - accuracy: 0.6134\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.613 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6957 - accuracy: 0.5057\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5410\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.541 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6970 - accuracy: 0.5022\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5753\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.575 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6954 - accuracy: 0.5068\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5968\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.597 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6969 - accuracy: 0.5040\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6887 - accuracy: 0.6009\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.601 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6914 - accuracy: 0.5270\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6766 - accuracy: 0.6372\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.637 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6978 - accuracy: 0.5074\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5891\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.589 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6938 - accuracy: 0.5151\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6843 - accuracy: 0.6476\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.648 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6946 - accuracy: 0.5143\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6840 - accuracy: 0.6396\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.640 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6935 - accuracy: 0.5145\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6835 - accuracy: 0.6410\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.641 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6925 - accuracy: 0.5177\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6852 - accuracy: 0.5859\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.586 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6930 - accuracy: 0.5170\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6752 - accuracy: 0.6490\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.649 total time=   6.3s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6948 - accuracy: 0.5158\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6866 - accuracy: 0.6558\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.656 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6943 - accuracy: 0.5167\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6864 - accuracy: 0.6209\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.621 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6938 - accuracy: 0.5187\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6822 - accuracy: 0.6241\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.624 total time=   6.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6969 - accuracy: 0.5052\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5927\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.593 total time=   6.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6963 - accuracy: 0.5008\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5012\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.501 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6950 - accuracy: 0.5038\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6894 - accuracy: 0.6122\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.612 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6945 - accuracy: 0.5149\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6839 - accuracy: 0.5927\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.593 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6910 - accuracy: 0.5212\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6796 - accuracy: 0.5796\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.580 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6929 - accuracy: 0.5232\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6820 - accuracy: 0.6488\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.649 total time=   6.2s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6951 - accuracy: 0.5071\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5840\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.584 total time=   5.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6962 - accuracy: 0.5065\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6902 - accuracy: 0.6195\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.619 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6936 - accuracy: 0.5116\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6853 - accuracy: 0.5838\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.584 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6788 - accuracy: 0.5675\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6726\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.673 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6930 - accuracy: 0.5191\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6817 - accuracy: 0.6339\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.634 total time=   7.0s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6937 - accuracy: 0.5103\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6887 - accuracy: 0.5635\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.564 total time=   5.4s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6880 - accuracy: 0.5416\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6629 - accuracy: 0.6488\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.649 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6957 - accuracy: 0.5085\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6897 - accuracy: 0.5708\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.571 total time=   6.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6892 - accuracy: 0.5361\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6652 - accuracy: 0.6587\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.659 total time=   7.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6944 - accuracy: 0.5151\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6842 - accuracy: 0.6347\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.635 total time=   5.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6944 - accuracy: 0.5086\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6874 - accuracy: 0.6361\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.636 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6884 - accuracy: 0.5363\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6617 - accuracy: 0.6418\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.642 total time=   7.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6918 - accuracy: 0.5274\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6752 - accuracy: 0.6063\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.606 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6947 - accuracy: 0.5191\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6875 - accuracy: 0.5568\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.557 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6909 - accuracy: 0.5335\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6727 - accuracy: 0.5985\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.598 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6939 - accuracy: 0.5138\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6877 - accuracy: 0.6470\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.647 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6939 - accuracy: 0.5097\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6899 - accuracy: 0.5667\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.567 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6905 - accuracy: 0.5350\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6739 - accuracy: 0.6176\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.618 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6938 - accuracy: 0.5180\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6842 - accuracy: 0.5514\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.551 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6931 - accuracy: 0.5133\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6856 - accuracy: 0.5673\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.567 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6917 - accuracy: 0.5235\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6782 - accuracy: 0.5930\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.593 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6938 - accuracy: 0.5161\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6833 - accuracy: 0.6232\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.623 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6939 - accuracy: 0.5158\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6845 - accuracy: 0.6447\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.645 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6937 - accuracy: 0.5146\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6805 - accuracy: 0.6320\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.632 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6894 - accuracy: 0.5380\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6598 - accuracy: 0.6607\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.661 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6914 - accuracy: 0.5239\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.6552\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.655 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6853 - accuracy: 0.5546\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6534 - accuracy: 0.6560\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.656 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6940 - accuracy: 0.5114\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6866 - accuracy: 0.6131\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.613 total time=   8.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6942 - accuracy: 0.5124\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6903 - accuracy: 0.6182\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.618 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6937 - accuracy: 0.5128\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6867 - accuracy: 0.6338\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.634 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6951 - accuracy: 0.5092\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6899 - accuracy: 0.6174\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.617 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6939 - accuracy: 0.5062\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6903 - accuracy: 0.6130\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.613 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6927 - accuracy: 0.5168\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6847 - accuracy: 0.6156\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.616 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6834 - accuracy: 0.5575\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6399 - accuracy: 0.6681\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.668 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6935 - accuracy: 0.5088\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6892 - accuracy: 0.5938\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.594 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6881 - accuracy: 0.5377\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6617 - accuracy: 0.6531\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.653 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6916 - accuracy: 0.5193\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6799 - accuracy: 0.6298\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.630 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6948 - accuracy: 0.4990\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5566\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.557 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6791 - accuracy: 0.5666\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6488\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.649 total time=   7.4s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6940 - accuracy: 0.5152\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6842 - accuracy: 0.5704\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.570 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6930 - accuracy: 0.5227\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6824 - accuracy: 0.6424\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.642 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6898 - accuracy: 0.5319\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6652 - accuracy: 0.6318\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.632 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6912 - accuracy: 0.5316\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6718 - accuracy: 0.6431\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.643 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6889 - accuracy: 0.5361\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6603 - accuracy: 0.6224\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.622 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6934 - accuracy: 0.5150\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6859 - accuracy: 0.5067\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.507 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6947 - accuracy: 0.5154\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6893 - accuracy: 0.5052\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.505 total time=   7.3s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6897 - accuracy: 0.5295\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6685 - accuracy: 0.6460\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.646 total time=   7.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6957 - accuracy: 0.5121\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6874 - accuracy: 0.6544\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.654 total time=   6.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6839 - accuracy: 0.5480\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6218\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.622 total time=   6.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6924 - accuracy: 0.5268\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6744 - accuracy: 0.6069\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.607 total time=   6.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6915 - accuracy: 0.5210\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6751 - accuracy: 0.6188\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.619 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6798 - accuracy: 0.5624\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6428 - accuracy: 0.6539\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.654 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6850 - accuracy: 0.5514\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6460 - accuracy: 0.6584\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.658 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6941 - accuracy: 0.5111\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6887 - accuracy: 0.6191\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.619 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6967 - accuracy: 0.5132\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6883 - accuracy: 0.5911\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.591 total time=   8.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6946 - accuracy: 0.5182\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6871 - accuracy: 0.6311\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.631 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6907 - accuracy: 0.5311\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6750 - accuracy: 0.6359\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.636 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6967 - accuracy: 0.5033\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6899 - accuracy: 0.5013\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.501 total time=   6.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6926 - accuracy: 0.5207\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6810 - accuracy: 0.6350\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.635 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6900 - accuracy: 0.5309\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6716 - accuracy: 0.6234\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.623 total time=   7.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6896 - accuracy: 0.5239\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6769 - accuracy: 0.5830\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.583 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6936 - accuracy: 0.5151\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6824 - accuracy: 0.5961\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.596 total time=   7.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6938 - accuracy: 0.5195\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6822 - accuracy: 0.6031\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.603 total time=   6.6s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6937 - accuracy: 0.5135\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6867 - accuracy: 0.6081\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.608 total time=   5.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6935 - accuracy: 0.5175\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6862 - accuracy: 0.5623\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.562 total time=   7.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6874 - accuracy: 0.5457\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6391\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.639 total time=   6.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6918 - accuracy: 0.5200\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6778 - accuracy: 0.6528\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.653 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6919 - accuracy: 0.5195\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6782 - accuracy: 0.6233\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.623 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6931 - accuracy: 0.5231\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6806 - accuracy: 0.6100\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.610 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6903 - accuracy: 0.5356\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6614 - accuracy: 0.6668\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.667 total time=   7.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6896 - accuracy: 0.5344\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6632 - accuracy: 0.6684\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6902 - accuracy: 0.5315\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6745 - accuracy: 0.6027\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.603 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6922 - accuracy: 0.5189\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6809 - accuracy: 0.6437\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.644 total time=   6.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6930 - accuracy: 0.5188\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6856 - accuracy: 0.5588\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.559 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6878 - accuracy: 0.5433\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6614 - accuracy: 0.6500\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.650 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6928 - accuracy: 0.5142\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6818 - accuracy: 0.6119\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.612 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6933 - accuracy: 0.5173\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6809 - accuracy: 0.6495\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.650 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6955 - accuracy: 0.5051\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6905 - accuracy: 0.5996\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.600 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6912 - accuracy: 0.5284\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6651 - accuracy: 0.6670\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   7.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6940 - accuracy: 0.5120\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6867 - accuracy: 0.5707\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.571 total time=   6.5s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6937 - accuracy: 0.5229\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6809 - accuracy: 0.6252\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.625 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6951 - accuracy: 0.5085\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6897 - accuracy: 0.5790\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.579 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6950 - accuracy: 0.5037\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6915 - accuracy: 0.5631\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.563 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6939 - accuracy: 0.5172\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6846 - accuracy: 0.5578\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.558 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6901 - accuracy: 0.5349\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6670 - accuracy: 0.6432\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.643 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6926 - accuracy: 0.5160\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6843 - accuracy: 0.5490\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.549 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6941 - accuracy: 0.5173\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6843 - accuracy: 0.6447\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.645 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6897 - accuracy: 0.5329\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6642 - accuracy: 0.6562\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.656 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6808 - accuracy: 0.5625\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6405 - accuracy: 0.6597\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.660 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6867 - accuracy: 0.5490\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6485 - accuracy: 0.6555\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.656 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6932 - accuracy: 0.5184\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6794 - accuracy: 0.6117\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.612 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6940 - accuracy: 0.5153\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6895 - accuracy: 0.6417\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.642 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6918 - accuracy: 0.5307\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6697 - accuracy: 0.6441\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.644 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6920 - accuracy: 0.5283\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6724 - accuracy: 0.6152\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.615 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6945 - accuracy: 0.5073\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6894 - accuracy: 0.5556\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.556 total time=   7.3s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6910 - accuracy: 0.5332\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6680 - accuracy: 0.6521\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.652 total time=   5.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6909 - accuracy: 0.5271\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6697 - accuracy: 0.6558\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.656 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6917 - accuracy: 0.5295\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6728 - accuracy: 0.6572\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.657 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6873 - accuracy: 0.5386\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6542 - accuracy: 0.6394\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.639 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6883 - accuracy: 0.5378\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6552 - accuracy: 0.6680\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.668 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6928 - accuracy: 0.5174\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6834 - accuracy: 0.6224\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.622 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6898 - accuracy: 0.5330\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6296\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.630 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6925 - accuracy: 0.5202\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6772 - accuracy: 0.6335\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.634 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6937 - accuracy: 0.5193\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6791 - accuracy: 0.6348\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.635 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6857 - accuracy: 0.5479\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6515 - accuracy: 0.6378\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.638 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6853 - accuracy: 0.5509\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6446 - accuracy: 0.6672\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6930 - accuracy: 0.5259\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6767 - accuracy: 0.6359\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.636 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6917 - accuracy: 0.5235\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6779 - accuracy: 0.6476\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.648 total time=   7.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6915 - accuracy: 0.5256\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6751 - accuracy: 0.6263\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.626 total time=   6.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6869 - accuracy: 0.5411\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6499 - accuracy: 0.6638\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.664 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6884 - accuracy: 0.5425\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6571 - accuracy: 0.6521\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.652 total time=   7.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6906 - accuracy: 0.5272\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6677 - accuracy: 0.6471\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.647 total time=   6.5s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6913 - accuracy: 0.5279\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.6358\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.636 total time=   6.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6891 - accuracy: 0.5356\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6662 - accuracy: 0.6438\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.644 total time=   7.0s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6895 - accuracy: 0.5364\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6627\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.663 total time=   5.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6904 - accuracy: 0.5315\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6684 - accuracy: 0.6108\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.611 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6861 - accuracy: 0.5444\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6737 - accuracy: 0.5636\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.564 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6942 - accuracy: 0.5080\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6876 - accuracy: 0.6511\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.651 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6909 - accuracy: 0.5327\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6714 - accuracy: 0.6207\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.621 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6878 - accuracy: 0.5428\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6716 - accuracy: 0.5906\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.591 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6881 - accuracy: 0.5378\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6507\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.651 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6913 - accuracy: 0.5284\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6758 - accuracy: 0.5693\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.569 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6918 - accuracy: 0.5318\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6678 - accuracy: 0.6433\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.643 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6915 - accuracy: 0.5274\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6720 - accuracy: 0.6124\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.612 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6926 - accuracy: 0.5212\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6758 - accuracy: 0.6602\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.660 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6768 - accuracy: 0.5705\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6369 - accuracy: 0.6613\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.661 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6866 - accuracy: 0.5388\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6440\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.644 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6894 - accuracy: 0.5302\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6603 - accuracy: 0.6484\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.648 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6831 - accuracy: 0.5573\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6394 - accuracy: 0.6564\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.656 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6913 - accuracy: 0.5275\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6743 - accuracy: 0.6525\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.653 total time=   6.9s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6907 - accuracy: 0.5301\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6686 - accuracy: 0.6319\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.632 total time=   5.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6914 - accuracy: 0.5267\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6671 - accuracy: 0.6337\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.634 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6915 - accuracy: 0.5301\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6741 - accuracy: 0.5955\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.595 total time=   6.9s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6933 - accuracy: 0.5176\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6748 - accuracy: 0.6382\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.638 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6974 - accuracy: 0.5095\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6882 - accuracy: 0.6364\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.636 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6953 - accuracy: 0.5142\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.5135\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.514 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6966 - accuracy: 0.5072\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6871 - accuracy: 0.6398\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.640 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6928 - accuracy: 0.5238\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6825 - accuracy: 0.6296\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.630 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6962 - accuracy: 0.5121\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6860 - accuracy: 0.6634\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.663 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6969 - accuracy: 0.5049\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5424\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.542 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6956 - accuracy: 0.5245\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6809 - accuracy: 0.5795\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.579 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6944 - accuracy: 0.5095\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6867 - accuracy: 0.6249\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.625 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6970 - accuracy: 0.5075\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6854 - accuracy: 0.6416\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.642 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6980 - accuracy: 0.4985\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5503\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.550 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6949 - accuracy: 0.5112\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6905 - accuracy: 0.6539\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.654 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6960 - accuracy: 0.5107\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6879 - accuracy: 0.6336\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.634 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6922 - accuracy: 0.5230\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6801 - accuracy: 0.6361\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.636 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6974 - accuracy: 0.5088\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6891 - accuracy: 0.5161\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.516 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6957 - accuracy: 0.5122\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6877 - accuracy: 0.5844\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.584 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6978 - accuracy: 0.5075\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6893 - accuracy: 0.6110\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.611 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6936 - accuracy: 0.5167\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6842 - accuracy: 0.6672\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.667 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6943 - accuracy: 0.5123\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6879 - accuracy: 0.5971\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.597 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6954 - accuracy: 0.5109\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6879 - accuracy: 0.6042\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.604 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6933 - accuracy: 0.5156\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6827 - accuracy: 0.6488\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.649 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6936 - accuracy: 0.5201\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6816 - accuracy: 0.6490\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.649 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6954 - accuracy: 0.5102\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6896 - accuracy: 0.5238\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.524 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6951 - accuracy: 0.5037\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6902 - accuracy: 0.6150\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.615 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6937 - accuracy: 0.5211\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6785 - accuracy: 0.6523\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.652 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6933 - accuracy: 0.5148\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6863 - accuracy: 0.6182\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.618 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6956 - accuracy: 0.5081\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6896 - accuracy: 0.6516\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.652 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6932 - accuracy: 0.5166\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6836 - accuracy: 0.6508\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.651 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6961 - accuracy: 0.5101\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6902 - accuracy: 0.5523\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.552 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6935 - accuracy: 0.5251\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6806 - accuracy: 0.6128\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.613 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6956 - accuracy: 0.5094\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6882 - accuracy: 0.6306\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.631 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6952 - accuracy: 0.5040\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.6209\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.621 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6948 - accuracy: 0.5060\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6902 - accuracy: 0.6515\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.651 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6928 - accuracy: 0.5209\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6819 - accuracy: 0.6551\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.655 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6945 - accuracy: 0.5077\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6898 - accuracy: 0.6239\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.624 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6929 - accuracy: 0.5163\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6822 - accuracy: 0.6540\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.654 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6954 - accuracy: 0.5053\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.6166\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.617 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6955 - accuracy: 0.5082\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5959\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.596 total time=   6.4s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6937 - accuracy: 0.5202\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6848 - accuracy: 0.6523\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.652 total time=   6.1s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6940 - accuracy: 0.5160\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6871 - accuracy: 0.6244\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.624 total time=   5.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6956 - accuracy: 0.5062\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6888 - accuracy: 0.6171\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.617 total time=   7.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6947 - accuracy: 0.5047\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6878 - accuracy: 0.5730\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.573 total time=   5.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6941 - accuracy: 0.5142\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6821 - accuracy: 0.6393\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.639 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6943 - accuracy: 0.5165\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6841 - accuracy: 0.6037\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.604 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6950 - accuracy: 0.5053\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6890 - accuracy: 0.5211\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.521 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6919 - accuracy: 0.5223\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6762 - accuracy: 0.6530\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.653 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6913 - accuracy: 0.5334\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6779 - accuracy: 0.6520\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.652 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6940 - accuracy: 0.5141\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5673\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.567 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6947 - accuracy: 0.5089\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6904 - accuracy: 0.6366\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.637 total time=   6.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6935 - accuracy: 0.5271\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6787 - accuracy: 0.6229\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.623 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6941 - accuracy: 0.5184\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6778 - accuracy: 0.6592\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.659 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6955 - accuracy: 0.5104\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6865 - accuracy: 0.6375\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.637 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6894 - accuracy: 0.5369\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6451\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.645 total time=   7.3s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6925 - accuracy: 0.5271\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6751 - accuracy: 0.6248\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.625 total time=   8.3s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6913 - accuracy: 0.5286\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6691 - accuracy: 0.6561\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.656 total time=   5.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6915 - accuracy: 0.5281\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6727 - accuracy: 0.6573\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.657 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6923 - accuracy: 0.5301\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6791 - accuracy: 0.6048\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.605 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6904 - accuracy: 0.5302\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6761 - accuracy: 0.6519\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.652 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6926 - accuracy: 0.5165\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6811 - accuracy: 0.6573\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.657 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6939 - accuracy: 0.5105\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6829 - accuracy: 0.6515\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.651 total time=   6.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6899 - accuracy: 0.5326\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.6513\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.651 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6926 - accuracy: 0.5193\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6813 - accuracy: 0.6502\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.650 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6920 - accuracy: 0.5230\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6786 - accuracy: 0.6484\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.648 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6890 - accuracy: 0.5364\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6588 - accuracy: 0.6633\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.663 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6913 - accuracy: 0.5275\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6716 - accuracy: 0.6416\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.642 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6946 - accuracy: 0.5058\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5017\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.502 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6918 - accuracy: 0.5205\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6732 - accuracy: 0.6473\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.647 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6909 - accuracy: 0.5252\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6655 - accuracy: 0.6438\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.644 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6961 - accuracy: 0.5049\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5014\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.501 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6952 - accuracy: 0.5060\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6887 - accuracy: 0.6396\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.640 total time=   6.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6924 - accuracy: 0.5241\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6781 - accuracy: 0.6152\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.615 total time=   7.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6906 - accuracy: 0.5277\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6712 - accuracy: 0.6394\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.639 total time=   5.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6930 - accuracy: 0.5264\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6756 - accuracy: 0.6555\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.656 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6946 - accuracy: 0.5111\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6878 - accuracy: 0.6443\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.644 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6872 - accuracy: 0.5411\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6592\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.659 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6951 - accuracy: 0.5135\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6888 - accuracy: 0.6512\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.651 total time=   6.3s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6885 - accuracy: 0.5418\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6545 - accuracy: 0.6532\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.653 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6924 - accuracy: 0.5211\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6775 - accuracy: 0.6587\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.659 total time=   7.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6937 - accuracy: 0.5190\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6849 - accuracy: 0.6399\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.640 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6867 - accuracy: 0.5462\n",
      "354/354 [==============================] - 3s 4ms/step - loss: 0.6507 - accuracy: 0.6438\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.644 total time=   8.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6922 - accuracy: 0.5257\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6760 - accuracy: 0.6037\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.604 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6917 - accuracy: 0.5285\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.6584\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.658 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6983 - accuracy: 0.5126\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6912 - accuracy: 0.5112\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.511 total time=   7.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6960 - accuracy: 0.5025\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6894 - accuracy: 0.6310\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.631 total time=   6.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6953 - accuracy: 0.5155\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6871 - accuracy: 0.6468\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.647 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6939 - accuracy: 0.5118\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6855 - accuracy: 0.6347\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.635 total time=   7.5s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6950 - accuracy: 0.5124\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6867 - accuracy: 0.6216\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.622 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6957 - accuracy: 0.5171\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6870 - accuracy: 0.6011\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.601 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6956 - accuracy: 0.5081\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6907 - accuracy: 0.6325\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.632 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6951 - accuracy: 0.5146\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6892 - accuracy: 0.5110\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.511 total time=   7.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6944 - accuracy: 0.5112\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6903 - accuracy: 0.5059\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.506 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6941 - accuracy: 0.5151\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6859 - accuracy: 0.6482\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.648 total time=   6.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6949 - accuracy: 0.5131\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.5958\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.596 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6950 - accuracy: 0.5159\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6860 - accuracy: 0.6204\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.620 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6960 - accuracy: 0.5120\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6878 - accuracy: 0.6561\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.656 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6957 - accuracy: 0.5132\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6879 - accuracy: 0.6560\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.656 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6940 - accuracy: 0.5130\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6872 - accuracy: 0.6034\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.603 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6939 - accuracy: 0.5157\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6806 - accuracy: 0.6025\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.602 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6948 - accuracy: 0.5223\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6775 - accuracy: 0.6606\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.661 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6935 - accuracy: 0.5155\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6813 - accuracy: 0.6045\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.605 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6917 - accuracy: 0.5237\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6755 - accuracy: 0.6324\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.632 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6928 - accuracy: 0.5240\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6754 - accuracy: 0.6522\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.652 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6953 - accuracy: 0.5015\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5368\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.537 total time=   7.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6937 - accuracy: 0.5208\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6812 - accuracy: 0.6189\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.619 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6954 - accuracy: 0.5108\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6845 - accuracy: 0.6397\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.640 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6943 - accuracy: 0.5121\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6847 - accuracy: 0.6008\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.601 total time=   6.9s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6936 - accuracy: 0.5188\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6800 - accuracy: 0.6305\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.631 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6934 - accuracy: 0.5220\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6825 - accuracy: 0.6231\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.623 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6918 - accuracy: 0.5268\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6756 - accuracy: 0.6493\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.649 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6958 - accuracy: 0.5123\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6897 - accuracy: 0.6132\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.613 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6932 - accuracy: 0.5292\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6859 - accuracy: 0.5502\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.550 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6958 - accuracy: 0.5091\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6894 - accuracy: 0.6139\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.614 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6949 - accuracy: 0.5201\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6842 - accuracy: 0.6306\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.631 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6941 - accuracy: 0.5161\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6848 - accuracy: 0.6615\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.661 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6956 - accuracy: 0.5062\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6906 - accuracy: 0.5941\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.594 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6939 - accuracy: 0.5163\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6849 - accuracy: 0.6603\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.660 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6900 - accuracy: 0.5330\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6690 - accuracy: 0.6384\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.638 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6929 - accuracy: 0.5161\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6800 - accuracy: 0.6484\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.648 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6945 - accuracy: 0.5166\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6866 - accuracy: 0.6237\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.624 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6914 - accuracy: 0.5309\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6719 - accuracy: 0.6066\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.607 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6957 - accuracy: 0.5112\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6873 - accuracy: 0.6524\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.652 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6942 - accuracy: 0.5237\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6796 - accuracy: 0.6475\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.648 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6942 - accuracy: 0.5148\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6831 - accuracy: 0.6579\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.658 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6930 - accuracy: 0.5202\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6735 - accuracy: 0.6277\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.628 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6928 - accuracy: 0.5237\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6748 - accuracy: 0.6197\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.620 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6920 - accuracy: 0.5264\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6728 - accuracy: 0.6598\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.660 total time=   6.9s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6927 - accuracy: 0.5215\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6745 - accuracy: 0.6237\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.624 total time=   5.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6935 - accuracy: 0.5139\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6869 - accuracy: 0.6416\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.642 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6885 - accuracy: 0.5387\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6634 - accuracy: 0.6273\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.627 total time=   7.3s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6924 - accuracy: 0.5191\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6764 - accuracy: 0.5840\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.584 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6911 - accuracy: 0.5314\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6673 - accuracy: 0.6567\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.657 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6899 - accuracy: 0.5340\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.6017\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.602 total time=   7.1s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6951 - accuracy: 0.5138\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6849 - accuracy: 0.5347\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.535 total time=   8.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6885 - accuracy: 0.5405\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6494\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.649 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6883 - accuracy: 0.5403\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6525 - accuracy: 0.6586\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.659 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6898 - accuracy: 0.5376\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6627 - accuracy: 0.6589\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.659 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6897 - accuracy: 0.5328\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6758 - accuracy: 0.5774\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.577 total time=   6.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6935 - accuracy: 0.5116\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6847 - accuracy: 0.6358\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.636 total time=   7.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6935 - accuracy: 0.5205\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6760 - accuracy: 0.6681\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6920 - accuracy: 0.5253\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6774 - accuracy: 0.6476\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.648 total time=   5.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6933 - accuracy: 0.5156\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6866 - accuracy: 0.5995\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.599 total time=   7.0s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6903 - accuracy: 0.5307\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6696 - accuracy: 0.6439\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.644 total time=   5.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6929 - accuracy: 0.5173\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6853 - accuracy: 0.5692\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.569 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6890 - accuracy: 0.5357\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6635 - accuracy: 0.6009\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.601 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6873 - accuracy: 0.5397\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6171\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.617 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6941 - accuracy: 0.5157\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6870 - accuracy: 0.6196\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.620 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6930 - accuracy: 0.5140\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6838 - accuracy: 0.5846\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.585 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6872 - accuracy: 0.5424\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6499 - accuracy: 0.6585\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.659 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6930 - accuracy: 0.5206\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6800 - accuracy: 0.6191\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.619 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6949 - accuracy: 0.5132\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6855 - accuracy: 0.5982\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.598 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6920 - accuracy: 0.5255\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6765 - accuracy: 0.6387\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.639 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6882 - accuracy: 0.5436\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6552 - accuracy: 0.6527\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.653 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6939 - accuracy: 0.5172\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6791 - accuracy: 0.6638\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.664 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6941 - accuracy: 0.5197\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6836 - accuracy: 0.6020\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.602 total time=   7.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6905 - accuracy: 0.5294\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6778 - accuracy: 0.6545\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.654 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6870 - accuracy: 0.5436\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6728\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.673 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6866 - accuracy: 0.5395\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6412\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.641 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6899 - accuracy: 0.5370\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6584 - accuracy: 0.6517\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.652 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6909 - accuracy: 0.5288\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6562 - accuracy: 0.6696\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6853 - accuracy: 0.5480\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6529 - accuracy: 0.6364\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.636 total time=   7.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6888 - accuracy: 0.5353\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6575\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.657 total time=   6.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6894 - accuracy: 0.5340\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6657\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.666 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6892 - accuracy: 0.5332\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6589 - accuracy: 0.6451\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.645 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6958 - accuracy: 0.5165\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6887 - accuracy: 0.6360\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.636 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6943 - accuracy: 0.5116\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6869 - accuracy: 0.6500\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.650 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6936 - accuracy: 0.5250\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6814 - accuracy: 0.6657\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.666 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6938 - accuracy: 0.5232\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6826 - accuracy: 0.5981\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.598 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6934 - accuracy: 0.5200\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6782 - accuracy: 0.6343\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.634 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6955 - accuracy: 0.5094\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6869 - accuracy: 0.5130\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.513 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6904 - accuracy: 0.5362\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6698 - accuracy: 0.6490\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.649 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6921 - accuracy: 0.5176\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6793 - accuracy: 0.6277\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.628 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6953 - accuracy: 0.5061\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6867 - accuracy: 0.6379\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.638 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6934 - accuracy: 0.5191\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6815 - accuracy: 0.5920\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.592 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6944 - accuracy: 0.5217\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6773 - accuracy: 0.6261\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.626 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6894 - accuracy: 0.5340\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6639 - accuracy: 0.6378\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.638 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6937 - accuracy: 0.5234\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6814 - accuracy: 0.6150\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.615 total time=   7.4s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6948 - accuracy: 0.5149\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6861 - accuracy: 0.5615\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.561 total time=   5.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6967 - accuracy: 0.5079\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6899 - accuracy: 0.6426\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.643 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6924 - accuracy: 0.5276\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6743 - accuracy: 0.6505\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.651 total time=   7.4s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6889 - accuracy: 0.5330\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6498 - accuracy: 0.6769\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.677 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6945 - accuracy: 0.5127\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6856 - accuracy: 0.5576\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.558 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6936 - accuracy: 0.5187\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6854 - accuracy: 0.5606\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.561 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6929 - accuracy: 0.5197\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6772 - accuracy: 0.6645\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.664 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6927 - accuracy: 0.5184\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6806 - accuracy: 0.6579\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.658 total time=   6.8s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6902 - accuracy: 0.5350\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6588 - accuracy: 0.6675\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.668 total time=   5.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6932 - accuracy: 0.5162\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6813 - accuracy: 0.6427\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.643 total time=   7.3s\n",
      "708/708 [==============================] - 6s 6ms/step - loss: 0.6951 - accuracy: 0.5105\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6895 - accuracy: 0.5909\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.591 total time=   6.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6912 - accuracy: 0.5239\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6650 - accuracy: 0.6441\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.644 total time=   6.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6896 - accuracy: 0.5320\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6553 - accuracy: 0.6614\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.661 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6902 - accuracy: 0.5283\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6654 - accuracy: 0.6247\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.625 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6943 - accuracy: 0.5190\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6856 - accuracy: 0.6230\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.623 total time=   8.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6958 - accuracy: 0.5094\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6895 - accuracy: 0.5457\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.546 total time=   5.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6942 - accuracy: 0.5081\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6878 - accuracy: 0.6148\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.615 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6924 - accuracy: 0.5264\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6759 - accuracy: 0.6388\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.639 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6925 - accuracy: 0.5242\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6705 - accuracy: 0.6666\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.667 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6922 - accuracy: 0.5251\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6753 - accuracy: 0.6650\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.665 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6924 - accuracy: 0.5255\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6817 - accuracy: 0.6324\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.632 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6921 - accuracy: 0.5237\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6763 - accuracy: 0.6586\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.659 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6931 - accuracy: 0.5210\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6854 - accuracy: 0.5810\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.581 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6941 - accuracy: 0.5156\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6894 - accuracy: 0.6101\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.610 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6914 - accuracy: 0.5327\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6696 - accuracy: 0.6531\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.653 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6953 - accuracy: 0.5128\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6874 - accuracy: 0.6474\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.647 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6906 - accuracy: 0.5372\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6657 - accuracy: 0.6395\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.639 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6958 - accuracy: 0.5089\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6890 - accuracy: 0.5927\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.593 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6942 - accuracy: 0.5115\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6871 - accuracy: 0.6053\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.605 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6910 - accuracy: 0.5286\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6653 - accuracy: 0.6481\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.648 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6955 - accuracy: 0.5127\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6849 - accuracy: 0.5815\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.582 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6930 - accuracy: 0.5214\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6780 - accuracy: 0.6513\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.651 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6848 - accuracy: 0.5516\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6385\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.638 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6904 - accuracy: 0.5327\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6758\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.676 total time=   7.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6927 - accuracy: 0.5236\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6790 - accuracy: 0.6523\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.652 total time=   5.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6907 - accuracy: 0.5354\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6549\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.655 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6926 - accuracy: 0.5241\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6774 - accuracy: 0.6010\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.601 total time=   7.2s\n",
      "708/708 [==============================] - 7s 7ms/step - loss: 0.6840 - accuracy: 0.5533\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6424 - accuracy: 0.6672\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.667 total time=   7.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6913 - accuracy: 0.5364\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6635 - accuracy: 0.6437\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.644 total time=   6.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6930 - accuracy: 0.5221\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6736 - accuracy: 0.6401\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.640 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6904 - accuracy: 0.5348\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6387\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.639 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6857 - accuracy: 0.5479\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6555 - accuracy: 0.6649\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.665 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6914 - accuracy: 0.5253\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6745 - accuracy: 0.6290\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.629 total time=   7.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6919 - accuracy: 0.5251\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6750 - accuracy: 0.6120\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.612 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6926 - accuracy: 0.5236\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6725 - accuracy: 0.6554\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.655 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6878 - accuracy: 0.5393\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6552\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.655 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6896 - accuracy: 0.5340\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6613 - accuracy: 0.6433\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.643 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6914 - accuracy: 0.5258\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6701 - accuracy: 0.6055\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.605 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6879 - accuracy: 0.5388\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6395\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.639 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6890 - accuracy: 0.5370\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6353\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.635 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6949 - accuracy: 0.5105\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6897 - accuracy: 0.5041\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.504 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6910 - accuracy: 0.5293\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6593 - accuracy: 0.6691\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.669 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6914 - accuracy: 0.5271\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6590\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.659 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6918 - accuracy: 0.5298\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6687 - accuracy: 0.6153\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.615 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6866 - accuracy: 0.5479\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6681\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.668 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6870 - accuracy: 0.5431\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6669 - accuracy: 0.5908\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.591 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6895 - accuracy: 0.5361\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6554 - accuracy: 0.6531\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.653 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6834 - accuracy: 0.5520\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6372 - accuracy: 0.6683\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   7.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6887 - accuracy: 0.5425\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6518 - accuracy: 0.6423\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.642 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6883 - accuracy: 0.5421\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6001\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.600 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6908 - accuracy: 0.5282\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6324\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.632 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6869 - accuracy: 0.5464\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6490 - accuracy: 0.6575\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.657 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6915 - accuracy: 0.5302\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6667 - accuracy: 0.6486\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.649 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6879 - accuracy: 0.5457\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6472 - accuracy: 0.6670\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.667 total time=   6.7s\n",
      "708/708 [==============================] - 7s 7ms/step - loss: 0.6864 - accuracy: 0.5445\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6416 - accuracy: 0.6726\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.673 total time=   7.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6834 - accuracy: 0.5533\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6384 - accuracy: 0.6607\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.661 total time=   7.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6839 - accuracy: 0.5547\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6429 - accuracy: 0.6631\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.663 total time=   5.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6776 - accuracy: 0.5686\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6723\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.672 total time=   6.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6970 - accuracy: 0.5068\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5688\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.569 total time=   6.4s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6955 - accuracy: 0.5084\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6880 - accuracy: 0.5938\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.594 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6974 - accuracy: 0.5007\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6900 - accuracy: 0.6432\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.643 total time=   6.8s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6950 - accuracy: 0.5062\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6878 - accuracy: 0.6368\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.637 total time=   5.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6934 - accuracy: 0.5248\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6825 - accuracy: 0.6708\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.671 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6954 - accuracy: 0.5148\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6873 - accuracy: 0.6014\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.601 total time=   6.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6953 - accuracy: 0.5013\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6898 - accuracy: 0.5548\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.555 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6934 - accuracy: 0.5176\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6857 - accuracy: 0.6156\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.616 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6952 - accuracy: 0.5117\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6881 - accuracy: 0.5930\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.593 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6944 - accuracy: 0.5187\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6842 - accuracy: 0.6148\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.615 total time=   6.0s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6948 - accuracy: 0.5137\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6863 - accuracy: 0.6678\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.668 total time=   5.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6941 - accuracy: 0.5125\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6881 - accuracy: 0.5855\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.586 total time=   5.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6941 - accuracy: 0.5186\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6860 - accuracy: 0.6400\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.640 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6927 - accuracy: 0.5240\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6810 - accuracy: 0.5993\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.599 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6953 - accuracy: 0.5110\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6824 - accuracy: 0.6620\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.662 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6938 - accuracy: 0.5225\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6784 - accuracy: 0.6476\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.648 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6918 - accuracy: 0.5315\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6698 - accuracy: 0.6484\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.648 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6932 - accuracy: 0.5196\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6789 - accuracy: 0.6330\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.633 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6910 - accuracy: 0.5229\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6750 - accuracy: 0.6239\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.624 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6936 - accuracy: 0.5116\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6879 - accuracy: 0.5571\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.557 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6900 - accuracy: 0.5386\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6651 - accuracy: 0.6662\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.666 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6920 - accuracy: 0.5297\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6757 - accuracy: 0.6385\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.638 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6922 - accuracy: 0.5220\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6782 - accuracy: 0.5276\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.528 total time=   6.9s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6929 - accuracy: 0.5226\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6848 - accuracy: 0.5197\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.520 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6921 - accuracy: 0.5249\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6816 - accuracy: 0.5614\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.561 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6880 - accuracy: 0.5416\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6505 - accuracy: 0.6675\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6915 - accuracy: 0.5256\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6702 - accuracy: 0.6483\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.648 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6956 - accuracy: 0.5078\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.6032\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.603 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6943 - accuracy: 0.5115\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6868 - accuracy: 0.6432\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.643 total time=   7.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6947 - accuracy: 0.5167\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6885 - accuracy: 0.5727\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.573 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6927 - accuracy: 0.5277\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6819 - accuracy: 0.5583\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.558 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6924 - accuracy: 0.5209\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6749 - accuracy: 0.6516\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.652 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6923 - accuracy: 0.5276\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6754 - accuracy: 0.6361\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.636 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6928 - accuracy: 0.5192\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6798 - accuracy: 0.6201\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.620 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6944 - accuracy: 0.5136\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6880 - accuracy: 0.6489\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.649 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6942 - accuracy: 0.5094\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6862 - accuracy: 0.6543\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.654 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6931 - accuracy: 0.5228\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6779 - accuracy: 0.6385\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.638 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6912 - accuracy: 0.5285\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6747 - accuracy: 0.6619\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.662 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6968 - accuracy: 0.5030\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5618\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.562 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6932 - accuracy: 0.5269\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6784 - accuracy: 0.6692\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.669 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6929 - accuracy: 0.5240\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6768 - accuracy: 0.6444\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.644 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6910 - accuracy: 0.5282\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6675 - accuracy: 0.6418\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.642 total time=   7.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6934 - accuracy: 0.5179\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6830 - accuracy: 0.5635\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.564 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6927 - accuracy: 0.5256\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6696 - accuracy: 0.6689\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.669 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6941 - accuracy: 0.5149\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6835 - accuracy: 0.6571\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.657 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6956 - accuracy: 0.4995\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6899 - accuracy: 0.6161\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.616 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6900 - accuracy: 0.5257\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.6763\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.676 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6893 - accuracy: 0.5335\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6706 - accuracy: 0.5984\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.598 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6908 - accuracy: 0.5323\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6655 - accuracy: 0.6583\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.658 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6942 - accuracy: 0.5156\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6821 - accuracy: 0.5897\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.590 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6935 - accuracy: 0.5220\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6751 - accuracy: 0.6217\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.622 total time=   8.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6909 - accuracy: 0.5253\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6673 - accuracy: 0.6470\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.647 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6915 - accuracy: 0.5194\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.6696\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.670 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6877 - accuracy: 0.5400\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6625\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.662 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6904 - accuracy: 0.5320\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6685 - accuracy: 0.6456\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.646 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6884 - accuracy: 0.5379\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6490\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.649 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6921 - accuracy: 0.5175\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6803 - accuracy: 0.6401\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.640 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6929 - accuracy: 0.5192\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6853 - accuracy: 0.6014\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.601 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6890 - accuracy: 0.5413\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6725\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.672 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6912 - accuracy: 0.5240\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6715 - accuracy: 0.6396\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.640 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6880 - accuracy: 0.5384\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6471\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.647 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6913 - accuracy: 0.5253\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6658 - accuracy: 0.6513\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.651 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6907 - accuracy: 0.5326\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6657 - accuracy: 0.6525\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.653 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6927 - accuracy: 0.5180\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6824 - accuracy: 0.6026\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.603 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6876 - accuracy: 0.5454\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6497\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.650 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6868 - accuracy: 0.5469\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6475 - accuracy: 0.6581\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.658 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6939 - accuracy: 0.5144\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6847 - accuracy: 0.6466\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.647 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6919 - accuracy: 0.5255\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6699 - accuracy: 0.6486\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.649 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6929 - accuracy: 0.5174\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6770 - accuracy: 0.6076\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.608 total time=   6.9s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6906 - accuracy: 0.5304\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6668 - accuracy: 0.6588\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.659 total time=   5.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6934 - accuracy: 0.5153\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6788 - accuracy: 0.6623\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.662 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6903 - accuracy: 0.5310\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6650 - accuracy: 0.6272\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.627 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6848 - accuracy: 0.5522\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6431 - accuracy: 0.6638\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.664 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6914 - accuracy: 0.5268\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6727 - accuracy: 0.6346\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.635 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6923 - accuracy: 0.5246\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6747 - accuracy: 0.6071\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.607 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6852 - accuracy: 0.5504\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6513 - accuracy: 0.6403\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.640 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6930 - accuracy: 0.5218\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6748 - accuracy: 0.6582\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.658 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6897 - accuracy: 0.5314\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6580 - accuracy: 0.6689\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.669 total time=   8.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6850 - accuracy: 0.5481\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6554 - accuracy: 0.6192\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.619 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6880 - accuracy: 0.5345\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6590\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.659 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6916 - accuracy: 0.5193\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6700 - accuracy: 0.6541\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.654 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6947 - accuracy: 0.5161\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6866 - accuracy: 0.6425\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.642 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6966 - accuracy: 0.5149\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6859 - accuracy: 0.6628\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.663 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6936 - accuracy: 0.5306\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6770 - accuracy: 0.5784\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.578 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6944 - accuracy: 0.5182\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6801 - accuracy: 0.6452\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.645 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6930 - accuracy: 0.5264\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6715 - accuracy: 0.6518\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.652 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6945 - accuracy: 0.5197\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5548\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.555 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6955 - accuracy: 0.5043\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6880 - accuracy: 0.6290\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.629 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6937 - accuracy: 0.5161\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6853 - accuracy: 0.6213\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.621 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6943 - accuracy: 0.5106\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6837 - accuracy: 0.6494\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.649 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6969 - accuracy: 0.5118\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6854 - accuracy: 0.6137\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.614 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6948 - accuracy: 0.5157\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6851 - accuracy: 0.6191\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.619 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6928 - accuracy: 0.5284\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6736 - accuracy: 0.6547\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.655 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6937 - accuracy: 0.5265\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6759 - accuracy: 0.6486\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.649 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6937 - accuracy: 0.5203\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6774 - accuracy: 0.6344\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.634 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6944 - accuracy: 0.5181\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6829 - accuracy: 0.6289\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.629 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6922 - accuracy: 0.5236\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6720 - accuracy: 0.6345\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.635 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6921 - accuracy: 0.5284\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6679 - accuracy: 0.6773\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.677 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6909 - accuracy: 0.5402\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6644 - accuracy: 0.6276\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.628 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6947 - accuracy: 0.5134\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6845 - accuracy: 0.5676\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.568 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6935 - accuracy: 0.5172\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6797 - accuracy: 0.5725\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.573 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6936 - accuracy: 0.5186\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6822 - accuracy: 0.5644\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.564 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6917 - accuracy: 0.5263\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6702 - accuracy: 0.6122\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.612 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6908 - accuracy: 0.5293\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6669 - accuracy: 0.6234\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.623 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6904 - accuracy: 0.5336\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6644 - accuracy: 0.6594\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.659 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6936 - accuracy: 0.5175\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6754 - accuracy: 0.6392\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.639 total time=   8.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6926 - accuracy: 0.5248\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6744 - accuracy: 0.5778\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.578 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6866 - accuracy: 0.5430\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6459 - accuracy: 0.6514\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.651 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6938 - accuracy: 0.5261\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6881 - accuracy: 0.5806\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.581 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6976 - accuracy: 0.5084\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6880 - accuracy: 0.5992\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.599 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6934 - accuracy: 0.5191\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6855 - accuracy: 0.6552\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.655 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6924 - accuracy: 0.5292\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6685 - accuracy: 0.6610\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.661 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6935 - accuracy: 0.5180\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6825 - accuracy: 0.5914\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.591 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6932 - accuracy: 0.5254\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6713 - accuracy: 0.6546\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.655 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6935 - accuracy: 0.5165\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6872 - accuracy: 0.6078\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.608 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6944 - accuracy: 0.5132\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6840 - accuracy: 0.6239\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.624 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6935 - accuracy: 0.5205\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6827 - accuracy: 0.6323\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.632 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6937 - accuracy: 0.5223\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6720 - accuracy: 0.6454\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.645 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6923 - accuracy: 0.5273\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6757 - accuracy: 0.6596\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.660 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6938 - accuracy: 0.5189\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6801 - accuracy: 0.6646\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.665 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6906 - accuracy: 0.5357\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6700 - accuracy: 0.6075\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.607 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6915 - accuracy: 0.5315\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6697 - accuracy: 0.6498\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.650 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6951 - accuracy: 0.5133\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6851 - accuracy: 0.6175\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.618 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6937 - accuracy: 0.5238\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6818 - accuracy: 0.5737\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.574 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6915 - accuracy: 0.5236\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6703 - accuracy: 0.6504\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.650 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6935 - accuracy: 0.5193\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6758 - accuracy: 0.6415\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.641 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6938 - accuracy: 0.5093\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6834 - accuracy: 0.6246\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.625 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6899 - accuracy: 0.5373\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6616 - accuracy: 0.6713\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.671 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6909 - accuracy: 0.5322\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6638 - accuracy: 0.6607\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.661 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6930 - accuracy: 0.5248\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6734 - accuracy: 0.6570\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.657 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6947 - accuracy: 0.5209\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6799 - accuracy: 0.6517\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.652 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6931 - accuracy: 0.5260\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6721 - accuracy: 0.6240\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.624 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6899 - accuracy: 0.5415\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6565 - accuracy: 0.6477\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.648 total time=   8.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6899 - accuracy: 0.5321\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6553 - accuracy: 0.6736\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6896 - accuracy: 0.5340\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6689 - accuracy: 0.5803\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.580 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6924 - accuracy: 0.5206\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6727 - accuracy: 0.6481\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.648 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6923 - accuracy: 0.5220\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6786 - accuracy: 0.6592\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.659 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6905 - accuracy: 0.5305\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6737 - accuracy: 0.5943\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.594 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6889 - accuracy: 0.5322\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6551 - accuracy: 0.6407\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.641 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6919 - accuracy: 0.5237\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6713 - accuracy: 0.6852\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.685 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6917 - accuracy: 0.5271\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6713 - accuracy: 0.6211\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.621 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6840 - accuracy: 0.5543\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6401 - accuracy: 0.6623\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.662 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6900 - accuracy: 0.5345\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6553 - accuracy: 0.6661\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.666 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6923 - accuracy: 0.5264\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6708 - accuracy: 0.6567\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.657 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6924 - accuracy: 0.5233\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6749 - accuracy: 0.6609\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.661 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6913 - accuracy: 0.5304\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6691 - accuracy: 0.6376\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.638 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6939 - accuracy: 0.5175\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6811 - accuracy: 0.6301\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.630 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6906 - accuracy: 0.5351\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6554 - accuracy: 0.6579\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.658 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6888 - accuracy: 0.5400\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6518 - accuracy: 0.6551\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.655 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6928 - accuracy: 0.5217\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6732 - accuracy: 0.6573\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.657 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6914 - accuracy: 0.5328\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6699 - accuracy: 0.6011\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.601 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6917 - accuracy: 0.5286\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6637 - accuracy: 0.6728\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.673 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6893 - accuracy: 0.5352\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6524 - accuracy: 0.6679\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6883 - accuracy: 0.5407\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6443\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.644 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6916 - accuracy: 0.5255\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6702 - accuracy: 0.6049\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.605 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6918 - accuracy: 0.5263\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6670 - accuracy: 0.6745\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.674 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6905 - accuracy: 0.5280\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6654 - accuracy: 0.5996\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.600 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6874 - accuracy: 0.5439\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6483 - accuracy: 0.6489\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.649 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6861 - accuracy: 0.5427\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6389 - accuracy: 0.6715\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6819 - accuracy: 0.5583\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6392 - accuracy: 0.6502\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.650 total time=   8.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6891 - accuracy: 0.5390\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6489 - accuracy: 0.6646\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.665 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6886 - accuracy: 0.5367\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6494 - accuracy: 0.6487\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.649 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6938 - accuracy: 0.5220\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6818 - accuracy: 0.6013\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.601 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6967 - accuracy: 0.5114\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6871 - accuracy: 0.6434\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.643 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6934 - accuracy: 0.5185\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6799 - accuracy: 0.6514\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.651 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6923 - accuracy: 0.5312\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6631 - accuracy: 0.6547\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.655 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6953 - accuracy: 0.5119\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6867 - accuracy: 0.6259\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.626 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6937 - accuracy: 0.5159\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6813 - accuracy: 0.6616\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.662 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6835 - accuracy: 0.5539\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6418 - accuracy: 0.6645\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.664 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6845 - accuracy: 0.5535\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6413 - accuracy: 0.6703\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.670 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6889 - accuracy: 0.5402\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6575 - accuracy: 0.6662\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.666 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6925 - accuracy: 0.5315\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6723 - accuracy: 0.6547\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.655 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6930 - accuracy: 0.5198\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6784 - accuracy: 0.6030\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.603 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6945 - accuracy: 0.5212\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6788 - accuracy: 0.6499\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.650 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6879 - accuracy: 0.5451\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6522 - accuracy: 0.6311\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.631 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6912 - accuracy: 0.5297\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6585 - accuracy: 0.6570\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.657 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6939 - accuracy: 0.5171\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6817 - accuracy: 0.5775\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.578 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6933 - accuracy: 0.5270\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6666 - accuracy: 0.6592\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.659 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6947 - accuracy: 0.5190\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6785 - accuracy: 0.6145\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.614 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6898 - accuracy: 0.5335\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6617 - accuracy: 0.6446\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.645 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6908 - accuracy: 0.5342\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6661 - accuracy: 0.6356\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.636 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6934 - accuracy: 0.5201\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6783 - accuracy: 0.6472\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.647 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6927 - accuracy: 0.5246\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6808 - accuracy: 0.6663\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.666 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6907 - accuracy: 0.5345\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6592 - accuracy: 0.6493\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.649 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6881 - accuracy: 0.5414\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6507 - accuracy: 0.6554\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.655 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6923 - accuracy: 0.5234\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6688 - accuracy: 0.6711\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6924 - accuracy: 0.5227\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6752 - accuracy: 0.6277\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.628 total time=   8.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6856 - accuracy: 0.5449\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6499 - accuracy: 0.6220\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.622 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6908 - accuracy: 0.5274\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6723 - accuracy: 0.5684\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.568 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6945 - accuracy: 0.5135\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6864 - accuracy: 0.5858\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.586 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6925 - accuracy: 0.5294\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6716 - accuracy: 0.6681\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6968 - accuracy: 0.5039\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6902 - accuracy: 0.6067\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.607 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6927 - accuracy: 0.5236\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6707 - accuracy: 0.6587\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.659 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6929 - accuracy: 0.5238\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6807 - accuracy: 0.6205\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.620 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6913 - accuracy: 0.5373\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6601 - accuracy: 0.6673\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.667 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6850 - accuracy: 0.5512\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6575 - accuracy: 0.6050\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.605 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6890 - accuracy: 0.5382\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6550 - accuracy: 0.6570\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.657 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6863 - accuracy: 0.5472\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6420 - accuracy: 0.6670\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.667 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6801 - accuracy: 0.5674\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6641\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.664 total time=   7.5s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6914 - accuracy: 0.5305\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6665 - accuracy: 0.6034\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.603 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6952 - accuracy: 0.5162\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6853 - accuracy: 0.5739\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.574 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6883 - accuracy: 0.5430\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6493 - accuracy: 0.6634\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.663 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6890 - accuracy: 0.5465\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6583 - accuracy: 0.6389\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.639 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6820 - accuracy: 0.5577\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6361 - accuracy: 0.6683\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.668 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6880 - accuracy: 0.5386\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6552 - accuracy: 0.6093\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.609 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6921 - accuracy: 0.5274\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6662 - accuracy: 0.6617\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.662 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6852 - accuracy: 0.5504\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6431 - accuracy: 0.6594\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.659 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6908 - accuracy: 0.5273\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6610 - accuracy: 0.6532\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.653 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6912 - accuracy: 0.5259\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6694 - accuracy: 0.6651\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.665 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6900 - accuracy: 0.5341\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6540 - accuracy: 0.6653\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.665 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6916 - accuracy: 0.5283\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6662 - accuracy: 0.6420\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.642 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6903 - accuracy: 0.5358\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6536\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.654 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6859 - accuracy: 0.5502\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6472 - accuracy: 0.6347\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.635 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6882 - accuracy: 0.5424\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6396 - accuracy: 0.6473\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.647 total time=   8.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6864 - accuracy: 0.5477\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6420 - accuracy: 0.6560\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.656 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6871 - accuracy: 0.5406\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6529 - accuracy: 0.6159\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.616 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6934 - accuracy: 0.5221\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6777 - accuracy: 0.6004\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.600 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6933 - accuracy: 0.5219\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6844 - accuracy: 0.6060\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.606 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6934 - accuracy: 0.5204\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6772 - accuracy: 0.6401\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.640 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6904 - accuracy: 0.5337\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6571 - accuracy: 0.6551\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.655 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6858 - accuracy: 0.5482\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6449 - accuracy: 0.6549\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.655 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6890 - accuracy: 0.5378\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6743\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6908 - accuracy: 0.5305\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6634 - accuracy: 0.6229\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.623 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6877 - accuracy: 0.5412\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6548 - accuracy: 0.6440\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.644 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6912 - accuracy: 0.5207\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6670 - accuracy: 0.6533\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.653 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6908 - accuracy: 0.5322\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6655 - accuracy: 0.6293\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.629 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6916 - accuracy: 0.5221\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6706 - accuracy: 0.6519\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.652 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6918 - accuracy: 0.5262\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6688 - accuracy: 0.5860\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.586 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6832 - accuracy: 0.5518\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6419 - accuracy: 0.6508\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.651 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6824 - accuracy: 0.5603\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6592 - accuracy: 0.5949\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.595 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6896 - accuracy: 0.5344\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6497 - accuracy: 0.6697\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.670 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6825 - accuracy: 0.5543\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6434 - accuracy: 0.6373\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.637 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6854 - accuracy: 0.5497\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6407 - accuracy: 0.6573\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.657 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6859 - accuracy: 0.5503\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6373 - accuracy: 0.6628\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.663 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6881 - accuracy: 0.5342\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6432 - accuracy: 0.6532\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.653 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6856 - accuracy: 0.5482\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6361 - accuracy: 0.6675\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.667 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6914 - accuracy: 0.5248\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6612 - accuracy: 0.6592\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.659 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6847 - accuracy: 0.5513\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6413 - accuracy: 0.6580\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.658 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6787 - accuracy: 0.5664\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6789\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.679 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6892 - accuracy: 0.5370\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6547 - accuracy: 0.6538\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.654 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6890 - accuracy: 0.5386\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6535 - accuracy: 0.6473\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.647 total time=   8.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6898 - accuracy: 0.5341\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6543 - accuracy: 0.6577\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.658 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6789 - accuracy: 0.5663\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6728\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.673 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6715 - accuracy: 0.5804\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6321 - accuracy: 0.6592\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.659 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6699 - accuracy: 0.5838\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6219 - accuracy: 0.6752\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.675 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6738 - accuracy: 0.5707\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6266 - accuracy: 0.6705\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6653 - accuracy: 0.5993\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6330 - accuracy: 0.6668\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.667 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6659 - accuracy: 0.5907\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6215 - accuracy: 0.6801\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.680 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6634 - accuracy: 0.6074\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6461 - accuracy: 0.6476\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.648 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6623 - accuracy: 0.6053\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6291 - accuracy: 0.6699\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.670 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6675 - accuracy: 0.6029\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6184 - accuracy: 0.6793\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.679 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6620 - accuracy: 0.6115\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6312 - accuracy: 0.6700\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.670 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6681 - accuracy: 0.5971\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6382 - accuracy: 0.6630\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.663 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6634 - accuracy: 0.6110\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6206 - accuracy: 0.6760\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.676 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6719 - accuracy: 0.5788\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6313 - accuracy: 0.6623\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.662 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6596 - accuracy: 0.6069\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6261 - accuracy: 0.6680\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.668 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6698 - accuracy: 0.5900\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6258 - accuracy: 0.6787\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.679 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6635 - accuracy: 0.6018\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6562 - accuracy: 0.6287\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.629 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6578 - accuracy: 0.6154\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6376 - accuracy: 0.6455\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.645 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6662 - accuracy: 0.6027\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6383 - accuracy: 0.6673\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.667 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6641 - accuracy: 0.6002\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6255\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.625 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6583 - accuracy: 0.6162\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6648\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.665 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6617 - accuracy: 0.6096\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6475 - accuracy: 0.6437\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.644 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6662 - accuracy: 0.5983\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6744\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.674 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6561 - accuracy: 0.6202\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6249 - accuracy: 0.6698\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6634 - accuracy: 0.6077\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6335 - accuracy: 0.6739\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6656 - accuracy: 0.5985\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6294 - accuracy: 0.6699\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6568 - accuracy: 0.6143\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6369 - accuracy: 0.6638\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.664 total time=   8.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6612 - accuracy: 0.6113\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6267 - accuracy: 0.6727\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.673 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6545 - accuracy: 0.6204\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6300 - accuracy: 0.6576\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.658 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6607 - accuracy: 0.6114\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6340 - accuracy: 0.6720\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.672 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6654 - accuracy: 0.6047\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6238 - accuracy: 0.6740\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.674 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6619 - accuracy: 0.6061\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6315 - accuracy: 0.6574\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.657 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6564 - accuracy: 0.6160\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6262 - accuracy: 0.6694\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6624 - accuracy: 0.6038\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6209 - accuracy: 0.6766\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.677 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6601 - accuracy: 0.6158\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6310 - accuracy: 0.6655\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.665 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6580 - accuracy: 0.6160\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6338 - accuracy: 0.6527\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.653 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6661 - accuracy: 0.5942\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6366 - accuracy: 0.6558\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.656 total time=   7.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6616 - accuracy: 0.6013\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6708\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.671 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6639 - accuracy: 0.6066\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6325 - accuracy: 0.6625\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.663 total time=   6.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6641 - accuracy: 0.6070\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6324 - accuracy: 0.6744\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.674 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6632 - accuracy: 0.6053\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6721\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6733 - accuracy: 0.5678\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6653\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.665 total time=   6.4s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6671 - accuracy: 0.5961\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6627\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.663 total time=   6.3s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6660 - accuracy: 0.5974\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6697\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.670 total time=   6.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6525 - accuracy: 0.6258\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6386 - accuracy: 0.6519\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.652 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6641 - accuracy: 0.6012\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6371 - accuracy: 0.6582\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.658 total time=   6.8s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6565 - accuracy: 0.6197\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6714\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.671 total time=   5.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6594 - accuracy: 0.6154\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6706\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.671 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6675 - accuracy: 0.5946\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6394 - accuracy: 0.6809\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.681 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6690 - accuracy: 0.5930\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6349 - accuracy: 0.6664\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.666 total time=   7.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6525 - accuracy: 0.6264\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6245 - accuracy: 0.6718\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.672 total time=   6.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6582 - accuracy: 0.6171\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6829\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.683 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6579 - accuracy: 0.6194\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6734\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.673 total time=   7.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6554 - accuracy: 0.6200\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6354 - accuracy: 0.6496\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.650 total time=   6.9s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6609 - accuracy: 0.6148\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6727\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.673 total time=   5.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6569 - accuracy: 0.6168\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6395 - accuracy: 0.6512\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.651 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6612 - accuracy: 0.6102\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6338 - accuracy: 0.6544\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.654 total time=   7.3s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6599 - accuracy: 0.6167\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6165 - accuracy: 0.6821\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.682 total time=   5.5s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6605 - accuracy: 0.6138\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6679\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   5.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6561 - accuracy: 0.6175\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6731\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.673 total time=   5.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6632 - accuracy: 0.6073\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6794\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.679 total time=   5.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6578 - accuracy: 0.6135\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6707\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.671 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6572 - accuracy: 0.6121\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6229\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.623 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6616 - accuracy: 0.6106\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6151 - accuracy: 0.6834\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.683 total time=   7.3s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6562 - accuracy: 0.6199\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6226 - accuracy: 0.6757\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.676 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6592 - accuracy: 0.6152\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6320 - accuracy: 0.6692\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.669 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6620 - accuracy: 0.6128\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6341 - accuracy: 0.6640\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.664 total time=   6.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6589 - accuracy: 0.6167\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6754\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.675 total time=   6.8s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6543 - accuracy: 0.6220\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6348 - accuracy: 0.6630\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.663 total time=   6.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6601 - accuracy: 0.6139\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6623\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.662 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6547 - accuracy: 0.6267\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6763\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.676 total time=   6.9s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6541 - accuracy: 0.6216\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6417 - accuracy: 0.6344\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.634 total time=   6.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6565 - accuracy: 0.6166\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6731\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.673 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6570 - accuracy: 0.6176\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6286 - accuracy: 0.6660\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.666 total time=   6.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6570 - accuracy: 0.6223\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6378 - accuracy: 0.6613\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.661 total time=   5.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6617 - accuracy: 0.6148\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6244 - accuracy: 0.6651\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.665 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6580 - accuracy: 0.6192\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6360 - accuracy: 0.6496\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.650 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6582 - accuracy: 0.6143\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6389 - accuracy: 0.6557\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.656 total time=   7.9s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6652 - accuracy: 0.5989\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6619\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.662 total time=   5.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6620 - accuracy: 0.6108\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6701\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   6.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6538 - accuracy: 0.6236\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6337 - accuracy: 0.6563\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.656 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6593 - accuracy: 0.6102\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6638\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.664 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6572 - accuracy: 0.6201\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6331 - accuracy: 0.6691\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.669 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6594 - accuracy: 0.6224\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6615\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.661 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6688 - accuracy: 0.5990\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6759\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.676 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6640 - accuracy: 0.6064\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6229 - accuracy: 0.6675\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6623 - accuracy: 0.6084\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6534\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.653 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6663 - accuracy: 0.5957\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6624\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.662 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6635 - accuracy: 0.6083\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6340 - accuracy: 0.6738\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6585 - accuracy: 0.6152\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6706\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.671 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6591 - accuracy: 0.6155\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6546\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.655 total time=   7.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6609 - accuracy: 0.6078\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6773\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.677 total time=   6.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6626 - accuracy: 0.6061\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6349 - accuracy: 0.6617\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.662 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6683 - accuracy: 0.5947\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6223 - accuracy: 0.6789\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.679 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6590 - accuracy: 0.6176\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6434 - accuracy: 0.6394\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.639 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6663 - accuracy: 0.5944\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6352 - accuracy: 0.6696\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.670 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6714 - accuracy: 0.5816\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6557\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.656 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6682 - accuracy: 0.5927\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6386 - accuracy: 0.6454\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.645 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6576 - accuracy: 0.6214\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6529\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.653 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6639 - accuracy: 0.6071\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6213 - accuracy: 0.6834\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.683 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6547 - accuracy: 0.6218\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6788\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.679 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6606 - accuracy: 0.6093\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6463 - accuracy: 0.6298\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.630 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6726 - accuracy: 0.5788\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6328\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.633 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6607 - accuracy: 0.6135\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6258 - accuracy: 0.6744\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.674 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6627 - accuracy: 0.6044\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6720\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.672 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6598 - accuracy: 0.6157\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6172 - accuracy: 0.6778\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.678 total time=   7.3s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6597 - accuracy: 0.6109\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6538\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.654 total time=   5.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6619 - accuracy: 0.6116\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6516\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.652 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6568 - accuracy: 0.6187\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6156 - accuracy: 0.6844\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.684 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6532 - accuracy: 0.6241\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6289 - accuracy: 0.6713\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.671 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6576 - accuracy: 0.6206\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6385 - accuracy: 0.6482\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.648 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6691 - accuracy: 0.5974\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6804\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.680 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6551 - accuracy: 0.6353\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6692\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.669 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6566 - accuracy: 0.6152\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6310 - accuracy: 0.6668\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.667 total time=   7.3s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6635 - accuracy: 0.6029\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6826\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.683 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6581 - accuracy: 0.6118\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6223 - accuracy: 0.6714\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.671 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6573 - accuracy: 0.6215\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6380 - accuracy: 0.6616\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.662 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6555 - accuracy: 0.6227\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6177 - accuracy: 0.6829\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.683 total time=   7.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6546 - accuracy: 0.6253\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6288 - accuracy: 0.6738\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.674 total time=   7.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6581 - accuracy: 0.6190\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6315 - accuracy: 0.6635\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.663 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6620 - accuracy: 0.6147\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6744\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.674 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6600 - accuracy: 0.6127\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6245 - accuracy: 0.6691\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.669 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6608 - accuracy: 0.6101\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6349 - accuracy: 0.6596\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.660 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6569 - accuracy: 0.6203\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6624\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.662 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6583 - accuracy: 0.6155\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6230 - accuracy: 0.6756\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.676 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6568 - accuracy: 0.6133\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6472\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.647 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6610 - accuracy: 0.6092\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6191 - accuracy: 0.6783\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.678 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6604 - accuracy: 0.6088\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6544\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.654 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6541 - accuracy: 0.6264\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.6717\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.672 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6698 - accuracy: 0.5876\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6804\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.680 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6560 - accuracy: 0.6177\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6619\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.662 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6555 - accuracy: 0.6213\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6483 - accuracy: 0.6305\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.631 total time=   6.4s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6648 - accuracy: 0.6024\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6317 - accuracy: 0.6581\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.658 total time=   6.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6602 - accuracy: 0.6102\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6314 - accuracy: 0.6696\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6482 - accuracy: 0.6328\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6647\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.665 total time=   8.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6577 - accuracy: 0.6222\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6771\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.677 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6531 - accuracy: 0.6240\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6628\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.663 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6605 - accuracy: 0.6101\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6706\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.671 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6647 - accuracy: 0.6079\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6338 - accuracy: 0.6798\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.680 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6620 - accuracy: 0.6061\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6361 - accuracy: 0.6685\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6547 - accuracy: 0.6215\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6705\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.670 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6584 - accuracy: 0.6182\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6731\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.673 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6576 - accuracy: 0.6182\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6242 - accuracy: 0.6742\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   7.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6547 - accuracy: 0.6213\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6695\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.669 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6649 - accuracy: 0.6009\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6353 - accuracy: 0.6500\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.650 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6550 - accuracy: 0.6168\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6388 - accuracy: 0.6420\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.642 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6579 - accuracy: 0.6194\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6248 - accuracy: 0.6698\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.670 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6655 - accuracy: 0.6001\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6310 - accuracy: 0.6671\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.667 total time=   6.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6582 - accuracy: 0.6159\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6252 - accuracy: 0.6654\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.665 total time=   6.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6568 - accuracy: 0.6184\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6363 - accuracy: 0.6539\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.654 total time=   6.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6547 - accuracy: 0.6222\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6152 - accuracy: 0.6819\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.682 total time=   6.3s\n",
      "708/708 [==============================] - 4s 5ms/step - loss: 0.6634 - accuracy: 0.6035\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.6736\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.674 total time=   5.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6576 - accuracy: 0.6118\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6227\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.623 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6637 - accuracy: 0.6033\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6662\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.666 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6577 - accuracy: 0.6195\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6627\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.663 total time=   7.3s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6576 - accuracy: 0.6226\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6333 - accuracy: 0.6679\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.668 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6612 - accuracy: 0.6142\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6257 - accuracy: 0.6582\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.658 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6586 - accuracy: 0.6152\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6665\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.666 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6547 - accuracy: 0.6239\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6290 - accuracy: 0.6665\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.666 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6617 - accuracy: 0.6065\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6731\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.673 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6591 - accuracy: 0.6153\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6794\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.679 total time=   6.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6522 - accuracy: 0.6291\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6368 - accuracy: 0.6675\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   8.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6610 - accuracy: 0.6046\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6331 - accuracy: 0.6660\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.666 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6536 - accuracy: 0.6255\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6365 - accuracy: 0.6763\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.676 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6574 - accuracy: 0.6157\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6708\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.671 total time=   7.0s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6652 - accuracy: 0.6063\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6719\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.672 total time=   5.5s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6623 - accuracy: 0.6059\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6240 - accuracy: 0.6695\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.669 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6597 - accuracy: 0.6165\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6238 - accuracy: 0.6722\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.672 total time=   6.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6585 - accuracy: 0.6122\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6851\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.685 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6609 - accuracy: 0.6110\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6269 - accuracy: 0.6718\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.672 total time=   6.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6636 - accuracy: 0.5997\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6554\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.655 total time=   6.3s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6648 - accuracy: 0.5991\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6770\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.677 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6573 - accuracy: 0.6169\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6268 - accuracy: 0.6776\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.678 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6541 - accuracy: 0.6254\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6653\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.665 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6602 - accuracy: 0.6128\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6794\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.679 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6524 - accuracy: 0.6280\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6685\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.668 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6616 - accuracy: 0.6075\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6368\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.637 total time=   7.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6620 - accuracy: 0.6109\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6348 - accuracy: 0.6753\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.675 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6566 - accuracy: 0.6188\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6314 - accuracy: 0.6741\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.674 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6559 - accuracy: 0.6182\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6301 - accuracy: 0.6668\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.667 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6565 - accuracy: 0.6216\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6357 - accuracy: 0.6745\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.675 total time=   7.1s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6510 - accuracy: 0.6310\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6722\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.672 total time=   5.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6540 - accuracy: 0.6316\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6715\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.672 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6555 - accuracy: 0.6296\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6165 - accuracy: 0.6849\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.685 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6574 - accuracy: 0.6191\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6612\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.661 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6560 - accuracy: 0.6208\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6569\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.657 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6610 - accuracy: 0.6112\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6619\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.662 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6560 - accuracy: 0.6231\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6334 - accuracy: 0.6755\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.675 total time=   7.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6490 - accuracy: 0.6314\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6385\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.638 total time=   6.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6574 - accuracy: 0.6207\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6132 - accuracy: 0.6803\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.680 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6549 - accuracy: 0.6270\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6268 - accuracy: 0.6769\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.677 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6605 - accuracy: 0.6135\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6482 - accuracy: 0.6341\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.634 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6666 - accuracy: 0.5894\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6719\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.672 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6647 - accuracy: 0.6013\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6708\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.671 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6597 - accuracy: 0.6103\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6287 - accuracy: 0.6613\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.661 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6707 - accuracy: 0.5843\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6169 - accuracy: 0.6825\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.682 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6601 - accuracy: 0.6113\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6426 - accuracy: 0.6472\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.647 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6555 - accuracy: 0.6241\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6253 - accuracy: 0.6685\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.669 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6621 - accuracy: 0.6102\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6212 - accuracy: 0.6779\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.678 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6557 - accuracy: 0.6194\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6635\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.664 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6557 - accuracy: 0.6194\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6679\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.668 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6582 - accuracy: 0.6236\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6201 - accuracy: 0.6821\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.682 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6573 - accuracy: 0.6165\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6248 - accuracy: 0.6705\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.671 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6560 - accuracy: 0.6234\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6685\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.669 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6564 - accuracy: 0.6226\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6852\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.685 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6573 - accuracy: 0.6203\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6797\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.680 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6522 - accuracy: 0.6264\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.6660\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.666 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6557 - accuracy: 0.6194\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6295 - accuracy: 0.6603\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.660 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6522 - accuracy: 0.6264\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6226 - accuracy: 0.6735\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.674 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6564 - accuracy: 0.6182\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6680\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.668 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6557 - accuracy: 0.6226\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6190 - accuracy: 0.6758\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.676 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6608 - accuracy: 0.6121\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6221 - accuracy: 0.6754\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6594 - accuracy: 0.6127\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6669\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.667 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6564 - accuracy: 0.6194\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6444 - accuracy: 0.6281\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.628 total time=   6.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6499 - accuracy: 0.6331\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6198 - accuracy: 0.6778\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.678 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6529 - accuracy: 0.6290\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6293 - accuracy: 0.6726\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.673 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6542 - accuracy: 0.6236\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6696\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.670 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6535 - accuracy: 0.6260\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6232 - accuracy: 0.6732\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.673 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6585 - accuracy: 0.6218\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6390 - accuracy: 0.6597\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.660 total time=   6.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6662 - accuracy: 0.6054\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6288 - accuracy: 0.6771\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.677 total time=   6.4s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6589 - accuracy: 0.6169\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6683\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   6.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6587 - accuracy: 0.6185\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6592\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.659 total time=   5.9s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6582 - accuracy: 0.6186\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6194 - accuracy: 0.6752\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.675 total time=   6.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6534 - accuracy: 0.6245\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6579\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.658 total time=   7.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6528 - accuracy: 0.6300\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6323 - accuracy: 0.6649\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.665 total time=   5.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6525 - accuracy: 0.6293\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6190 - accuracy: 0.6805\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.681 total time=   6.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6548 - accuracy: 0.6196\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6310 - accuracy: 0.6592\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.659 total time=   5.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6566 - accuracy: 0.6213\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6245 - accuracy: 0.6623\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.662 total time=   6.4s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6569 - accuracy: 0.6247\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6640\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.664 total time=   6.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6559 - accuracy: 0.6234\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6245 - accuracy: 0.6763\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.676 total time=   7.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6594 - accuracy: 0.6129\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6371 - accuracy: 0.6501\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.650 total time=   5.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6530 - accuracy: 0.6305\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6252 - accuracy: 0.6724\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.672 total time=   6.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6576 - accuracy: 0.6149\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6264 - accuracy: 0.6720\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.672 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6567 - accuracy: 0.6230\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6645\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.664 total time=   6.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6551 - accuracy: 0.6209\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6295 - accuracy: 0.6780\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.678 total time=   6.4s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6518 - accuracy: 0.6305\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6241 - accuracy: 0.6783\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.678 total time=   5.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6588 - accuracy: 0.6155\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6389 - accuracy: 0.6472\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.647 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6617 - accuracy: 0.6107\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6622\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.662 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6656 - accuracy: 0.5960\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6388 - accuracy: 0.6565\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.656 total time=   6.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6507 - accuracy: 0.6343\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6431 - accuracy: 0.6400\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.640 total time=   5.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6582 - accuracy: 0.6207\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6231 - accuracy: 0.6702\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   7.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6558 - accuracy: 0.6207\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6291 - accuracy: 0.6749\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.675 total time=   5.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6566 - accuracy: 0.6204\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6337 - accuracy: 0.6580\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.658 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6565 - accuracy: 0.6239\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6228 - accuracy: 0.6739\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   6.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6557 - accuracy: 0.6221\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6673\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   5.9s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6672 - accuracy: 0.5945\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6380\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.638 total time=   5.6s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6718 - accuracy: 0.5910\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6860\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.686 total time=   5.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6623 - accuracy: 0.6089\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6244 - accuracy: 0.6749\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.675 total time=   6.5s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6630 - accuracy: 0.6042\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6295 - accuracy: 0.6639\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.664 total time=   6.2s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6709 - accuracy: 0.5863\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6502\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.650 total time=   5.6s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6661 - accuracy: 0.5962\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6284 - accuracy: 0.6649\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.665 total time=   6.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6609 - accuracy: 0.6058\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6706\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.671 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6700 - accuracy: 0.5856\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6136 - accuracy: 0.6824\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.682 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6604 - accuracy: 0.6075\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6419 - accuracy: 0.6408\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.641 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6602 - accuracy: 0.6146\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6292 - accuracy: 0.6623\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.662 total time=   6.7s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6691 - accuracy: 0.5922\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6789\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.679 total time=   5.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6615 - accuracy: 0.6129\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6506\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.651 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6636 - accuracy: 0.6015\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6158\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.616 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6625 - accuracy: 0.6070\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6674\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.667 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6637 - accuracy: 0.5993\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6454 - accuracy: 0.6544\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.654 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6571 - accuracy: 0.6139\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6657\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.666 total time=   7.3s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6625 - accuracy: 0.6083\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6231 - accuracy: 0.6847\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.685 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6578 - accuracy: 0.6190\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6215 - accuracy: 0.6724\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.672 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6619 - accuracy: 0.6100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6720\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.672 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6618 - accuracy: 0.6101\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6196 - accuracy: 0.6821\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.682 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6641 - accuracy: 0.6044\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6707\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.671 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6554 - accuracy: 0.6203\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6320 - accuracy: 0.6628\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.663 total time=   7.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6618 - accuracy: 0.6093\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6575\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.657 total time=   5.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6587 - accuracy: 0.6107\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6206 - accuracy: 0.6756\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.676 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6572 - accuracy: 0.6146\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6248 - accuracy: 0.6677\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6573 - accuracy: 0.6149\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6131 - accuracy: 0.6838\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.684 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6629 - accuracy: 0.6052\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6670\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6587 - accuracy: 0.6201\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6296 - accuracy: 0.6669\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.667 total time=   6.9s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6640 - accuracy: 0.6063\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6784\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.678 total time=   5.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6696 - accuracy: 0.5891\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6378\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.638 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6579 - accuracy: 0.6190\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6683\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.668 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6690 - accuracy: 0.5858\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6328 - accuracy: 0.6570\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.657 total time=   7.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6615 - accuracy: 0.6060\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6618\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.662 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6626 - accuracy: 0.6030\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6560\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.656 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6645 - accuracy: 0.6043\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6315 - accuracy: 0.6584\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.658 total time=   6.9s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6605 - accuracy: 0.6055\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6711\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.671 total time=   5.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6592 - accuracy: 0.6154\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.6660\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.666 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6654 - accuracy: 0.6005\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6215 - accuracy: 0.6830\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.683 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6650 - accuracy: 0.6012\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.6744\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.674 total time=   7.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6562 - accuracy: 0.6181\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6680\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.668 total time=   6.8s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6581 - accuracy: 0.6159\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6165 - accuracy: 0.6804\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.680 total time=   5.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6585 - accuracy: 0.6166\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6677\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.668 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6581 - accuracy: 0.6171\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6354 - accuracy: 0.6618\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.662 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6586 - accuracy: 0.6104\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6735\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.673 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6558 - accuracy: 0.6199\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6194 - accuracy: 0.6767\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.677 total time=   7.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6613 - accuracy: 0.6062\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6635\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.663 total time=   5.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6602 - accuracy: 0.6139\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6239 - accuracy: 0.6741\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.674 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6678 - accuracy: 0.5894\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6767\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.677 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6586 - accuracy: 0.6138\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6404 - accuracy: 0.6519\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.652 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6641 - accuracy: 0.6055\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6276 - accuracy: 0.6759\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.676 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6549 - accuracy: 0.6207\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6760\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.676 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6555 - accuracy: 0.6186\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6312 - accuracy: 0.6705\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.670 total time=   7.7s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6590 - accuracy: 0.6150\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6652\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.665 total time=   6.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6544 - accuracy: 0.6220\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6231 - accuracy: 0.6635\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.663 total time=   7.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6695 - accuracy: 0.5882\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6348 - accuracy: 0.6548\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.655 total time=   5.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6645 - accuracy: 0.6012\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6132 - accuracy: 0.6827\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.683 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6624 - accuracy: 0.6085\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6367 - accuracy: 0.6711\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.671 total time=   6.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6594 - accuracy: 0.6122\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6237 - accuracy: 0.6728\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.673 total time=   6.4s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6561 - accuracy: 0.6218\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6631\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.663 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6588 - accuracy: 0.6152\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6287 - accuracy: 0.6791\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.679 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6576 - accuracy: 0.6151\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6244 - accuracy: 0.6711\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.671 total time=   7.0s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6558 - accuracy: 0.6218\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6164 - accuracy: 0.6722\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.672 total time=   6.3s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6573 - accuracy: 0.6139\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6308 - accuracy: 0.6602\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.660 total time=   5.5s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6559 - accuracy: 0.6220\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6702\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.670 total time=   6.3s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6558 - accuracy: 0.6229\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6723\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   6.9s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6587 - accuracy: 0.6221\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.6794\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.679 total time=   6.7s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6623 - accuracy: 0.6061\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6540\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.654 total time=   5.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6586 - accuracy: 0.6160\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6355 - accuracy: 0.6479\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.648 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6513 - accuracy: 0.6286\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6248 - accuracy: 0.6747\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.675 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6564 - accuracy: 0.6175\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6701\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.670 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6584 - accuracy: 0.6161\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6230 - accuracy: 0.6669\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.667 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6561 - accuracy: 0.6210\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6237 - accuracy: 0.6739\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.674 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6558 - accuracy: 0.6243\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6385 - accuracy: 0.6556\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.656 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6629 - accuracy: 0.6049\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6190 - accuracy: 0.6794\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.679 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6546 - accuracy: 0.6253\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6668\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.667 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6512 - accuracy: 0.6315\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6667\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.667 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6612 - accuracy: 0.6060\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6625\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.663 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6557 - accuracy: 0.6217\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6237 - accuracy: 0.6764\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.676 total time=   7.0s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6543 - accuracy: 0.6201\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6620\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.662 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6567 - accuracy: 0.6168\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6385 - accuracy: 0.6408\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.641 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6542 - accuracy: 0.6249\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6223 - accuracy: 0.6749\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.675 total time=   7.4s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6626 - accuracy: 0.6118\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6446\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.645 total time=   6.3s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6680 - accuracy: 0.5985\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6158 - accuracy: 0.6814\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.681 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6567 - accuracy: 0.6225\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6246 - accuracy: 0.6729\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.673 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6631 - accuracy: 0.6065\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6318 - accuracy: 0.6525\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.653 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6648 - accuracy: 0.5998\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6759\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.676 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6650 - accuracy: 0.5995\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6573\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.657 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6675 - accuracy: 0.5969\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6346 - accuracy: 0.6603\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.660 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6591 - accuracy: 0.6198\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6191 - accuracy: 0.6808\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.681 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6585 - accuracy: 0.6151\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6597\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.660 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6615 - accuracy: 0.6121\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6670\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.667 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6638 - accuracy: 0.6048\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6657\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.666 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6578 - accuracy: 0.6140\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6340 - accuracy: 0.6544\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.654 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6567 - accuracy: 0.6204\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6220 - accuracy: 0.6737\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.674 total time=   7.3s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6585 - accuracy: 0.6216\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6185 - accuracy: 0.6814\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.681 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6585 - accuracy: 0.6191\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6196 - accuracy: 0.6787\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.679 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6600 - accuracy: 0.6117\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6336 - accuracy: 0.6545\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.654 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6588 - accuracy: 0.6162\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6165 - accuracy: 0.6852\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.685 total time=   7.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6547 - accuracy: 0.6211\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6202 - accuracy: 0.6775\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.678 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6658 - accuracy: 0.6024\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6691\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6612 - accuracy: 0.6121\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6745 - accuracy: 0.5417\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.542 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6585 - accuracy: 0.6194\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6218 - accuracy: 0.6711\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.671 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6602 - accuracy: 0.6122\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6315 - accuracy: 0.6706\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   7.2s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6612 - accuracy: 0.6081\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6775\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.678 total time=   6.7s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6564 - accuracy: 0.6239\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6212 - accuracy: 0.6742\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   6.9s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6526 - accuracy: 0.6225\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6546\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.655 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6563 - accuracy: 0.6220\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6134 - accuracy: 0.6862\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.686 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6540 - accuracy: 0.6196\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.5996\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.600 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6539 - accuracy: 0.6302\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6661\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.666 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6661 - accuracy: 0.6057\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6766\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.677 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6692 - accuracy: 0.5855\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6313 - accuracy: 0.6570\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.657 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6565 - accuracy: 0.6209\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6304 - accuracy: 0.6586\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.659 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6685 - accuracy: 0.5895\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6376 - accuracy: 0.6594\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.659 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6636 - accuracy: 0.6029\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6749\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.675 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6609 - accuracy: 0.6077\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6284 - accuracy: 0.6730\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6611 - accuracy: 0.6109\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6227 - accuracy: 0.6789\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.679 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6594 - accuracy: 0.6117\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6756\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.676 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6625 - accuracy: 0.6113\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6363 - accuracy: 0.6661\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.666 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6642 - accuracy: 0.6036\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6430 - accuracy: 0.6362\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.636 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6603 - accuracy: 0.6080\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.6751\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.675 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6602 - accuracy: 0.6099\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.6642\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.664 total time=   7.1s\n",
      "708/708 [==============================] - 5s 6ms/step - loss: 0.6630 - accuracy: 0.6083\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6365 - accuracy: 0.6619\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.662 total time=   5.6s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6585 - accuracy: 0.6151\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6224 - accuracy: 0.6734\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.673 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6519 - accuracy: 0.6274\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6610\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.661 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6636 - accuracy: 0.6043\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6231 - accuracy: 0.6763\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.676 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6546 - accuracy: 0.6224\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6251 - accuracy: 0.6736\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.674 total time=   7.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6595 - accuracy: 0.6142\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6580\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.658 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6685 - accuracy: 0.5939\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6246 - accuracy: 0.6690\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   7.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6620 - accuracy: 0.6107\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6205 - accuracy: 0.6745\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.674 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6569 - accuracy: 0.6228\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6653\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.665 total time=   7.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6556 - accuracy: 0.6224\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6355 - accuracy: 0.6584\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.658 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6578 - accuracy: 0.6209\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6716\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.672 total time=   6.9s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6582 - accuracy: 0.6128\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6565\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.657 total time=   5.5s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6583 - accuracy: 0.6141\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6210 - accuracy: 0.6818\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.682 total time=   8.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6544 - accuracy: 0.6228\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6690\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.669 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6525 - accuracy: 0.6336\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.6756\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.676 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6625 - accuracy: 0.6041\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6137 - accuracy: 0.6808\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.681 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6589 - accuracy: 0.6223\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6514\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.651 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6623 - accuracy: 0.6084\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6715\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.672 total time=   6.9s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6591 - accuracy: 0.6193\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6201 - accuracy: 0.6788\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.679 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6591 - accuracy: 0.6173\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6345 - accuracy: 0.6669\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.667 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6546 - accuracy: 0.6259\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6268 - accuracy: 0.6701\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.670 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6574 - accuracy: 0.6138\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6796\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.680 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6540 - accuracy: 0.6219\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6214 - accuracy: 0.6720\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.672 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6556 - accuracy: 0.6133\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6411\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.641 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6630 - accuracy: 0.6103\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6798\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.680 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6568 - accuracy: 0.6195\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6264 - accuracy: 0.6757\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.676 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6591 - accuracy: 0.6161\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6645\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.665 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6602 - accuracy: 0.6129\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6804\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.680 total time=   7.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6587 - accuracy: 0.6124\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6255 - accuracy: 0.6751\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.675 total time=   7.0s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6522 - accuracy: 0.6270\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6268 - accuracy: 0.6668\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.667 total time=   6.8s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6552 - accuracy: 0.6195\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6161 - accuracy: 0.6814\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.681 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6534 - accuracy: 0.6256\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6726\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.673 total time=   7.3s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6576 - accuracy: 0.6112\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6293 - accuracy: 0.6635\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.663 total time=   6.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6622 - accuracy: 0.6112\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6310 - accuracy: 0.6587\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.659 total time=   7.2s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6522 - accuracy: 0.6248\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.5762\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.576 total time=   6.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6490 - accuracy: 0.6337\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6698\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   7.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6592 - accuracy: 0.6190\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6350\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.635 total time=   7.1s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6533 - accuracy: 0.6286\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6401 - accuracy: 0.6720\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.672 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6535 - accuracy: 0.6268\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.6354 - accuracy: 0.6615\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.662 total time=   6.4s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6534 - accuracy: 0.6225\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6146 - accuracy: 0.6825\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.682 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6528 - accuracy: 0.6271\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6247 - accuracy: 0.6716\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.672 total time=   7.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6538 - accuracy: 0.6251\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6247 - accuracy: 0.6697\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.670 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6713 - accuracy: 0.5874\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6258 - accuracy: 0.6803\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.680 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6636 - accuracy: 0.6046\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6223 - accuracy: 0.6779\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.678 total time=   7.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6596 - accuracy: 0.6155\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6296 - accuracy: 0.6670\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.667 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6604 - accuracy: 0.6108\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6304 - accuracy: 0.6664\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.666 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6606 - accuracy: 0.6048\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6453 - accuracy: 0.6378\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.638 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6561 - accuracy: 0.6210\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6316 - accuracy: 0.6642\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.664 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6624 - accuracy: 0.6019\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6430 - accuracy: 0.6574\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.657 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6582 - accuracy: 0.6170\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6260 - accuracy: 0.6796\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.680 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6574 - accuracy: 0.6149\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6225 - accuracy: 0.6734\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.673 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6597 - accuracy: 0.6118\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6305 - accuracy: 0.6630\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.663 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6637 - accuracy: 0.6034\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6380 - accuracy: 0.6675\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.668 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6562 - accuracy: 0.6208\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6225 - accuracy: 0.6740\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.674 total time=   7.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6570 - accuracy: 0.6224\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6173 - accuracy: 0.6763\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.676 total time=   6.8s\n",
      "708/708 [==============================] - 5s 7ms/step - loss: 0.6583 - accuracy: 0.6113\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.6771\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.677 total time=   6.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6466 - accuracy: 0.6385\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6397 - accuracy: 0.6507\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.651 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6578 - accuracy: 0.6152\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6321 - accuracy: 0.6411\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.641 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6589 - accuracy: 0.6122\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6312 - accuracy: 0.6713\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.671 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6635 - accuracy: 0.6025\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6457 - accuracy: 0.6653\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.665 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6583 - accuracy: 0.6203\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6207 - accuracy: 0.6791\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.679 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6624 - accuracy: 0.6099\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6223 - accuracy: 0.6752\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6523 - accuracy: 0.6318\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6329 - accuracy: 0.6572\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.657 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6563 - accuracy: 0.6198\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6266 - accuracy: 0.6815\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.682 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6570 - accuracy: 0.6157\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6263 - accuracy: 0.6713\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6529 - accuracy: 0.6269\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6745 - accuracy: 0.5673\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.567 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6598 - accuracy: 0.6087\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6139 - accuracy: 0.6769\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.677 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6509 - accuracy: 0.6264\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6198 - accuracy: 0.6753\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.675 total time=   8.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6544 - accuracy: 0.6266\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6385 - accuracy: 0.6605\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.660 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6640 - accuracy: 0.6099\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6233 - accuracy: 0.6827\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.683 total time=   7.8s\n",
      "708/708 [==============================] - 4s 6ms/step - loss: 0.6544 - accuracy: 0.6252\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6223 - accuracy: 0.6699\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.670 total time=   5.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6561 - accuracy: 0.6221\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6254 - accuracy: 0.6736\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6579 - accuracy: 0.6182\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6205 - accuracy: 0.6804\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.680 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6542 - accuracy: 0.6264\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6286 - accuracy: 0.6637\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.664 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6465 - accuracy: 0.6423\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6245 - accuracy: 0.6707\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6592 - accuracy: 0.6141\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6310 - accuracy: 0.6556\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.656 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6536 - accuracy: 0.6241\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6388 - accuracy: 0.6575\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.658 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6547 - accuracy: 0.6273\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6249 - accuracy: 0.6732\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.673 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6572 - accuracy: 0.6228\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6308 - accuracy: 0.6616\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.662 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6578 - accuracy: 0.6191\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6398 - accuracy: 0.6593\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.659 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6500 - accuracy: 0.6350\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6274 - accuracy: 0.6631\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.663 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6611 - accuracy: 0.6080\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6288 - accuracy: 0.6830\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.683 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6537 - accuracy: 0.6286\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6243 - accuracy: 0.6712\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6495 - accuracy: 0.6286\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6266 - accuracy: 0.6741\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.674 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6544 - accuracy: 0.6264\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6255 - accuracy: 0.6810\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.681 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6547 - accuracy: 0.6199\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6276 - accuracy: 0.6762\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.676 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6569 - accuracy: 0.6227\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6318 - accuracy: 0.6578\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.658 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6608 - accuracy: 0.6125\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6285 - accuracy: 0.6660\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.666 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6590 - accuracy: 0.6204\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6405 - accuracy: 0.6443\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.644 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6585 - accuracy: 0.6152\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6292 - accuracy: 0.6650\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.665 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6581 - accuracy: 0.6155\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6148 - accuracy: 0.6831\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.683 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6589 - accuracy: 0.6159\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6310 - accuracy: 0.6742\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6508 - accuracy: 0.6313\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6263 - accuracy: 0.6675\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6513 - accuracy: 0.6277\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6212 - accuracy: 0.6819\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.682 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6501 - accuracy: 0.6312\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6299 - accuracy: 0.6667\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   8.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6579 - accuracy: 0.6234\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6328 - accuracy: 0.6697\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.670 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6593 - accuracy: 0.6226\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6170 - accuracy: 0.6843\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.684 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6561 - accuracy: 0.6235\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6198 - accuracy: 0.6786\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.679 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6512 - accuracy: 0.6299\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.6715\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.672 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6614 - accuracy: 0.6100\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6287 - accuracy: 0.6767\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.677 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6546 - accuracy: 0.6222\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6270 - accuracy: 0.6742\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6565 - accuracy: 0.6156\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6307 - accuracy: 0.6668\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.667 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6595 - accuracy: 0.6135\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6178 - accuracy: 0.6794\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.679 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6525 - accuracy: 0.6177\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6163 - accuracy: 0.6760\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.676 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6513 - accuracy: 0.6250\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6262 - accuracy: 0.6651\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.665 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6563 - accuracy: 0.6210\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6211 - accuracy: 0.6704\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.670 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6560 - accuracy: 0.6219\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6353 - accuracy: 0.6571\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.657 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6531 - accuracy: 0.6262\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6251 - accuracy: 0.6781\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.678 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6582 - accuracy: 0.6172\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6181 - accuracy: 0.6842\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.684 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6587 - accuracy: 0.6176\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6259 - accuracy: 0.6718\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.672 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6505 - accuracy: 0.6312\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6343 - accuracy: 0.6660\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.666 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6510 - accuracy: 0.6280\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6193 - accuracy: 0.6643\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.664 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6500 - accuracy: 0.6350\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6325\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.632 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6545 - accuracy: 0.6265\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6318 - accuracy: 0.6713\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6571 - accuracy: 0.6227\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6149 - accuracy: 0.6795\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.679 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6569 - accuracy: 0.6186\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6362 - accuracy: 0.6540\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.654 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6553 - accuracy: 0.6262\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6517 - accuracy: 0.6126\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.613 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6546 - accuracy: 0.6254\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6260 - accuracy: 0.6810\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.681 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6516 - accuracy: 0.6303\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6499 - accuracy: 0.6300\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.630 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6502 - accuracy: 0.6320\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6489 - accuracy: 0.6396\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.640 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6557 - accuracy: 0.6225\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6250 - accuracy: 0.6657\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.666 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6513 - accuracy: 0.6269\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6231 - accuracy: 0.6717\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.672 total time=   7.7s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6671 - accuracy: 0.5959\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6688\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.669 total time=   8.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6625 - accuracy: 0.6066\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6156 - accuracy: 0.6812\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.681 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6683 - accuracy: 0.5917\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6264 - accuracy: 0.6658\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.666 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6623 - accuracy: 0.6079\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6246 - accuracy: 0.6691\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6661 - accuracy: 0.5982\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6280 - accuracy: 0.6808\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.681 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6677 - accuracy: 0.5912\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.6698\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.670 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6610 - accuracy: 0.6111\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6364 - accuracy: 0.6552\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.655 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6704 - accuracy: 0.5873\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6175 - accuracy: 0.6846\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.685 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6610 - accuracy: 0.6067\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6296 - accuracy: 0.6640\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.664 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6671 - accuracy: 0.5971\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6245 - accuracy: 0.6724\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6712 - accuracy: 0.5879\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6331 - accuracy: 0.6758\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.676 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6738 - accuracy: 0.5761\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6736\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.674 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6580 - accuracy: 0.6164\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6322 - accuracy: 0.6647\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.665 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6626 - accuracy: 0.6107\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6147 - accuracy: 0.6836\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.684 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6613 - accuracy: 0.6087\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6367 - accuracy: 0.6491\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.649 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6596 - accuracy: 0.6143\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6314 - accuracy: 0.6585\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.658 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6656 - accuracy: 0.5991\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6214 - accuracy: 0.6728\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.673 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6539 - accuracy: 0.6233\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6261 - accuracy: 0.6768\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.677 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6568 - accuracy: 0.6195\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6271 - accuracy: 0.6706\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6630 - accuracy: 0.6079\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6282 - accuracy: 0.6712\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.671 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6671 - accuracy: 0.5977\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6771\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.677 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6571 - accuracy: 0.6197\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6255 - accuracy: 0.6660\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.666 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6636 - accuracy: 0.6046\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6162 - accuracy: 0.6846\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.685 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6570 - accuracy: 0.6214\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6284 - accuracy: 0.6696\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6551 - accuracy: 0.6201\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6317 - accuracy: 0.6689\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.669 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6580 - accuracy: 0.6109\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6182 - accuracy: 0.6830\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.683 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6549 - accuracy: 0.6203\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6277 - accuracy: 0.6737\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6622 - accuracy: 0.6083\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6476 - accuracy: 0.6280\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.628 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6652 - accuracy: 0.6039\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6343 - accuracy: 0.6697\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.670 total time=   8.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6578 - accuracy: 0.6231\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6253 - accuracy: 0.6683\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6559 - accuracy: 0.6199\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6352 - accuracy: 0.6697\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.670 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6669 - accuracy: 0.5983\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6275 - accuracy: 0.6800\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.680 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6600 - accuracy: 0.6083\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6317 - accuracy: 0.6576\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.658 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6639 - accuracy: 0.5975\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6324 - accuracy: 0.6683\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.668 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6660 - accuracy: 0.5953\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6196 - accuracy: 0.6827\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.683 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6626 - accuracy: 0.6021\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6299 - accuracy: 0.6756\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.676 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6643 - accuracy: 0.6078\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6289 - accuracy: 0.6712\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6664 - accuracy: 0.5959\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6265 - accuracy: 0.6725\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6632 - accuracy: 0.6076\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6255 - accuracy: 0.6784\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.678 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6563 - accuracy: 0.6142\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6502 - accuracy: 0.6250\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.625 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6622 - accuracy: 0.6079\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6176 - accuracy: 0.6862\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.686 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6565 - accuracy: 0.6176\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6317 - accuracy: 0.6767\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.677 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6563 - accuracy: 0.6174\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6307 - accuracy: 0.6670\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.667 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6587 - accuracy: 0.6124\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6201 - accuracy: 0.6766\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.677 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6620 - accuracy: 0.6097\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6304 - accuracy: 0.6642\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.664 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6607 - accuracy: 0.6163\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6491 - accuracy: 0.6269\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.627 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6665 - accuracy: 0.5967\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6195 - accuracy: 0.6796\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.680 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6600 - accuracy: 0.6134\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6295 - accuracy: 0.6761\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.676 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6535 - accuracy: 0.6243\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6267 - accuracy: 0.6637\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.664 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6624 - accuracy: 0.6084\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6235 - accuracy: 0.6839\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.684 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6623 - accuracy: 0.6069\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6356 - accuracy: 0.6579\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.658 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6509 - accuracy: 0.6311\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6241 - accuracy: 0.6706\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.671 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6549 - accuracy: 0.6248\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6182 - accuracy: 0.6756\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.676 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6559 - accuracy: 0.6211\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6356 - accuracy: 0.6673\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6548 - accuracy: 0.6214\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6234 - accuracy: 0.6721\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.672 total time=   7.8s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6652 - accuracy: 0.6053\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6811\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.681 total time=   8.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6604 - accuracy: 0.6119\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6270 - accuracy: 0.6676\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6559 - accuracy: 0.6221\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6297 - accuracy: 0.6738\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6601 - accuracy: 0.6088\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6338 - accuracy: 0.6556\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.656 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6594 - accuracy: 0.6105\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6247 - accuracy: 0.6776\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.678 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6527 - accuracy: 0.6237\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6261 - accuracy: 0.6731\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6581 - accuracy: 0.6196\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6251 - accuracy: 0.6745\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.675 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6584 - accuracy: 0.6126\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6245 - accuracy: 0.6780\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.678 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6583 - accuracy: 0.6152\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6656\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.666 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6618 - accuracy: 0.6111\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6294 - accuracy: 0.6646\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.665 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6641 - accuracy: 0.5997\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6255 - accuracy: 0.6746\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.675 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6587 - accuracy: 0.6128\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6323 - accuracy: 0.6570\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.657 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6611 - accuracy: 0.6101\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6168 - accuracy: 0.6806\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.681 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6546 - accuracy: 0.6229\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6184 - accuracy: 0.6760\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.676 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6561 - accuracy: 0.6219\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6408 - accuracy: 0.6502\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.650 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6595 - accuracy: 0.6118\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6192 - accuracy: 0.6803\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.680 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6591 - accuracy: 0.6127\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6270 - accuracy: 0.6641\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.664 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6555 - accuracy: 0.6263\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6234 - accuracy: 0.6732\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.673 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6585 - accuracy: 0.6204\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6139 - accuracy: 0.6795\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.679 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6618 - accuracy: 0.6126\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6327 - accuracy: 0.6692\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6544 - accuracy: 0.6244\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6322 - accuracy: 0.6529\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.653 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6625 - accuracy: 0.6068\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6232 - accuracy: 0.6683\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.668 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6559 - accuracy: 0.6210\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6266 - accuracy: 0.6715\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6493 - accuracy: 0.6334\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6222 - accuracy: 0.6737\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6589 - accuracy: 0.6158\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6396 - accuracy: 0.6706\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.671 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6520 - accuracy: 0.6291\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6236 - accuracy: 0.6728\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.673 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6645 - accuracy: 0.5999\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6310 - accuracy: 0.6636\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.664 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6657 - accuracy: 0.6023\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6173 - accuracy: 0.6807\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.681 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6689 - accuracy: 0.5958\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6320 - accuracy: 0.6575\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.658 total time=   8.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6584 - accuracy: 0.6157\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6343 - accuracy: 0.6652\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.665 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6647 - accuracy: 0.6010\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6172 - accuracy: 0.6838\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.684 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6619 - accuracy: 0.6034\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6205 - accuracy: 0.6756\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.676 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6555 - accuracy: 0.6239\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6480 - accuracy: 0.6233\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.623 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6639 - accuracy: 0.6032\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6161 - accuracy: 0.6824\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.682 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6520 - accuracy: 0.6293\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6688\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.669 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6610 - accuracy: 0.6160\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6262 - accuracy: 0.6697\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.670 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6677 - accuracy: 0.5973\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6229 - accuracy: 0.6839\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.684 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6590 - accuracy: 0.6129\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6336 - accuracy: 0.6573\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.657 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6554 - accuracy: 0.6212\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6235 - accuracy: 0.6719\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.672 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6582 - accuracy: 0.6162\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6174 - accuracy: 0.6827\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.683 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6565 - accuracy: 0.6220\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6240 - accuracy: 0.6688\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6555 - accuracy: 0.6221\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6318 - accuracy: 0.6616\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.662 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6587 - accuracy: 0.6112\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6339 - accuracy: 0.6610\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.661 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6584 - accuracy: 0.6156\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6352 - accuracy: 0.6566\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.657 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6552 - accuracy: 0.6283\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6465 - accuracy: 0.6356\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.636 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6678 - accuracy: 0.5949\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6240 - accuracy: 0.6738\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.674 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6598 - accuracy: 0.6124\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6223 - accuracy: 0.6734\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.673 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6543 - accuracy: 0.6233\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6270 - accuracy: 0.6745\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.675 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6621 - accuracy: 0.6113\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6190 - accuracy: 0.6849\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.685 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6601 - accuracy: 0.6120\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6219 - accuracy: 0.6720\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.672 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6520 - accuracy: 0.6260\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6225 - accuracy: 0.6758\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.676 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6589 - accuracy: 0.6136\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6158 - accuracy: 0.6776\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.678 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6527 - accuracy: 0.6233\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6310 - accuracy: 0.6689\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.669 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6654 - accuracy: 0.6004\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6310 - accuracy: 0.6570\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.657 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6642 - accuracy: 0.6034\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6205 - accuracy: 0.6769\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.677 total time=   7.8s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6643 - accuracy: 0.5985\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6204 - accuracy: 0.6755\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.675 total time=   8.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6590 - accuracy: 0.6169\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6257 - accuracy: 0.6711\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6633 - accuracy: 0.6067\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6300 - accuracy: 0.6752\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.675 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6554 - accuracy: 0.6225\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6222 - accuracy: 0.6725\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.672 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6602 - accuracy: 0.6140\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6250 - accuracy: 0.6724\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.672 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6600 - accuracy: 0.6122\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6295 - accuracy: 0.6625\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.663 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6557 - accuracy: 0.6205\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6328 - accuracy: 0.6630\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.663 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6558 - accuracy: 0.6226\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6328 - accuracy: 0.6626\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.663 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6630 - accuracy: 0.6067\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6307 - accuracy: 0.6702\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.670 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6581 - accuracy: 0.6155\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6225 - accuracy: 0.6673\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.667 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6551 - accuracy: 0.6183\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6366 - accuracy: 0.6463\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.646 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6619 - accuracy: 0.6076\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6232 - accuracy: 0.6663\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.666 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6631 - accuracy: 0.6029\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6224 - accuracy: 0.6734\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.673 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6547 - accuracy: 0.6216\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6565 - accuracy: 0.6231\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.623 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6566 - accuracy: 0.6190\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6238 - accuracy: 0.6655\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.666 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6542 - accuracy: 0.6245\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6186 - accuracy: 0.6758\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.676 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6582 - accuracy: 0.6135\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6322 - accuracy: 0.6695\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6600 - accuracy: 0.6169\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6296 - accuracy: 0.6762\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.676 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6596 - accuracy: 0.6110\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6264 - accuracy: 0.6709\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6587 - accuracy: 0.6126\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6696 - accuracy: 0.5646\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.565 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6666 - accuracy: 0.5969\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6349 - accuracy: 0.6539\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.654 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6591 - accuracy: 0.6156\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6293 - accuracy: 0.6725\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.672 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6498 - accuracy: 0.6310\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6239 - accuracy: 0.6720\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.672 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6534 - accuracy: 0.6199\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6202 - accuracy: 0.6694\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.669 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6546 - accuracy: 0.6172\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6164 - accuracy: 0.6772\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.677 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6620 - accuracy: 0.6057\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6319 - accuracy: 0.6577\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.658 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6677 - accuracy: 0.5952\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.6795\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.679 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6627 - accuracy: 0.6066\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6333 - accuracy: 0.6730\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.673 total time=   7.8s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6544 - accuracy: 0.6217\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6390 - accuracy: 0.6494\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.649 total time=   8.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6582 - accuracy: 0.6133\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6162 - accuracy: 0.6774\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.677 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6564 - accuracy: 0.6221\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6276 - accuracy: 0.6763\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.676 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6529 - accuracy: 0.6262\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6280 - accuracy: 0.6600\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.660 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6576 - accuracy: 0.6135\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6193 - accuracy: 0.6827\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.683 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6534 - accuracy: 0.6266\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6286 - accuracy: 0.6714\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6664 - accuracy: 0.5921\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6460 - accuracy: 0.6383\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.638 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6629 - accuracy: 0.6063\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6169 - accuracy: 0.6804\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.680 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6564 - accuracy: 0.6173\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6240 - accuracy: 0.6700\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.670 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6553 - accuracy: 0.6224\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6676\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.668 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6650 - accuracy: 0.6023\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6427 - accuracy: 0.6417\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.642 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6565 - accuracy: 0.6189\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6338 - accuracy: 0.6624\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.662 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6499 - accuracy: 0.6264\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6227 - accuracy: 0.6713\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.671 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6551 - accuracy: 0.6223\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6792\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.679 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6519 - accuracy: 0.6300\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6193 - accuracy: 0.6780\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.678 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6540 - accuracy: 0.6263\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6264 - accuracy: 0.6683\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.668 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6638 - accuracy: 0.6121\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6270 - accuracy: 0.6586\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.659 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6622 - accuracy: 0.6101\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6306 - accuracy: 0.6776\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.678 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6519 - accuracy: 0.6297\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6306 - accuracy: 0.6689\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6526 - accuracy: 0.6297\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6306 - accuracy: 0.6643\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.664 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6569 - accuracy: 0.6178\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6235 - accuracy: 0.6724\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.672 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6488 - accuracy: 0.6380\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6285 - accuracy: 0.6704\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.670 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6541 - accuracy: 0.6240\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6178 - accuracy: 0.6823\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.682 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6525 - accuracy: 0.6272\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6209 - accuracy: 0.6764\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.676 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6621 - accuracy: 0.6080\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6314 - accuracy: 0.6641\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.664 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6619 - accuracy: 0.6076\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6257 - accuracy: 0.6599\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.660 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6597 - accuracy: 0.6200\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6235 - accuracy: 0.6733\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.673 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6589 - accuracy: 0.6191\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6298 - accuracy: 0.6766\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.677 total time=   8.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6603 - accuracy: 0.6126\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6237 - accuracy: 0.6668\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.667 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6588 - accuracy: 0.6130\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6291 - accuracy: 0.6737\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6535 - accuracy: 0.6256\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6282 - accuracy: 0.6732\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6591 - accuracy: 0.6163\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6203 - accuracy: 0.6799\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.680 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6575 - accuracy: 0.6169\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6234 - accuracy: 0.6767\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.677 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6548 - accuracy: 0.6237\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6359 - accuracy: 0.6698\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.670 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6659 - accuracy: 0.6008\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6230 - accuracy: 0.6818\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.682 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6623 - accuracy: 0.6039\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6344 - accuracy: 0.6473\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.647 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6541 - accuracy: 0.6230\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6239 - accuracy: 0.6764\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.676 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6581 - accuracy: 0.6208\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6230 - accuracy: 0.6698\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.670 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6567 - accuracy: 0.6188\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6213 - accuracy: 0.6673\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.667 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6589 - accuracy: 0.6142\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6363 - accuracy: 0.6527\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.653 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6562 - accuracy: 0.6211\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6338 - accuracy: 0.6838\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.684 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6540 - accuracy: 0.6237\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6288 - accuracy: 0.6800\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.680 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6594 - accuracy: 0.6136\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6348 - accuracy: 0.6544\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.654 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6578 - accuracy: 0.6169\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6776 - accuracy: 0.6235\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.623 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6556 - accuracy: 0.6184\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6284 - accuracy: 0.6720\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.672 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6529 - accuracy: 0.6269\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6353 - accuracy: 0.6545\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.654 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6587 - accuracy: 0.6197\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6223 - accuracy: 0.6756\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.676 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6563 - accuracy: 0.6182\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6271 - accuracy: 0.6779\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.678 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6491 - accuracy: 0.6364\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.6646\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.665 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6537 - accuracy: 0.6212\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6203 - accuracy: 0.6734\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.673 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6494 - accuracy: 0.6312\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6232 - accuracy: 0.6736\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6642 - accuracy: 0.6074\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6300 - accuracy: 0.6622\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.662 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6551 - accuracy: 0.6330\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6244 - accuracy: 0.6835\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.684 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6561 - accuracy: 0.6214\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6313 - accuracy: 0.6766\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.677 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6563 - accuracy: 0.6221\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6229 - accuracy: 0.6689\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.669 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6645 - accuracy: 0.6052\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6190 - accuracy: 0.6807\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.681 total time=   8.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6566 - accuracy: 0.6144\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6235 - accuracy: 0.6703\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.670 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6516 - accuracy: 0.6269\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6246 - accuracy: 0.6724\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.672 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6592 - accuracy: 0.6141\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6141 - accuracy: 0.6805\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.681 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6561 - accuracy: 0.6192\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6208 - accuracy: 0.6752\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.675 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6514 - accuracy: 0.6337\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6232 - accuracy: 0.6722\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6595 - accuracy: 0.6169\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6301 - accuracy: 0.6684\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.668 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6622 - accuracy: 0.6116\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6254 - accuracy: 0.6709\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6532 - accuracy: 0.6308\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6312 - accuracy: 0.6675\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.667 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6556 - accuracy: 0.6234\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6341 - accuracy: 0.6539\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.654 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6530 - accuracy: 0.6241\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6399 - accuracy: 0.6267\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.627 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6504 - accuracy: 0.6345\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6319 - accuracy: 0.6547\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.655 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6554 - accuracy: 0.6219\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6211 - accuracy: 0.6781\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.678 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6551 - accuracy: 0.6181\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6206 - accuracy: 0.6780\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.678 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6547 - accuracy: 0.6275\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6274 - accuracy: 0.6654\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.665 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6609 - accuracy: 0.6147\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6158 - accuracy: 0.6804\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.680 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6536 - accuracy: 0.6286\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6362 - accuracy: 0.6501\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.650 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6545 - accuracy: 0.6233\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6648 - accuracy: 0.5787\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.579 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6592 - accuracy: 0.6182\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6214 - accuracy: 0.6785\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.679 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6554 - accuracy: 0.6191\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6258 - accuracy: 0.6779\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.678 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6462 - accuracy: 0.6373\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6524 - accuracy: 0.6274\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.627 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6532 - accuracy: 0.6281\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6165 - accuracy: 0.6764\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.676 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6522 - accuracy: 0.6260\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6324 - accuracy: 0.6694\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.669 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6589 - accuracy: 0.6186\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6344 - accuracy: 0.6642\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.664 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6567 - accuracy: 0.6217\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6115 - accuracy: 0.6828\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.683 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6579 - accuracy: 0.6202\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6299 - accuracy: 0.6729\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.673 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6543 - accuracy: 0.6242\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6288 - accuracy: 0.6716\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.672 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6577 - accuracy: 0.6232\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6298 - accuracy: 0.6849\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.685 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6561 - accuracy: 0.6202\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6283 - accuracy: 0.6663\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.666 total time=   8.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6518 - accuracy: 0.6263\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6728\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6573 - accuracy: 0.6187\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6120 - accuracy: 0.6849\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.685 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6508 - accuracy: 0.6280\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6195 - accuracy: 0.6720\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.672 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6505 - accuracy: 0.6365\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6360 - accuracy: 0.6506\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.651 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6562 - accuracy: 0.6247\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6221 - accuracy: 0.6825\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.682 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6593 - accuracy: 0.6169\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6241 - accuracy: 0.6711\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6543 - accuracy: 0.6256\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6693\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6556 - accuracy: 0.6255\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6128 - accuracy: 0.6800\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.680 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6527 - accuracy: 0.6250\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6351 - accuracy: 0.6432\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.643 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6492 - accuracy: 0.6311\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6284 - accuracy: 0.6739\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.674 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6551 - accuracy: 0.6287\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6237 - accuracy: 0.6759\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.676 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6502 - accuracy: 0.6342\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6203 - accuracy: 0.6767\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.677 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6604 - accuracy: 0.6137\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6405 - accuracy: 0.6618\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.662 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6610 - accuracy: 0.6143\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6184 - accuracy: 0.6812\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.681 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6539 - accuracy: 0.6290\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6267 - accuracy: 0.6748\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6500 - accuracy: 0.6352\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.6721\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.672 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6536 - accuracy: 0.6279\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6415 - accuracy: 0.6453\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.645 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6505 - accuracy: 0.6320\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6245 - accuracy: 0.6749\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.675 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6467 - accuracy: 0.6381\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6561\n",
      "[CV 1/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.656 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6520 - accuracy: 0.6275\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6182 - accuracy: 0.6785\n",
      "[CV 2/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.679 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6550 - accuracy: 0.6218\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6336 - accuracy: 0.6776\n",
      "[CV 3/3] END activation=sigmoid, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.678 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6874 - accuracy: 0.5493\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6606 - accuracy: 0.6575\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.657 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6946 - accuracy: 0.5165\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6861 - accuracy: 0.6284\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.628 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6943 - accuracy: 0.5075\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6916 - accuracy: 0.4954\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.495 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6877 - accuracy: 0.5445\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6627 - accuracy: 0.6297\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.630 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6886 - accuracy: 0.5412\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6734 - accuracy: 0.6011\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.601 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6929 - accuracy: 0.5229\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6813 - accuracy: 0.6642\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.664 total time=   8.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6855 - accuracy: 0.5519\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6491 - accuracy: 0.6545\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.654 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6752 - accuracy: 0.5812\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6274 - accuracy: 0.6732\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6890 - accuracy: 0.5341\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6688 - accuracy: 0.6295\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.630 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6930 - accuracy: 0.5255\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6843 - accuracy: 0.5636\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.564 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6916 - accuracy: 0.5289\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6776 - accuracy: 0.6399\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.640 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6915 - accuracy: 0.5273\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6804 - accuracy: 0.6166\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.617 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6893 - accuracy: 0.5382\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6672 - accuracy: 0.6446\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.645 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6908 - accuracy: 0.5312\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6721 - accuracy: 0.6521\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.652 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6914 - accuracy: 0.5317\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6759 - accuracy: 0.6438\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.644 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6921 - accuracy: 0.5193\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6818 - accuracy: 0.5725\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.573 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6845 - accuracy: 0.5587\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6458 - accuracy: 0.6681\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6925 - accuracy: 0.5147\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6818 - accuracy: 0.5727\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.573 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6899 - accuracy: 0.5265\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6742 - accuracy: 0.5619\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.562 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6933 - accuracy: 0.5151\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6885 - accuracy: 0.5447\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.545 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6945 - accuracy: 0.5175\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6794 - accuracy: 0.6335\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.634 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6862 - accuracy: 0.5456\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6551 - accuracy: 0.6444\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.644 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6940 - accuracy: 0.5180\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6807 - accuracy: 0.6305\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.631 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6912 - accuracy: 0.5291\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6772 - accuracy: 0.5878\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.588 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6897 - accuracy: 0.5373\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6656 - accuracy: 0.6416\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.642 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6870 - accuracy: 0.5410\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6527 - accuracy: 0.6519\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.652 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6859 - accuracy: 0.5559\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6556 - accuracy: 0.6476\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.648 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6892 - accuracy: 0.5398\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6714 - accuracy: 0.6328\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.633 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6817 - accuracy: 0.5621\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6505 - accuracy: 0.6504\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.650 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6932 - accuracy: 0.5159\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6869 - accuracy: 0.6067\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.607 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6883 - accuracy: 0.5397\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6610 - accuracy: 0.6508\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.651 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6897 - accuracy: 0.5344\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6648 - accuracy: 0.6651\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.665 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6900 - accuracy: 0.5314\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6705 - accuracy: 0.6305\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.631 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6903 - accuracy: 0.5312\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6704 - accuracy: 0.6262\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.626 total time=   8.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6944 - accuracy: 0.5124\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6867 - accuracy: 0.6222\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.622 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6904 - accuracy: 0.5314\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6719 - accuracy: 0.5884\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.588 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6893 - accuracy: 0.5415\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6639 - accuracy: 0.6616\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.662 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6914 - accuracy: 0.5279\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6739 - accuracy: 0.6387\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.639 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6863 - accuracy: 0.5511\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6539 - accuracy: 0.6491\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.649 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6848 - accuracy: 0.5529\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6513 - accuracy: 0.6466\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.647 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6913 - accuracy: 0.5212\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6729 - accuracy: 0.6430\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.643 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6847 - accuracy: 0.5540\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6477 - accuracy: 0.6601\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.660 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6807 - accuracy: 0.5630\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6468 - accuracy: 0.6458\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.646 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6920 - accuracy: 0.5204\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6764 - accuracy: 0.5571\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.557 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6791 - accuracy: 0.5661\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6378 - accuracy: 0.6552\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.655 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6867 - accuracy: 0.5495\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6583 - accuracy: 0.6324\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.632 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6883 - accuracy: 0.5393\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6603 - accuracy: 0.6525\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.653 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6884 - accuracy: 0.5379\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6593 - accuracy: 0.6450\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.645 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6854 - accuracy: 0.5494\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6513 - accuracy: 0.6345\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.635 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6898 - accuracy: 0.5394\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6669 - accuracy: 0.6448\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.645 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6875 - accuracy: 0.5433\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6687 - accuracy: 0.5740\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.574 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6929 - accuracy: 0.5200\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6815 - accuracy: 0.6189\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.619 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6819 - accuracy: 0.5544\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6308 - accuracy: 0.6721\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.672 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6888 - accuracy: 0.5352\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6606 - accuracy: 0.6378\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.638 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6876 - accuracy: 0.5446\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6643 - accuracy: 0.6320\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.632 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6904 - accuracy: 0.5267\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6639 - accuracy: 0.6418\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.642 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6873 - accuracy: 0.5424\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6547 - accuracy: 0.6635\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.664 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6884 - accuracy: 0.5339\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6670 - accuracy: 0.6403\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.640 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6904 - accuracy: 0.5287\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6737 - accuracy: 0.5833\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.583 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6881 - accuracy: 0.5419\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6619 - accuracy: 0.6236\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.624 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6699 - accuracy: 0.5891\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6270 - accuracy: 0.6660\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.666 total time=   9.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6892 - accuracy: 0.5338\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6612 - accuracy: 0.6320\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.632 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6915 - accuracy: 0.5213\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6766 - accuracy: 0.6165\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.616 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6837 - accuracy: 0.5551\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6474 - accuracy: 0.6501\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.650 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6822 - accuracy: 0.5551\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6391 - accuracy: 0.6616\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.662 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6835 - accuracy: 0.5554\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6451 - accuracy: 0.6660\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.666 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6830 - accuracy: 0.5560\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6452 - accuracy: 0.6618\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.662 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6848 - accuracy: 0.5453\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6425 - accuracy: 0.6630\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.663 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6846 - accuracy: 0.5496\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6506 - accuracy: 0.6300\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.630 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6823 - accuracy: 0.5570\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6408 - accuracy: 0.6512\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.651 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6850 - accuracy: 0.5526\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6551 - accuracy: 0.6231\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.623 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6821 - accuracy: 0.5503\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6443 - accuracy: 0.6393\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.639 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6819 - accuracy: 0.5557\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6402 - accuracy: 0.6544\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.654 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6886 - accuracy: 0.5354\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6584 - accuracy: 0.6496\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.650 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6922 - accuracy: 0.5229\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6821 - accuracy: 0.6079\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.608 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6794 - accuracy: 0.5695\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6451 - accuracy: 0.6531\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.653 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6802 - accuracy: 0.5641\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6435 - accuracy: 0.6512\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.651 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6776 - accuracy: 0.5718\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6299 - accuracy: 0.6711\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6837 - accuracy: 0.5555\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6417 - accuracy: 0.6527\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.653 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6800 - accuracy: 0.5591\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6453 - accuracy: 0.6486\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.649 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6756 - accuracy: 0.5701\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6301 - accuracy: 0.6649\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.665 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6888 - accuracy: 0.5437\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6657 - accuracy: 0.6137\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.614 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6945 - accuracy: 0.5092\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6882 - accuracy: 0.5866\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.587 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6898 - accuracy: 0.5348\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6652 - accuracy: 0.6567\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.657 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6897 - accuracy: 0.5347\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6672 - accuracy: 0.6607\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.661 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6957 - accuracy: 0.5122\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6887 - accuracy: 0.5689\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.569 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6864 - accuracy: 0.5504\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6556 - accuracy: 0.6635\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.663 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6915 - accuracy: 0.5272\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6783 - accuracy: 0.6011\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.601 total time=   7.8s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6937 - accuracy: 0.5119\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6883 - accuracy: 0.5548\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.555 total time=   9.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6898 - accuracy: 0.5349\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6694 - accuracy: 0.6366\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.637 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6911 - accuracy: 0.5297\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6740 - accuracy: 0.6243\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.624 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6885 - accuracy: 0.5437\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6628 - accuracy: 0.6723\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6859 - accuracy: 0.5466\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6559 - accuracy: 0.6350\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.635 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6935 - accuracy: 0.5155\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6811 - accuracy: 0.6150\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.615 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6873 - accuracy: 0.5487\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6686 - accuracy: 0.5971\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.597 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6817 - accuracy: 0.5588\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6468 - accuracy: 0.6471\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.647 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6893 - accuracy: 0.5395\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6648 - accuracy: 0.6308\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.631 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6876 - accuracy: 0.5422\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6637 - accuracy: 0.6210\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.621 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6848 - accuracy: 0.5453\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6593 - accuracy: 0.6291\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.629 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6813 - accuracy: 0.5610\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6424 - accuracy: 0.6623\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.662 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6877 - accuracy: 0.5452\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6520 - accuracy: 0.6771\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6866 - accuracy: 0.5469\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6614 - accuracy: 0.6248\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.625 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6915 - accuracy: 0.5264\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6760 - accuracy: 0.6401\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.640 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6860 - accuracy: 0.5461\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6504 - accuracy: 0.6464\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.646 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6871 - accuracy: 0.5411\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6533 - accuracy: 0.6523\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.652 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6800 - accuracy: 0.5617\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6430 - accuracy: 0.6473\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.647 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6900 - accuracy: 0.5320\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6735 - accuracy: 0.5835\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.583 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6910 - accuracy: 0.5276\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6711 - accuracy: 0.6370\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.637 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6866 - accuracy: 0.5482\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6556 - accuracy: 0.6460\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.646 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6912 - accuracy: 0.5241\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6687 - accuracy: 0.6618\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.662 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6911 - accuracy: 0.5310\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6679 - accuracy: 0.6619\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.662 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6842 - accuracy: 0.5544\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6489 - accuracy: 0.6471\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.647 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6805 - accuracy: 0.5665\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6370 - accuracy: 0.6557\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.656 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6896 - accuracy: 0.5410\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6651 - accuracy: 0.6382\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.638 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6867 - accuracy: 0.5448\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6527 - accuracy: 0.6552\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.655 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6870 - accuracy: 0.5459\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6612 - accuracy: 0.6102\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.610 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6832 - accuracy: 0.5558\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6444 - accuracy: 0.6597\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.660 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6880 - accuracy: 0.5429\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6680 - accuracy: 0.6179\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.618 total time=   8.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6891 - accuracy: 0.5381\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6643 - accuracy: 0.6638\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.664 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6866 - accuracy: 0.5414\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6617 - accuracy: 0.6077\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.608 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6932 - accuracy: 0.5189\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6807 - accuracy: 0.5986\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.599 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6867 - accuracy: 0.5427\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6463 - accuracy: 0.6685\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6882 - accuracy: 0.5406\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6519 - accuracy: 0.6583\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.658 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6808 - accuracy: 0.5642\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6462 - accuracy: 0.6412\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.641 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6852 - accuracy: 0.5502\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6468 - accuracy: 0.6581\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.658 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6882 - accuracy: 0.5397\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6567 - accuracy: 0.6481\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.648 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6866 - accuracy: 0.5430\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6583 - accuracy: 0.6147\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.615 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6792 - accuracy: 0.5670\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6337 - accuracy: 0.6684\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.668 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6907 - accuracy: 0.5254\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6673 - accuracy: 0.6440\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.644 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6739 - accuracy: 0.5807\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6321 - accuracy: 0.6623\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.662 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6810 - accuracy: 0.5571\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6334 - accuracy: 0.6736\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6876 - accuracy: 0.5447\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6628 - accuracy: 0.5942\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.594 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6840 - accuracy: 0.5512\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6368 - accuracy: 0.6624\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.662 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6746 - accuracy: 0.5785\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6260 - accuracy: 0.6803\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.680 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6903 - accuracy: 0.5327\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6684 - accuracy: 0.6159\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.616 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6816 - accuracy: 0.5596\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6409 - accuracy: 0.6538\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.654 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6816 - accuracy: 0.5598\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6393 - accuracy: 0.6677\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6786 - accuracy: 0.5719\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6363 - accuracy: 0.6605\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.661 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6888 - accuracy: 0.5403\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6624 - accuracy: 0.6372\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.637 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6682 - accuracy: 0.5982\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6264 - accuracy: 0.6753\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.675 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6839 - accuracy: 0.5510\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6446 - accuracy: 0.6743\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6770 - accuracy: 0.5729\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6413 - accuracy: 0.6439\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.644 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6858 - accuracy: 0.5381\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6539 - accuracy: 0.6448\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.645 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6877 - accuracy: 0.5375\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6556 - accuracy: 0.6355\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.635 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6812 - accuracy: 0.5593\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6413 - accuracy: 0.6618\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.662 total time=   8.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6791 - accuracy: 0.5682\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6312 - accuracy: 0.6677\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.668 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6757 - accuracy: 0.5822\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6335 - accuracy: 0.6681\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6741 - accuracy: 0.5799\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6515\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.651 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6709 - accuracy: 0.5919\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6221 - accuracy: 0.6793\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.679 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6812 - accuracy: 0.5554\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6502 - accuracy: 0.6271\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.627 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6841 - accuracy: 0.5521\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6438 - accuracy: 0.6616\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.662 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6815 - accuracy: 0.5587\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6283 - accuracy: 0.6729\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.673 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6817 - accuracy: 0.5646\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6466 - accuracy: 0.6470\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.647 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6772 - accuracy: 0.5694\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6371 - accuracy: 0.6540\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.654 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6937 - accuracy: 0.5031\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6860 - accuracy: 0.5941\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.594 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6845 - accuracy: 0.5474\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6450 - accuracy: 0.6618\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.662 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6734 - accuracy: 0.5803\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6356 - accuracy: 0.6554\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.655 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6852 - accuracy: 0.5417\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6447 - accuracy: 0.6430\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.643 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6748 - accuracy: 0.5737\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6406 - accuracy: 0.6492\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.649 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6777 - accuracy: 0.5680\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6369 - accuracy: 0.6545\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.654 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6826 - accuracy: 0.5594\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6500 - accuracy: 0.6373\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.637 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6715 - accuracy: 0.5837\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6287 - accuracy: 0.6644\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.664 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6799 - accuracy: 0.5706\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6455 - accuracy: 0.6535\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.654 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6852 - accuracy: 0.5532\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6496 - accuracy: 0.6496\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.650 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6907 - accuracy: 0.5348\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6649 - accuracy: 0.6650\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.665 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6880 - accuracy: 0.5471\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6557 - accuracy: 0.6456\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.646 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6826 - accuracy: 0.5600\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6371 - accuracy: 0.6698\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.670 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6808 - accuracy: 0.5692\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6431 - accuracy: 0.6525\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.653 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6844 - accuracy: 0.5464\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6440 - accuracy: 0.6497\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.650 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6910 - accuracy: 0.5250\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6712 - accuracy: 0.6674\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.667 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6892 - accuracy: 0.5350\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6593 - accuracy: 0.6548\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.655 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6872 - accuracy: 0.5442\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6616 - accuracy: 0.6068\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.607 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6895 - accuracy: 0.5373\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6605 - accuracy: 0.6206\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.621 total time=   9.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6876 - accuracy: 0.5411\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6484 - accuracy: 0.6614\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.661 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6867 - accuracy: 0.5498\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6589 - accuracy: 0.6247\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.625 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6882 - accuracy: 0.5429\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6605 - accuracy: 0.6176\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.618 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6888 - accuracy: 0.5406\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6572 - accuracy: 0.6503\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.650 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6759 - accuracy: 0.5764\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6389 - accuracy: 0.6534\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.653 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6802 - accuracy: 0.5641\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6393 - accuracy: 0.6485\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.648 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6909 - accuracy: 0.5280\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6725 - accuracy: 0.6220\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.622 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6822 - accuracy: 0.5573\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6403 - accuracy: 0.6637\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.664 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6779 - accuracy: 0.5728\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6318 - accuracy: 0.6703\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.670 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6886 - accuracy: 0.5377\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6595 - accuracy: 0.6482\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.648 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6916 - accuracy: 0.5270\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6758 - accuracy: 0.6068\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.607 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6838 - accuracy: 0.5540\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6416 - accuracy: 0.6571\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.657 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6857 - accuracy: 0.5489\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6480 - accuracy: 0.6332\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.633 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6826 - accuracy: 0.5611\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6467 - accuracy: 0.6324\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.632 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6795 - accuracy: 0.5611\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6396 - accuracy: 0.6751\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.675 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6711 - accuracy: 0.5876\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6310 - accuracy: 0.6575\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.657 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6912 - accuracy: 0.5281\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6764 - accuracy: 0.6166\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.617 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6845 - accuracy: 0.5426\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6509 - accuracy: 0.6735\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.673 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6813 - accuracy: 0.5644\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6369 - accuracy: 0.6615\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.662 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6887 - accuracy: 0.5398\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6652 - accuracy: 0.6259\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.626 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6816 - accuracy: 0.5572\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6549 - accuracy: 0.6195\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.620 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6781 - accuracy: 0.5701\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6364 - accuracy: 0.6526\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.653 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6815 - accuracy: 0.5626\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6481 - accuracy: 0.6327\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.633 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6738 - accuracy: 0.5815\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6293 - accuracy: 0.6708\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6837 - accuracy: 0.5554\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6425 - accuracy: 0.6619\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.662 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6840 - accuracy: 0.5528\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6476 - accuracy: 0.6387\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.639 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6875 - accuracy: 0.5419\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6529 - accuracy: 0.6529\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.653 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6881 - accuracy: 0.5427\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6578 - accuracy: 0.6350\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.635 total time=   9.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6839 - accuracy: 0.5543\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6454 - accuracy: 0.6509\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.651 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6855 - accuracy: 0.5485\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6549 - accuracy: 0.6090\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.609 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6746 - accuracy: 0.5798\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6362 - accuracy: 0.6606\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.661 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6884 - accuracy: 0.5384\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6543 - accuracy: 0.6435\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.644 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6871 - accuracy: 0.5435\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6542 - accuracy: 0.6277\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.628 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6841 - accuracy: 0.5505\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6451 - accuracy: 0.6440\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.644 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6823 - accuracy: 0.5564\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6434 - accuracy: 0.6436\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.644 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6866 - accuracy: 0.5438\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6451 - accuracy: 0.6656\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.666 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6814 - accuracy: 0.5629\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6363 - accuracy: 0.6704\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.670 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6687 - accuracy: 0.5920\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6284 - accuracy: 0.6699\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6736 - accuracy: 0.5788\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6394 - accuracy: 0.6465\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.647 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6835 - accuracy: 0.5559\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6462 - accuracy: 0.6491\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.649 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6788 - accuracy: 0.5666\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6324 - accuracy: 0.6638\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.664 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6788 - accuracy: 0.5674\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6372 - accuracy: 0.6637\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.664 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6884 - accuracy: 0.5366\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6532 - accuracy: 0.6354\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.635 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6843 - accuracy: 0.5560\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6435 - accuracy: 0.6682\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.5690\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6456 - accuracy: 0.6493\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.649 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6908 - accuracy: 0.5279\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6677 - accuracy: 0.6507\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.651 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6788 - accuracy: 0.5713\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6461 - accuracy: 0.6620\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.662 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6855 - accuracy: 0.5456\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6488 - accuracy: 0.6511\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.651 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6827 - accuracy: 0.5541\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6426 - accuracy: 0.6582\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.658 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6725 - accuracy: 0.5853\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6326 - accuracy: 0.6613\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.661 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6786 - accuracy: 0.5724\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6344 - accuracy: 0.6607\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.661 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6878 - accuracy: 0.5408\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6533 - accuracy: 0.6568\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.657 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6787 - accuracy: 0.5693\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6397 - accuracy: 0.6524\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.652 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6856 - accuracy: 0.5473\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6515 - accuracy: 0.6627\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.663 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6746 - accuracy: 0.5809\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6614\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.661 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6813 - accuracy: 0.5586\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6441 - accuracy: 0.6422\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.642 total time=   8.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6817 - accuracy: 0.5614\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6287 - accuracy: 0.6709\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6798 - accuracy: 0.5648\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6419 - accuracy: 0.6530\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.653 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6743 - accuracy: 0.5766\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6410 - accuracy: 0.6396\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.640 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6795 - accuracy: 0.5578\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6478 - accuracy: 0.6224\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.622 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6790 - accuracy: 0.5661\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6427 - accuracy: 0.6435\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.643 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6813 - accuracy: 0.5624\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6457 - accuracy: 0.6494\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.649 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6675 - accuracy: 0.5993\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6270 - accuracy: 0.6791\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.679 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6857 - accuracy: 0.5443\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6556 - accuracy: 0.6375\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.637 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6759 - accuracy: 0.5712\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6455 - accuracy: 0.6386\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.639 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6747 - accuracy: 0.5780\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6306 - accuracy: 0.6706\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6828 - accuracy: 0.5545\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6378 - accuracy: 0.6598\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.660 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6632 - accuracy: 0.5982\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6356 - accuracy: 0.6526\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.653 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6748 - accuracy: 0.5715\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6227 - accuracy: 0.6746\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.675 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6787 - accuracy: 0.5661\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6405 - accuracy: 0.6539\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.654 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6926 - accuracy: 0.5263\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6820 - accuracy: 0.6186\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.619 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6918 - accuracy: 0.5239\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6768 - accuracy: 0.6617\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.662 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6878 - accuracy: 0.5454\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6599 - accuracy: 0.6572\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.657 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6940 - accuracy: 0.5157\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6780 - accuracy: 0.6116\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.612 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6935 - accuracy: 0.5167\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6838 - accuracy: 0.6416\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.642 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6899 - accuracy: 0.5388\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6680 - accuracy: 0.6501\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.650 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6873 - accuracy: 0.5489\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6594 - accuracy: 0.6479\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.648 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6882 - accuracy: 0.5455\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6570 - accuracy: 0.6658\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.666 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6914 - accuracy: 0.5297\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6735 - accuracy: 0.6452\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.645 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6933 - accuracy: 0.5267\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6807 - accuracy: 0.6136\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.614 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6916 - accuracy: 0.5248\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6702 - accuracy: 0.6522\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.652 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6879 - accuracy: 0.5332\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6625 - accuracy: 0.6375\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.637 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6873 - accuracy: 0.5431\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6561 - accuracy: 0.6600\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.660 total time=   7.9s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6893 - accuracy: 0.5407\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6627 - accuracy: 0.6544\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.654 total time=   9.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6904 - accuracy: 0.5389\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6699 - accuracy: 0.6556\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.656 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6878 - accuracy: 0.5405\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6515 - accuracy: 0.6457\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.646 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6837 - accuracy: 0.5558\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6373 - accuracy: 0.6683\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6893 - accuracy: 0.5404\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6588 - accuracy: 0.6487\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.649 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6865 - accuracy: 0.5500\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6588 - accuracy: 0.6369\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.637 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6869 - accuracy: 0.5419\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6517 - accuracy: 0.6751\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6864 - accuracy: 0.5481\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6512 - accuracy: 0.6614\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.661 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6873 - accuracy: 0.5364\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6522 - accuracy: 0.6471\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.647 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6886 - accuracy: 0.5361\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6532 - accuracy: 0.6754\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.675 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6833 - accuracy: 0.5522\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6413 - accuracy: 0.6576\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.658 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6870 - accuracy: 0.5461\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6476 - accuracy: 0.6643\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.664 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6854 - accuracy: 0.5458\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6473 - accuracy: 0.6317\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.632 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6832 - accuracy: 0.5559\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6420 - accuracy: 0.6633\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.663 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6876 - accuracy: 0.5429\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6567 - accuracy: 0.6673\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.667 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6896 - accuracy: 0.5316\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6660 - accuracy: 0.6489\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.649 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6844 - accuracy: 0.5558\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6460 - accuracy: 0.6628\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.663 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6824 - accuracy: 0.5641\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6467 - accuracy: 0.6420\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.642 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6894 - accuracy: 0.5333\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6606 - accuracy: 0.6758\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.676 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6861 - accuracy: 0.5499\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6724\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.672 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6911 - accuracy: 0.5272\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6733 - accuracy: 0.5963\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.596 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6849 - accuracy: 0.5516\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6458 - accuracy: 0.6534\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.653 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6917 - accuracy: 0.5222\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6759 - accuracy: 0.6515\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.652 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6771 - accuracy: 0.5766\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6369 - accuracy: 0.6622\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.662 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6805 - accuracy: 0.5663\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6475 - accuracy: 0.6312\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.631 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6853 - accuracy: 0.5487\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6473 - accuracy: 0.6688\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.669 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6877 - accuracy: 0.5412\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6574 - accuracy: 0.6294\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.629 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6930 - accuracy: 0.5202\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6823 - accuracy: 0.5823\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.582 total time=   8.0s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6875 - accuracy: 0.5400\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6520 - accuracy: 0.6688\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.669 total time=   9.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6862 - accuracy: 0.5491\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6464 - accuracy: 0.6431\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.643 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6912 - accuracy: 0.5252\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6681 - accuracy: 0.6483\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.648 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6816 - accuracy: 0.5586\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6422 - accuracy: 0.6440\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.644 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6841 - accuracy: 0.5527\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6451 - accuracy: 0.6557\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.656 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6865 - accuracy: 0.5450\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6472 - accuracy: 0.6674\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.667 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6809 - accuracy: 0.5587\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6366 - accuracy: 0.6688\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6889 - accuracy: 0.5431\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6579 - accuracy: 0.6280\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.628 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6857 - accuracy: 0.5454\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6420 - accuracy: 0.6656\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.666 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6862 - accuracy: 0.5405\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6564 - accuracy: 0.6100\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.610 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6814 - accuracy: 0.5614\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6353 - accuracy: 0.6598\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.660 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6818 - accuracy: 0.5558\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6467 - accuracy: 0.6320\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.632 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6844 - accuracy: 0.5511\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6396 - accuracy: 0.6579\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.658 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6842 - accuracy: 0.5540\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6445 - accuracy: 0.6648\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.665 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6817 - accuracy: 0.5658\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6332 - accuracy: 0.6748\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.675 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6830 - accuracy: 0.5530\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6491 - accuracy: 0.6391\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.639 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6848 - accuracy: 0.5569\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6480 - accuracy: 0.6525\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.653 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6909 - accuracy: 0.5270\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6688 - accuracy: 0.6648\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.665 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6765 - accuracy: 0.5783\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6310 - accuracy: 0.6679\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6837 - accuracy: 0.5570\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6410 - accuracy: 0.6585\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.658 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6840 - accuracy: 0.5494\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6428 - accuracy: 0.6587\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.659 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6778 - accuracy: 0.5707\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6361 - accuracy: 0.6619\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.662 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6817 - accuracy: 0.5597\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6363 - accuracy: 0.6650\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.665 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6908 - accuracy: 0.5308\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6600 - accuracy: 0.6652\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.665 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6755 - accuracy: 0.5756\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6383 - accuracy: 0.6597\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.660 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6792 - accuracy: 0.5611\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6379 - accuracy: 0.6609\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.661 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6734 - accuracy: 0.5824\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6266 - accuracy: 0.6718\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.672 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6845 - accuracy: 0.5472\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6452 - accuracy: 0.6502\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.650 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6794 - accuracy: 0.5641\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6393 - accuracy: 0.6525\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.653 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6859 - accuracy: 0.5475\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6411 - accuracy: 0.6609\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.661 total time=   9.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6861 - accuracy: 0.5405\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6426 - accuracy: 0.6719\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.672 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6837 - accuracy: 0.5499\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6446 - accuracy: 0.6395\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.639 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6774 - accuracy: 0.5719\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6274 - accuracy: 0.6771\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6699 - accuracy: 0.5885\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6295 - accuracy: 0.6621\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.662 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6756 - accuracy: 0.5739\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6413 - accuracy: 0.6447\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.645 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6822 - accuracy: 0.5541\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6478 - accuracy: 0.6237\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.624 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6740 - accuracy: 0.5756\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6289 - accuracy: 0.6651\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.665 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6870 - accuracy: 0.5395\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6526 - accuracy: 0.6160\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.616 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6854 - accuracy: 0.5425\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6443 - accuracy: 0.6494\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.649 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6762 - accuracy: 0.5655\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6431 - accuracy: 0.6386\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.639 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6925 - accuracy: 0.5272\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6766 - accuracy: 0.6545\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.654 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6868 - accuracy: 0.5475\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6593 - accuracy: 0.6254\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.625 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6867 - accuracy: 0.5526\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6583 - accuracy: 0.6597\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.660 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6875 - accuracy: 0.5418\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6558 - accuracy: 0.6434\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.643 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6925 - accuracy: 0.5239\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6752 - accuracy: 0.6701\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.670 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6806 - accuracy: 0.5684\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6392 - accuracy: 0.6543\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.654 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6818 - accuracy: 0.5586\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6399 - accuracy: 0.6572\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.657 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6928 - accuracy: 0.5199\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6806 - accuracy: 0.6418\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.642 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6928 - accuracy: 0.5212\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6810 - accuracy: 0.6449\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.645 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6870 - accuracy: 0.5472\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6552 - accuracy: 0.6423\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.642 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6889 - accuracy: 0.5396\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6630 - accuracy: 0.6445\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.644 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6890 - accuracy: 0.5390\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6624 - accuracy: 0.6441\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.644 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6927 - accuracy: 0.5218\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6789 - accuracy: 0.5977\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.598 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6890 - accuracy: 0.5407\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6551 - accuracy: 0.6604\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.660 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6857 - accuracy: 0.5530\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6473 - accuracy: 0.6577\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.658 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6804 - accuracy: 0.5647\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6334 - accuracy: 0.6656\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.666 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6796 - accuracy: 0.5655\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6275 - accuracy: 0.6673\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.667 total time=   9.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6909 - accuracy: 0.5315\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6652 - accuracy: 0.6256\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.626 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6874 - accuracy: 0.5450\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6572 - accuracy: 0.6335\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.633 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6925 - accuracy: 0.5198\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6771 - accuracy: 0.5876\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.588 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6845 - accuracy: 0.5523\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6455 - accuracy: 0.6555\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.656 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6860 - accuracy: 0.5453\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6458 - accuracy: 0.6595\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.660 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6853 - accuracy: 0.5507\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6425 - accuracy: 0.6486\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.649 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6782 - accuracy: 0.5667\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6410 - accuracy: 0.6434\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.643 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6707 - accuracy: 0.5890\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6316 - accuracy: 0.6615\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.662 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6805 - accuracy: 0.5631\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6312 - accuracy: 0.6698\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.670 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6912 - accuracy: 0.5276\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6602 - accuracy: 0.6597\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.660 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6903 - accuracy: 0.5322\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6657 - accuracy: 0.6409\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.641 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6924 - accuracy: 0.5250\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6710 - accuracy: 0.6441\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.644 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6858 - accuracy: 0.5464\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6507 - accuracy: 0.6449\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.645 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6758 - accuracy: 0.5763\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6358 - accuracy: 0.6631\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.663 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6840 - accuracy: 0.5500\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6498 - accuracy: 0.6312\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.631 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6842 - accuracy: 0.5482\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6515 - accuracy: 0.6327\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.633 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6881 - accuracy: 0.5440\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6587 - accuracy: 0.6199\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.620 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6803 - accuracy: 0.5611\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6323 - accuracy: 0.6670\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.667 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6837 - accuracy: 0.5548\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6412 - accuracy: 0.6701\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.670 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6815 - accuracy: 0.5573\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6425 - accuracy: 0.6541\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.654 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6915 - accuracy: 0.5250\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6742 - accuracy: 0.5817\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.582 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6845 - accuracy: 0.5535\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6468 - accuracy: 0.6707\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6794 - accuracy: 0.5698\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6471 - accuracy: 0.6283\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.628 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6857 - accuracy: 0.5454\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6440 - accuracy: 0.6591\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.659 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6846 - accuracy: 0.5504\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6468 - accuracy: 0.6559\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.656 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6738 - accuracy: 0.5804\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6336 - accuracy: 0.6536\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.654 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6822 - accuracy: 0.5555\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6341 - accuracy: 0.6697\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.670 total time=   7.9s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6842 - accuracy: 0.5543\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6460 - accuracy: 0.6426\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.643 total time=   9.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6873 - accuracy: 0.5424\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6543 - accuracy: 0.6405\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.641 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6830 - accuracy: 0.5523\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6425 - accuracy: 0.6608\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.661 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6785 - accuracy: 0.5663\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6352 - accuracy: 0.6588\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.659 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6823 - accuracy: 0.5621\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6428 - accuracy: 0.6536\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.654 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6772 - accuracy: 0.5743\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6264 - accuracy: 0.6746\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.675 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6795 - accuracy: 0.5656\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6377 - accuracy: 0.6569\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.657 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6666 - accuracy: 0.5988\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6334 - accuracy: 0.6555\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.656 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6701 - accuracy: 0.5860\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6280 - accuracy: 0.6585\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.658 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6782 - accuracy: 0.5701\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6304 - accuracy: 0.6643\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.664 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6858 - accuracy: 0.5490\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6528 - accuracy: 0.6290\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.629 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6854 - accuracy: 0.5518\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6499 - accuracy: 0.6426\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.643 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6780 - accuracy: 0.5669\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6286 - accuracy: 0.6720\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.672 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6787 - accuracy: 0.5677\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6470 - accuracy: 0.6335\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.633 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6816 - accuracy: 0.5596\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6341 - accuracy: 0.6652\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.665 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6799 - accuracy: 0.5648\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6315 - accuracy: 0.6742\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6757 - accuracy: 0.5795\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6321 - accuracy: 0.6621\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.662 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6864 - accuracy: 0.5463\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6523 - accuracy: 0.6297\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.630 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6702 - accuracy: 0.5864\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6287 - accuracy: 0.6666\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.667 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6847 - accuracy: 0.5509\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6545 - accuracy: 0.6227\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.623 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6801 - accuracy: 0.5642\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6383 - accuracy: 0.6715\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6748 - accuracy: 0.5771\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6323 - accuracy: 0.6732\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.673 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6801 - accuracy: 0.5642\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6343 - accuracy: 0.6635\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.663 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6805 - accuracy: 0.5668\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6317 - accuracy: 0.6793\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.679 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6893 - accuracy: 0.5364\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6594 - accuracy: 0.6398\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.640 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6658 - accuracy: 0.5952\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6377 - accuracy: 0.6486\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.649 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6759 - accuracy: 0.5731\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6356 - accuracy: 0.6527\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.653 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6832 - accuracy: 0.5544\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6407 - accuracy: 0.6504\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.650 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6749 - accuracy: 0.5751\n",
      "354/354 [==============================] - 3s 4ms/step - loss: 0.6377 - accuracy: 0.6588\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.659 total time=   9.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6749 - accuracy: 0.5760\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6252 - accuracy: 0.6724\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.672 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6766 - accuracy: 0.5712\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6296 - accuracy: 0.6641\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.664 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6824 - accuracy: 0.5608\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6413 - accuracy: 0.6559\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.656 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6770 - accuracy: 0.5658\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6270 - accuracy: 0.6716\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.672 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6776 - accuracy: 0.5697\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6360 - accuracy: 0.6592\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.659 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6757 - accuracy: 0.5725\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6474\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.647 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6860 - accuracy: 0.5426\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6408 - accuracy: 0.6617\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.662 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6761 - accuracy: 0.5694\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6275 - accuracy: 0.6762\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.676 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6872 - accuracy: 0.5485\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6565 - accuracy: 0.6654\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.665 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6906 - accuracy: 0.5373\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6624 - accuracy: 0.6554\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.655 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6803 - accuracy: 0.5658\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6390 - accuracy: 0.6616\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.662 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6872 - accuracy: 0.5429\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6555 - accuracy: 0.6349\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.635 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6782 - accuracy: 0.5732\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6266 - accuracy: 0.6859\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.686 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6845 - accuracy: 0.5557\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6399 - accuracy: 0.6674\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.667 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6843 - accuracy: 0.5536\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6493 - accuracy: 0.6350\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.635 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6855 - accuracy: 0.5445\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6469 - accuracy: 0.6601\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.660 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6825 - accuracy: 0.5562\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6407 - accuracy: 0.6451\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.645 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6789 - accuracy: 0.5638\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6352 - accuracy: 0.6622\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.662 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6866 - accuracy: 0.5442\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6496 - accuracy: 0.6577\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.658 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6900 - accuracy: 0.5320\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6623 - accuracy: 0.6575\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.657 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6835 - accuracy: 0.5547\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6415 - accuracy: 0.6623\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.662 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6882 - accuracy: 0.5474\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6651 - accuracy: 0.5894\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.589 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6839 - accuracy: 0.5560\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6348 - accuracy: 0.6700\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.670 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6817 - accuracy: 0.5649\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6402 - accuracy: 0.6509\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.651 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6821 - accuracy: 0.5612\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6311 - accuracy: 0.6688\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.669 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6745 - accuracy: 0.5783\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6270 - accuracy: 0.6670\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.667 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6822 - accuracy: 0.5571\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6405 - accuracy: 0.6600\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.660 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6836 - accuracy: 0.5520\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6390 - accuracy: 0.6673\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.667 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6841 - accuracy: 0.5534\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6401 - accuracy: 0.6632\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.663 total time=   9.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6891 - accuracy: 0.5364\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6579 - accuracy: 0.6581\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.658 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6760 - accuracy: 0.5750\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6708\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6736 - accuracy: 0.5815\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6315 - accuracy: 0.6622\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.662 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6826 - accuracy: 0.5635\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6468 - accuracy: 0.6319\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.632 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6792 - accuracy: 0.5626\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6252 - accuracy: 0.6777\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.678 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6818 - accuracy: 0.5531\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6370 - accuracy: 0.6663\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.666 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6715 - accuracy: 0.5926\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6372 - accuracy: 0.6565\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.657 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6792 - accuracy: 0.5681\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6427 - accuracy: 0.6434\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.643 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6880 - accuracy: 0.5427\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6561 - accuracy: 0.6681\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6864 - accuracy: 0.5484\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6585 - accuracy: 0.6135\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.613 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6748 - accuracy: 0.5790\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6217 - accuracy: 0.6805\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.681 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6826 - accuracy: 0.5538\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6465 - accuracy: 0.6358\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.636 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6856 - accuracy: 0.5511\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6463 - accuracy: 0.6580\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.658 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6831 - accuracy: 0.5573\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6315 - accuracy: 0.6746\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.675 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6770 - accuracy: 0.5722\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6376 - accuracy: 0.6673\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.667 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6838 - accuracy: 0.5571\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6439 - accuracy: 0.6494\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.649 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6752 - accuracy: 0.5767\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6313 - accuracy: 0.6706\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6782 - accuracy: 0.5659\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6290 - accuracy: 0.6692\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.669 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6780 - accuracy: 0.5721\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6308 - accuracy: 0.6593\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.659 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6739 - accuracy: 0.5747\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6239 - accuracy: 0.6730\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.673 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6724 - accuracy: 0.5829\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6254 - accuracy: 0.6754\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.675 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6749 - accuracy: 0.5778\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6477 - accuracy: 0.6245\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.625 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6771 - accuracy: 0.5710\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6267 - accuracy: 0.6728\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.673 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6694 - accuracy: 0.5937\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6274 - accuracy: 0.6658\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.666 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6755 - accuracy: 0.5710\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6361 - accuracy: 0.6562\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.656 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6801 - accuracy: 0.5619\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6266 - accuracy: 0.6775\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.678 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6766 - accuracy: 0.5710\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6310 - accuracy: 0.6684\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6684 - accuracy: 0.5925\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6282 - accuracy: 0.6637\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.664 total time=   8.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6691 - accuracy: 0.5903\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6223 - accuracy: 0.6779\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.678 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6768 - accuracy: 0.5688\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6230 - accuracy: 0.6736\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6708 - accuracy: 0.5854\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6311 - accuracy: 0.6570\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.657 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6764 - accuracy: 0.5691\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6242 - accuracy: 0.6776\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.678 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6735 - accuracy: 0.5753\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6242 - accuracy: 0.6698\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.670 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6714 - accuracy: 0.5807\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6276 - accuracy: 0.6643\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.664 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6723 - accuracy: 0.5828\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6314 - accuracy: 0.6605\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.660 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6793 - accuracy: 0.5651\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6471 - accuracy: 0.6390\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.639 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6813 - accuracy: 0.5567\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6429 - accuracy: 0.6512\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.651 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6814 - accuracy: 0.5606\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6398 - accuracy: 0.6678\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6799 - accuracy: 0.5634\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6445 - accuracy: 0.6470\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.647 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6778 - accuracy: 0.5700\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6343 - accuracy: 0.6576\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.658 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6728 - accuracy: 0.5890\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6281 - accuracy: 0.6701\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.670 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6826 - accuracy: 0.5541\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6370 - accuracy: 0.6734\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6821 - accuracy: 0.5558\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6415 - accuracy: 0.6526\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.653 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6755 - accuracy: 0.5731\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6243 - accuracy: 0.6784\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.678 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6675 - accuracy: 0.5950\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6267 - accuracy: 0.6723\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6719 - accuracy: 0.5784\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6266 - accuracy: 0.6639\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.664 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6753 - accuracy: 0.5774\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6195 - accuracy: 0.6814\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.681 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6747 - accuracy: 0.5764\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6253 - accuracy: 0.6735\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.674 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6637 - accuracy: 0.6020\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6265 - accuracy: 0.6675\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6810 - accuracy: 0.5582\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6330 - accuracy: 0.6719\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.672 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6779 - accuracy: 0.5658\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6339 - accuracy: 0.6654\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.665 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6670 - accuracy: 0.5940\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6280 - accuracy: 0.6624\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.662 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6764 - accuracy: 0.5713\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6249 - accuracy: 0.6659\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.666 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6747 - accuracy: 0.5709\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6336 - accuracy: 0.6617\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.662 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6656 - accuracy: 0.5952\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6278 - accuracy: 0.6652\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.665 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6771 - accuracy: 0.5696\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6399 - accuracy: 0.6404\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.640 total time=   8.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6728 - accuracy: 0.5839\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6293 - accuracy: 0.6637\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.664 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6712 - accuracy: 0.5796\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6255 - accuracy: 0.6665\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.666 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6818 - accuracy: 0.5558\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6364 - accuracy: 0.6630\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.663 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6758 - accuracy: 0.5692\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6658\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.666 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6916 - accuracy: 0.5330\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6747 - accuracy: 0.5737\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.574 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6923 - accuracy: 0.5221\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6771 - accuracy: 0.6589\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.659 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6906 - accuracy: 0.5281\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6620 - accuracy: 0.6373\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.637 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6902 - accuracy: 0.5276\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6641 - accuracy: 0.6379\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.638 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6873 - accuracy: 0.5448\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6534 - accuracy: 0.6770\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.677 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6846 - accuracy: 0.5566\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6466 - accuracy: 0.6682\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.668 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6897 - accuracy: 0.5372\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6653 - accuracy: 0.6335\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.634 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6915 - accuracy: 0.5169\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6740 - accuracy: 0.6710\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.671 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6795 - accuracy: 0.5643\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6417 - accuracy: 0.6442\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.644 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6896 - accuracy: 0.5424\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6621 - accuracy: 0.6515\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.651 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6894 - accuracy: 0.5331\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6563 - accuracy: 0.6577\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.658 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6884 - accuracy: 0.5365\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6565 - accuracy: 0.6559\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.656 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6870 - accuracy: 0.5455\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6522 - accuracy: 0.6592\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.659 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6880 - accuracy: 0.5429\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6527 - accuracy: 0.6706\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.671 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6902 - accuracy: 0.5327\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6716 - accuracy: 0.6370\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.637 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6879 - accuracy: 0.5379\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6542 - accuracy: 0.6483\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.648 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6877 - accuracy: 0.5452\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6530 - accuracy: 0.6246\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.625 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6844 - accuracy: 0.5435\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6406 - accuracy: 0.6585\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.659 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6843 - accuracy: 0.5581\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6479 - accuracy: 0.6569\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.657 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6863 - accuracy: 0.5439\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6543 - accuracy: 0.6328\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.633 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6860 - accuracy: 0.5504\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6513 - accuracy: 0.6585\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.659 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6879 - accuracy: 0.5363\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6511 - accuracy: 0.6499\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.650 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6822 - accuracy: 0.5612\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6422 - accuracy: 0.6514\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.651 total time=   8.1s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6747 - accuracy: 0.5788\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6359 - accuracy: 0.6509\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.651 total time=   9.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.5659\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6343 - accuracy: 0.6633\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.663 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6766 - accuracy: 0.5717\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6188 - accuracy: 0.6787\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.679 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6828 - accuracy: 0.5554\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6346 - accuracy: 0.6658\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.666 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6831 - accuracy: 0.5544\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6464 - accuracy: 0.6579\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.658 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6845 - accuracy: 0.5542\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6385 - accuracy: 0.6698\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.670 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6932 - accuracy: 0.5193\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6841 - accuracy: 0.6241\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.624 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6881 - accuracy: 0.5418\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6612 - accuracy: 0.6062\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.606 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6821 - accuracy: 0.5589\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6378 - accuracy: 0.6583\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.658 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6830 - accuracy: 0.5580\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6513 - accuracy: 0.6209\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.621 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6800 - accuracy: 0.5631\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6366 - accuracy: 0.6562\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.656 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6855 - accuracy: 0.5519\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6353 - accuracy: 0.6810\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.681 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6846 - accuracy: 0.5452\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6400 - accuracy: 0.6756\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.676 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6765 - accuracy: 0.5725\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6376 - accuracy: 0.6609\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.661 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6842 - accuracy: 0.5532\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6373 - accuracy: 0.6810\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.681 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6812 - accuracy: 0.5570\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6390 - accuracy: 0.6597\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.660 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6823 - accuracy: 0.5592\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6387 - accuracy: 0.6616\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.662 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6836 - accuracy: 0.5557\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6368 - accuracy: 0.6794\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.679 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6879 - accuracy: 0.5406\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6575 - accuracy: 0.6319\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.632 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6824 - accuracy: 0.5591\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6506 - accuracy: 0.6253\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.625 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6874 - accuracy: 0.5397\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6261\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.626 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6774 - accuracy: 0.5699\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6305 - accuracy: 0.6651\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.665 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6799 - accuracy: 0.5671\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6465 - accuracy: 0.6395\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.639 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6834 - accuracy: 0.5505\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6384 - accuracy: 0.6703\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.670 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6774 - accuracy: 0.5683\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6344 - accuracy: 0.6594\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.659 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6770 - accuracy: 0.5694\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6311 - accuracy: 0.6672\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.667 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6802 - accuracy: 0.5636\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6273 - accuracy: 0.6749\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.675 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6780 - accuracy: 0.5691\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6323 - accuracy: 0.6658\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.666 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6717 - accuracy: 0.5851\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6257 - accuracy: 0.6709\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.671 total time=   8.1s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6786 - accuracy: 0.5680\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6330 - accuracy: 0.6682\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.668 total time=   9.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6817 - accuracy: 0.5586\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6401 - accuracy: 0.6508\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.651 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6773 - accuracy: 0.5774\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6408 - accuracy: 0.6479\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.648 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6829 - accuracy: 0.5605\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6331 - accuracy: 0.6751\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.675 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6777 - accuracy: 0.5726\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6313 - accuracy: 0.6631\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.663 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6808 - accuracy: 0.5610\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6406 - accuracy: 0.6610\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.661 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6744 - accuracy: 0.5842\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6262 - accuracy: 0.6775\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.678 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6852 - accuracy: 0.5461\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6382 - accuracy: 0.6688\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.669 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6729 - accuracy: 0.5822\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6293 - accuracy: 0.6632\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.663 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6808 - accuracy: 0.5620\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6374 - accuracy: 0.6556\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.656 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6776 - accuracy: 0.5681\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6301 - accuracy: 0.6627\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.663 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6766 - accuracy: 0.5759\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6358 - accuracy: 0.6622\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.662 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6824 - accuracy: 0.5548\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6297 - accuracy: 0.6731\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.673 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6775 - accuracy: 0.5707\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6281 - accuracy: 0.6718\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6796 - accuracy: 0.5619\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6517 - accuracy: 0.6175\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.617 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6796 - accuracy: 0.5667\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6375 - accuracy: 0.6572\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.657 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6789 - accuracy: 0.5621\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6336 - accuracy: 0.6632\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.663 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6734 - accuracy: 0.5814\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6338 - accuracy: 0.6552\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.655 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6807 - accuracy: 0.5595\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6377 - accuracy: 0.6484\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.648 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6750 - accuracy: 0.5750\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6306 - accuracy: 0.6650\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.665 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6772 - accuracy: 0.5728\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6321 - accuracy: 0.6706\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.671 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6769 - accuracy: 0.5721\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6324 - accuracy: 0.6753\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6816 - accuracy: 0.5602\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6443 - accuracy: 0.6378\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.638 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6713 - accuracy: 0.5855\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6495 - accuracy: 0.6257\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.626 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6780 - accuracy: 0.5690\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6285 - accuracy: 0.6740\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6614 - accuracy: 0.6096\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6244 - accuracy: 0.6720\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.672 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6674 - accuracy: 0.5944\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6297 - accuracy: 0.6600\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.660 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6683 - accuracy: 0.5925\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6189 - accuracy: 0.6739\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6684 - accuracy: 0.5856\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6285 - accuracy: 0.6624\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.662 total time=   9.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6876 - accuracy: 0.5449\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6523 - accuracy: 0.6444\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.644 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6895 - accuracy: 0.5408\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6635 - accuracy: 0.6413\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.641 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6900 - accuracy: 0.5321\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6663 - accuracy: 0.6522\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.652 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6801 - accuracy: 0.5669\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6349 - accuracy: 0.6640\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.664 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6794 - accuracy: 0.5718\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6337 - accuracy: 0.6567\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.657 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6862 - accuracy: 0.5467\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6470 - accuracy: 0.6620\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.662 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6846 - accuracy: 0.5530\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6438 - accuracy: 0.6512\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.651 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6835 - accuracy: 0.5610\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6375 - accuracy: 0.6677\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6785 - accuracy: 0.5727\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6325 - accuracy: 0.6740\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6797 - accuracy: 0.5616\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6353 - accuracy: 0.6676\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6845 - accuracy: 0.5551\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6407 - accuracy: 0.6653\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.665 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6831 - accuracy: 0.5559\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6404 - accuracy: 0.6670\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.667 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6830 - accuracy: 0.5561\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6611 - accuracy: 0.5743\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.574 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6797 - accuracy: 0.5661\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6302 - accuracy: 0.6618\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.662 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6862 - accuracy: 0.5456\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6535 - accuracy: 0.6114\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.611 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6824 - accuracy: 0.5524\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6512 - accuracy: 0.6227\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.623 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6828 - accuracy: 0.5547\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6445 - accuracy: 0.6353\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.635 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.5703\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6300 - accuracy: 0.6713\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.671 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6770 - accuracy: 0.5688\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6335 - accuracy: 0.6611\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.661 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6799 - accuracy: 0.5634\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6384 - accuracy: 0.6518\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.652 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6828 - accuracy: 0.5533\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6361 - accuracy: 0.6636\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.664 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6819 - accuracy: 0.5526\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6419 - accuracy: 0.6485\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.648 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6801 - accuracy: 0.5671\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6325 - accuracy: 0.6691\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.669 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6776 - accuracy: 0.5712\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6279 - accuracy: 0.6705\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6800 - accuracy: 0.5577\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6410 - accuracy: 0.6436\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.644 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6727 - accuracy: 0.5815\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6207 - accuracy: 0.6753\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.675 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6827 - accuracy: 0.5609\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6370 - accuracy: 0.6559\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.656 total time=   8.0s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6798 - accuracy: 0.5677\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6365 - accuracy: 0.6629\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.663 total time=   9.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6858 - accuracy: 0.5538\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6457 - accuracy: 0.6585\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.658 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6803 - accuracy: 0.5622\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6396 - accuracy: 0.6535\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.653 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6775 - accuracy: 0.5766\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6314 - accuracy: 0.6654\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.665 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6761 - accuracy: 0.5789\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6216 - accuracy: 0.6800\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.680 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6741 - accuracy: 0.5825\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6296 - accuracy: 0.6700\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.670 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6798 - accuracy: 0.5603\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6339 - accuracy: 0.6596\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.660 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6855 - accuracy: 0.5467\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6404 - accuracy: 0.6777\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.678 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6824 - accuracy: 0.5587\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6328 - accuracy: 0.6741\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6744 - accuracy: 0.5830\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6298 - accuracy: 0.6650\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.665 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6784 - accuracy: 0.5672\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6287 - accuracy: 0.6666\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.667 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6893 - accuracy: 0.5392\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6530 - accuracy: 0.6674\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.667 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6784 - accuracy: 0.5704\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6357 - accuracy: 0.6606\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.661 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6810 - accuracy: 0.5613\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6279 - accuracy: 0.6762\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.676 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6799 - accuracy: 0.5644\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6275 - accuracy: 0.6720\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.672 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6790 - accuracy: 0.5681\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6318 - accuracy: 0.6652\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.665 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6765 - accuracy: 0.5797\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6236 - accuracy: 0.6788\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.679 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6771 - accuracy: 0.5696\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6353 - accuracy: 0.6490\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.649 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6765 - accuracy: 0.5742\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6302 - accuracy: 0.6691\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6802 - accuracy: 0.5611\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6447 - accuracy: 0.6387\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.639 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6775 - accuracy: 0.5706\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6374 - accuracy: 0.6564\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.656 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6731 - accuracy: 0.5793\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6291 - accuracy: 0.6640\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.664 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6709 - accuracy: 0.5875\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6205 - accuracy: 0.6806\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.681 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6691 - accuracy: 0.5859\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6239 - accuracy: 0.6713\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6762 - accuracy: 0.5747\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6275 - accuracy: 0.6675\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.668 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6759 - accuracy: 0.5744\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6198 - accuracy: 0.6800\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.680 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6659 - accuracy: 0.5990\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6291 - accuracy: 0.6612\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.661 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6856 - accuracy: 0.5432\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6422 - accuracy: 0.6568\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.657 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6746 - accuracy: 0.5827\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6234 - accuracy: 0.6760\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.676 total time=   8.1s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6743 - accuracy: 0.5808\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6268 - accuracy: 0.6713\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.671 total time=   9.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6722 - accuracy: 0.5851\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6363 - accuracy: 0.6539\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.654 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6768 - accuracy: 0.5733\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6266 - accuracy: 0.6744\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6763 - accuracy: 0.5731\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6375 - accuracy: 0.6537\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.654 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6656 - accuracy: 0.6021\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6283 - accuracy: 0.6699\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.670 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6703 - accuracy: 0.5885\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6649\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.665 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6700 - accuracy: 0.5877\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6250 - accuracy: 0.6722\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.672 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6711 - accuracy: 0.5826\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6275 - accuracy: 0.6645\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.665 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6716 - accuracy: 0.5939\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6230 - accuracy: 0.6774\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6753 - accuracy: 0.5796\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6311 - accuracy: 0.6716\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6783 - accuracy: 0.5659\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6376 - accuracy: 0.6458\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.646 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6783 - accuracy: 0.5668\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6240 - accuracy: 0.6811\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.681 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6764 - accuracy: 0.5711\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6294 - accuracy: 0.6659\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.666 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6774 - accuracy: 0.5661\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6316 - accuracy: 0.6628\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.663 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6698 - accuracy: 0.5906\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6199 - accuracy: 0.6786\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.679 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6769 - accuracy: 0.5705\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6277 - accuracy: 0.6693\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.669 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6704 - accuracy: 0.5885\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6541\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.654 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6757 - accuracy: 0.5743\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6204 - accuracy: 0.6829\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.683 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6711 - accuracy: 0.5813\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6261 - accuracy: 0.6730\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.673 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6746 - accuracy: 0.5717\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6338 - accuracy: 0.6542\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.654 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6663 - accuracy: 0.5964\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6162 - accuracy: 0.6787\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.679 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6668 - accuracy: 0.5901\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6243 - accuracy: 0.6728\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.673 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6718 - accuracy: 0.5788\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6253 - accuracy: 0.6685\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6662 - accuracy: 0.5893\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6322 - accuracy: 0.6524\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.652 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6695 - accuracy: 0.5845\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6238 - accuracy: 0.6719\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.672 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6779 - accuracy: 0.5791\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6322 - accuracy: 0.6683\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6786 - accuracy: 0.5661\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6406 - accuracy: 0.6592\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.659 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6827 - accuracy: 0.5635\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6363 - accuracy: 0.6754\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.675 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6854 - accuracy: 0.5566\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6515 - accuracy: 0.6258\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.626 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6862 - accuracy: 0.5544\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6380 - accuracy: 0.6725\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.672 total time=   9.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6825 - accuracy: 0.5585\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6301 - accuracy: 0.6697\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.670 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6759 - accuracy: 0.5804\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6320 - accuracy: 0.6625\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.663 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6722 - accuracy: 0.5867\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6229 - accuracy: 0.6745\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.675 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6828 - accuracy: 0.5622\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6333 - accuracy: 0.6736\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.674 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6711 - accuracy: 0.5928\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6305 - accuracy: 0.6519\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.652 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6838 - accuracy: 0.5581\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6359 - accuracy: 0.6762\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.676 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6826 - accuracy: 0.5539\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6348 - accuracy: 0.6691\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.669 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6736 - accuracy: 0.5829\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6294 - accuracy: 0.6660\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.666 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6805 - accuracy: 0.5579\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6321 - accuracy: 0.6701\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.670 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6704 - accuracy: 0.5846\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6289 - accuracy: 0.6593\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.659 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6771 - accuracy: 0.5687\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6601 - accuracy: 0.6193\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.619 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6710 - accuracy: 0.5885\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6164 - accuracy: 0.6802\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.680 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6765 - accuracy: 0.5702\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6259 - accuracy: 0.6726\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.673 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6735 - accuracy: 0.5774\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6310 - accuracy: 0.6643\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.664 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6767 - accuracy: 0.5790\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6234 - accuracy: 0.6765\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.676 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6736 - accuracy: 0.5812\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6281 - accuracy: 0.6621\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.662 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6751 - accuracy: 0.5746\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6357 - accuracy: 0.6649\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.665 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6714 - accuracy: 0.5858\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6185 - accuracy: 0.6834\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.683 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6759 - accuracy: 0.5751\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6304 - accuracy: 0.6687\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.669 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6738 - accuracy: 0.5806\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6313 - accuracy: 0.6588\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.659 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6714 - accuracy: 0.5839\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6177 - accuracy: 0.6768\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.677 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6754 - accuracy: 0.5731\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6345 - accuracy: 0.6529\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.653 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6801 - accuracy: 0.5626\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6289 - accuracy: 0.6692\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6807 - accuracy: 0.5706\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6297 - accuracy: 0.6716\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.672 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6759 - accuracy: 0.5722\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6291 - accuracy: 0.6763\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.676 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6813 - accuracy: 0.5585\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6441 - accuracy: 0.6450\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.645 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6788 - accuracy: 0.5680\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6257 - accuracy: 0.6781\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.678 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6736 - accuracy: 0.5814\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6266 - accuracy: 0.6772\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.677 total time=   9.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6722 - accuracy: 0.5838\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6307 - accuracy: 0.6629\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.663 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6748 - accuracy: 0.5773\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6223 - accuracy: 0.6778\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.678 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6753 - accuracy: 0.5750\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6711\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6737 - accuracy: 0.5787\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6319 - accuracy: 0.6583\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.658 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6779 - accuracy: 0.5729\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6212 - accuracy: 0.6772\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.677 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6808 - accuracy: 0.5624\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6358 - accuracy: 0.6560\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.656 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6788 - accuracy: 0.5642\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6294 - accuracy: 0.6615\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.661 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6746 - accuracy: 0.5775\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6226 - accuracy: 0.6773\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.677 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6714 - accuracy: 0.5879\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6247 - accuracy: 0.6729\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.673 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6679 - accuracy: 0.5914\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6296 - accuracy: 0.6603\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.660 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6733 - accuracy: 0.5807\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6247 - accuracy: 0.6713\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.671 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6807 - accuracy: 0.5589\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6342 - accuracy: 0.6571\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.657 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6654 - accuracy: 0.5990\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6260 - accuracy: 0.6682\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.668 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6695 - accuracy: 0.5869\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6246 - accuracy: 0.6710\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6750 - accuracy: 0.5788\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6360 - accuracy: 0.6472\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.647 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6685 - accuracy: 0.5898\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6277 - accuracy: 0.6650\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.665 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6726 - accuracy: 0.5804\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6207 - accuracy: 0.6732\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.673 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6766 - accuracy: 0.5712\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6334 - accuracy: 0.6540\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.654 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6757 - accuracy: 0.5737\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6319 - accuracy: 0.6601\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.660 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6685 - accuracy: 0.5910\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6171 - accuracy: 0.6784\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.678 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6736 - accuracy: 0.5784\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6261 - accuracy: 0.6695\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6782 - accuracy: 0.5675\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6340 - accuracy: 0.6630\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.663 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6649 - accuracy: 0.6062\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6331 - accuracy: 0.6569\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.657 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6696 - accuracy: 0.5876\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6245 - accuracy: 0.6729\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.673 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6676 - accuracy: 0.5937\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.6588\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.659 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6676 - accuracy: 0.5983\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6208 - accuracy: 0.6765\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.676 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6751 - accuracy: 0.5752\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.6750\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.675 total time=   8.1s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6608 - accuracy: 0.6107\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6234 - accuracy: 0.6703\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.670 total time=   9.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6756 - accuracy: 0.5723\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6452 - accuracy: 0.6367\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.637 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6741 - accuracy: 0.5772\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6442 - accuracy: 0.6434\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.643 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6731 - accuracy: 0.5789\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6281 - accuracy: 0.6640\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.664 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6739 - accuracy: 0.5790\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6251 - accuracy: 0.6673\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.667 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6678 - accuracy: 0.5911\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6276 - accuracy: 0.6683\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6693 - accuracy: 0.5865\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6284 - accuracy: 0.6621\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.662 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6743 - accuracy: 0.5766\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6344 - accuracy: 0.6535\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.654 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6656 - accuracy: 0.5915\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6215 - accuracy: 0.6728\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.673 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6590 - accuracy: 0.6132\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6251 - accuracy: 0.6638\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.664 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6735 - accuracy: 0.5823\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6193 - accuracy: 0.6832\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.683 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6707 - accuracy: 0.5825\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6684\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6640 - accuracy: 0.6025\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6284 - accuracy: 0.6640\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.664 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6695 - accuracy: 0.5820\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6207 - accuracy: 0.6783\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.678 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6634 - accuracy: 0.5997\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6238 - accuracy: 0.6711\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.671 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6722 - accuracy: 0.5741\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6403 - accuracy: 0.6455\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.645 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6740 - accuracy: 0.5782\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6225 - accuracy: 0.6814\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.681 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6709 - accuracy: 0.5848\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6277 - accuracy: 0.6633\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.663 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6697 - accuracy: 0.5830\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6686\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.669 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6716 - accuracy: 0.5829\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6207 - accuracy: 0.6771\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.677 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6718 - accuracy: 0.5840\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6199 - accuracy: 0.6759\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.676 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6523 - accuracy: 0.6263\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6415 - accuracy: 0.6582\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.658 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6649 - accuracy: 0.5974\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6696\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.670 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6482 - accuracy: 0.6393\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6196 - accuracy: 0.6789\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.679 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6551 - accuracy: 0.6175\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6285 - accuracy: 0.6676\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6537 - accuracy: 0.6196\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6252 - accuracy: 0.6778\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.678 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6586 - accuracy: 0.6057\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6234 - accuracy: 0.6691\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6500 - accuracy: 0.6282\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6249 - accuracy: 0.6684\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.668 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6548 - accuracy: 0.6224\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6191 - accuracy: 0.6841\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.684 total time=   9.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6505 - accuracy: 0.6282\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6227 - accuracy: 0.6709\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6573 - accuracy: 0.6137\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6283 - accuracy: 0.6690\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.669 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6518 - accuracy: 0.6305\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6139 - accuracy: 0.6812\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.681 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6472 - accuracy: 0.6369\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6263 - accuracy: 0.6770\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6525 - accuracy: 0.6216\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6281 - accuracy: 0.6597\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.660 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6553 - accuracy: 0.6249\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6136 - accuracy: 0.6841\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.684 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6567 - accuracy: 0.6209\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6242 - accuracy: 0.6780\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.678 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6516 - accuracy: 0.6281\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6345 - accuracy: 0.6537\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.654 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6572 - accuracy: 0.6137\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6192 - accuracy: 0.6788\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.679 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6502 - accuracy: 0.6320\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6242 - accuracy: 0.6725\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.672 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6431 - accuracy: 0.6460\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6219 - accuracy: 0.6698\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.670 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6532 - accuracy: 0.6248\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6217 - accuracy: 0.6767\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6541 - accuracy: 0.6173\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6295 - accuracy: 0.6525\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.653 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6427 - accuracy: 0.6407\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6203 - accuracy: 0.6760\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.676 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6501 - accuracy: 0.6304\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6139 - accuracy: 0.6813\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.681 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6499 - accuracy: 0.6295\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6346 - accuracy: 0.6567\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.657 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6490 - accuracy: 0.6242\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6279 - accuracy: 0.6667\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6543 - accuracy: 0.6210\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6310 - accuracy: 0.6804\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.680 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6542 - accuracy: 0.6211\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6323 - accuracy: 0.6704\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.670 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6563 - accuracy: 0.6171\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6266 - accuracy: 0.6645\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.665 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6614 - accuracy: 0.6085\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6155 - accuracy: 0.6774\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6548 - accuracy: 0.6147\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6252 - accuracy: 0.6709\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6499 - accuracy: 0.6326\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6409 - accuracy: 0.6494\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.649 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6525 - accuracy: 0.6264\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6214 - accuracy: 0.6769\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.677 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6542 - accuracy: 0.6224\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6311 - accuracy: 0.6611\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.661 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6508 - accuracy: 0.6248\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6364 - accuracy: 0.6677\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6502 - accuracy: 0.6311\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6248 - accuracy: 0.6764\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.676 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6466 - accuracy: 0.6344\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6280 - accuracy: 0.6529\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.653 total time=   9.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6514 - accuracy: 0.6282\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6318 - accuracy: 0.6578\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.658 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6537 - accuracy: 0.6244\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6187 - accuracy: 0.6789\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.679 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6469 - accuracy: 0.6396\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6280 - accuracy: 0.6628\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.663 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6510 - accuracy: 0.6289\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.6723\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.672 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6524 - accuracy: 0.6189\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6161 - accuracy: 0.6812\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.681 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6475 - accuracy: 0.6401\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6277 - accuracy: 0.6703\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.670 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6446 - accuracy: 0.6385\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6223 - accuracy: 0.6716\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.672 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6471 - accuracy: 0.6391\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6161 - accuracy: 0.6862\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.686 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6520 - accuracy: 0.6283\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6230 - accuracy: 0.6765\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6490 - accuracy: 0.6327\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6332 - accuracy: 0.6698\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.670 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6523 - accuracy: 0.6279\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6194 - accuracy: 0.6737\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.674 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6521 - accuracy: 0.6271\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6350 - accuracy: 0.6576\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.658 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6460 - accuracy: 0.6360\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6506 - accuracy: 0.6537\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.654 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6508 - accuracy: 0.6281\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6096 - accuracy: 0.6843\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.684 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6527 - accuracy: 0.6240\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6200 - accuracy: 0.6734\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.673 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6439 - accuracy: 0.6422\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6232 - accuracy: 0.6681\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.668 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6568 - accuracy: 0.6213\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6778\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.678 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6506 - accuracy: 0.6272\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6268 - accuracy: 0.6749\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.675 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6542 - accuracy: 0.6217\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6241 - accuracy: 0.6701\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.670 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6616 - accuracy: 0.6061\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6275 - accuracy: 0.6786\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.679 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6537 - accuracy: 0.6229\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6670\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.667 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6452 - accuracy: 0.6403\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6398 - accuracy: 0.6547\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.655 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6509 - accuracy: 0.6303\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6139 - accuracy: 0.6778\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.678 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6502 - accuracy: 0.6290\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6204 - accuracy: 0.6754\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.675 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6499 - accuracy: 0.6294\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6449 - accuracy: 0.6338\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.634 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6486 - accuracy: 0.6310\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6122 - accuracy: 0.6823\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.682 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6464 - accuracy: 0.6363\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6235 - accuracy: 0.6764\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.676 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6482 - accuracy: 0.6362\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6321 - accuracy: 0.6597\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.660 total time=   8.1s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6560 - accuracy: 0.6212\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6210 - accuracy: 0.6836\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.684 total time=   9.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6465 - accuracy: 0.6389\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6205 - accuracy: 0.6763\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.676 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6482 - accuracy: 0.6293\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6195 - accuracy: 0.6765\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.676 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6544 - accuracy: 0.6226\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6204 - accuracy: 0.6778\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.678 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6512 - accuracy: 0.6222\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6237 - accuracy: 0.6684\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6480 - accuracy: 0.6311\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6374 - accuracy: 0.6617\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.662 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6512 - accuracy: 0.6300\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6253 - accuracy: 0.6732\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.673 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6553 - accuracy: 0.6189\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6262 - accuracy: 0.6672\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.667 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6520 - accuracy: 0.6326\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6252 - accuracy: 0.6692\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6583 - accuracy: 0.6147\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6241 - accuracy: 0.6667\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.667 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6554 - accuracy: 0.6222\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6280 - accuracy: 0.6719\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.672 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6464 - accuracy: 0.6338\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6222 - accuracy: 0.6759\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.676 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6526 - accuracy: 0.6281\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6246 - accuracy: 0.6750\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.675 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6534 - accuracy: 0.6230\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6251 - accuracy: 0.6743\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6439 - accuracy: 0.6371\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6279 - accuracy: 0.6674\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6508 - accuracy: 0.6268\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6264 - accuracy: 0.6799\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.680 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6474 - accuracy: 0.6330\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6274 - accuracy: 0.6703\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.670 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6512 - accuracy: 0.6283\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6301 - accuracy: 0.6606\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.661 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6507 - accuracy: 0.6325\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6177 - accuracy: 0.6798\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.680 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6482 - accuracy: 0.6301\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6232 - accuracy: 0.6698\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.670 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6528 - accuracy: 0.6281\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6239 - accuracy: 0.6712\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6584 - accuracy: 0.6124\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6130 - accuracy: 0.6805\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.681 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6519 - accuracy: 0.6216\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6232 - accuracy: 0.6733\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.673 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6502 - accuracy: 0.6278\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6298 - accuracy: 0.6686\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.669 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6530 - accuracy: 0.6219\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6202 - accuracy: 0.6797\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.680 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6545 - accuracy: 0.6172\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6225 - accuracy: 0.6732\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6498 - accuracy: 0.6325\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6268 - accuracy: 0.6675\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.668 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6525 - accuracy: 0.6244\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6203 - accuracy: 0.6757\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.676 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6497 - accuracy: 0.6335\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6202 - accuracy: 0.6774\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.677 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6496 - accuracy: 0.6290\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6297 - accuracy: 0.6669\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.667 total time=   9.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6554 - accuracy: 0.6238\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6122 - accuracy: 0.6861\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.686 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6542 - accuracy: 0.6184\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6224 - accuracy: 0.6722\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.672 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6497 - accuracy: 0.6277\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6244 - accuracy: 0.6702\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.670 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6528 - accuracy: 0.6256\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6248 - accuracy: 0.6649\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.665 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6445 - accuracy: 0.6386\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6301 - accuracy: 0.6625\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.662 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6461 - accuracy: 0.6400\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6252 - accuracy: 0.6698\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.670 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6514 - accuracy: 0.6281\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6268 - accuracy: 0.6575\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.657 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6556 - accuracy: 0.6223\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6458 - accuracy: 0.6512\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.651 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6491 - accuracy: 0.6300\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6265 - accuracy: 0.6660\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.666 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6552 - accuracy: 0.6187\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6165 - accuracy: 0.6760\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.676 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6489 - accuracy: 0.6291\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6244 - accuracy: 0.6683\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.668 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6467 - accuracy: 0.6344\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6290 - accuracy: 0.6639\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.664 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6500 - accuracy: 0.6263\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6226 - accuracy: 0.6842\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.684 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6466 - accuracy: 0.6353\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6153 - accuracy: 0.6817\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.682 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6507 - accuracy: 0.6321\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6226 - accuracy: 0.6744\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6566 - accuracy: 0.6227\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6217 - accuracy: 0.6856\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.686 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6501 - accuracy: 0.6294\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6421 - accuracy: 0.6319\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.632 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6513 - accuracy: 0.6260\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6268 - accuracy: 0.6669\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.667 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6519 - accuracy: 0.6242\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6430 - accuracy: 0.6604\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.660 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6456 - accuracy: 0.6365\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6235 - accuracy: 0.6703\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.670 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6493 - accuracy: 0.6267\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6253 - accuracy: 0.6624\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.662 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6513 - accuracy: 0.6246\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6220 - accuracy: 0.6755\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.675 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6468 - accuracy: 0.6374\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6226 - accuracy: 0.6658\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.666 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6536 - accuracy: 0.6237\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6395 - accuracy: 0.6621\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.662 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6536 - accuracy: 0.6244\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6249 - accuracy: 0.6774\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.677 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6493 - accuracy: 0.6336\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6303 - accuracy: 0.6631\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.663 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6502 - accuracy: 0.6311\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6228 - accuracy: 0.6658\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.666 total time=   8.1s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6535 - accuracy: 0.6269\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6222 - accuracy: 0.6751\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.675 total time=   9.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6481 - accuracy: 0.6346\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6207 - accuracy: 0.6733\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.673 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6506 - accuracy: 0.6252\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6330 - accuracy: 0.6676\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6504 - accuracy: 0.6338\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6199 - accuracy: 0.6849\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.685 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6556 - accuracy: 0.6212\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6201 - accuracy: 0.6741\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6483 - accuracy: 0.6360\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6271 - accuracy: 0.6706\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6513 - accuracy: 0.6251\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6152 - accuracy: 0.6787\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.679 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6501 - accuracy: 0.6292\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6226 - accuracy: 0.6720\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.672 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6483 - accuracy: 0.6330\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6236 - accuracy: 0.6681\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6485 - accuracy: 0.6299\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6208 - accuracy: 0.6742\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6520 - accuracy: 0.6256\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6343 - accuracy: 0.6552\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.655 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6515 - accuracy: 0.6241\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6276 - accuracy: 0.6671\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6570 - accuracy: 0.6216\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6641 - accuracy: 0.5862\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.586 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6475 - accuracy: 0.6283\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6224 - accuracy: 0.6776\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.678 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6497 - accuracy: 0.6300\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6267 - accuracy: 0.6615\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.661 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6567 - accuracy: 0.6172\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6197 - accuracy: 0.6833\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.683 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6474 - accuracy: 0.6358\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6343 - accuracy: 0.6499\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.650 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6483 - accuracy: 0.6300\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6228 - accuracy: 0.6721\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.672 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6554 - accuracy: 0.6247\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6156 - accuracy: 0.6749\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.675 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6477 - accuracy: 0.6313\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6211 - accuracy: 0.6756\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.676 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6466 - accuracy: 0.6375\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6321 - accuracy: 0.6600\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.660 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6478 - accuracy: 0.6362\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6190 - accuracy: 0.6741\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.674 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6480 - accuracy: 0.6354\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6256 - accuracy: 0.6673\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.667 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6507 - accuracy: 0.6296\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6296 - accuracy: 0.6634\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.663 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6591 - accuracy: 0.6139\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6202 - accuracy: 0.6821\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.682 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6490 - accuracy: 0.6364\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6259 - accuracy: 0.6756\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.676 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6457 - accuracy: 0.6351\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6198 - accuracy: 0.6751\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.675 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6551 - accuracy: 0.6232\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6161 - accuracy: 0.6833\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.683 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6482 - accuracy: 0.6347\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6244 - accuracy: 0.6704\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.670 total time=   8.1s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6477 - accuracy: 0.6341\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6289 - accuracy: 0.6670\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.667 total time=   9.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6487 - accuracy: 0.6348\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6147 - accuracy: 0.6828\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.683 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6472 - accuracy: 0.6402\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6326 - accuracy: 0.6664\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.666 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6497 - accuracy: 0.6333\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6264 - accuracy: 0.6686\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6482 - accuracy: 0.6327\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6127 - accuracy: 0.6827\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.683 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6475 - accuracy: 0.6354\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6238 - accuracy: 0.6643\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.664 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6527 - accuracy: 0.6147\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6329 - accuracy: 0.6629\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.663 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6484 - accuracy: 0.6306\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6238 - accuracy: 0.6685\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.669 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6470 - accuracy: 0.6284\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6223 - accuracy: 0.6799\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.680 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6472 - accuracy: 0.6326\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6363 - accuracy: 0.6497\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.650 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6462 - accuracy: 0.6346\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6145 - accuracy: 0.6798\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.680 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6492 - accuracy: 0.6372\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6456 - accuracy: 0.6384\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.638 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6481 - accuracy: 0.6326\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6252 - accuracy: 0.6708\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.671 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6555 - accuracy: 0.6217\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6203 - accuracy: 0.6821\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.682 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6501 - accuracy: 0.6298\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6203 - accuracy: 0.6695\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.669 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6553 - accuracy: 0.6189\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6262 - accuracy: 0.6706\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.671 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6520 - accuracy: 0.6242\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6307 - accuracy: 0.6612\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.661 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6562 - accuracy: 0.6175\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6243 - accuracy: 0.6754\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.675 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6496 - accuracy: 0.6337\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6340 - accuracy: 0.6630\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.663 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6520 - accuracy: 0.6258\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6230 - accuracy: 0.6839\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.684 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6486 - accuracy: 0.6308\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6232 - accuracy: 0.6717\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.672 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6518 - accuracy: 0.6319\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.6673\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.667 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6577 - accuracy: 0.6164\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6213 - accuracy: 0.6754\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.675 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6460 - accuracy: 0.6371\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6284 - accuracy: 0.6720\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6465 - accuracy: 0.6360\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6244 - accuracy: 0.6693\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.669 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6511 - accuracy: 0.6288\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6311 - accuracy: 0.6536\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.654 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6466 - accuracy: 0.6362\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6409 - accuracy: 0.6448\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.645 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6461 - accuracy: 0.6355\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6554 - accuracy: 0.6151\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.615 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6494 - accuracy: 0.6288\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6339 - accuracy: 0.6493\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.649 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6425 - accuracy: 0.6455\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6445 - accuracy: 0.6492\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.649 total time=   9.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6534 - accuracy: 0.6274\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6262 - accuracy: 0.6705\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.670 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6523 - accuracy: 0.6272\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6120 - accuracy: 0.6855\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.685 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6507 - accuracy: 0.6296\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6286 - accuracy: 0.6628\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.663 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6502 - accuracy: 0.6294\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6240 - accuracy: 0.6673\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.667 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6501 - accuracy: 0.6301\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6211 - accuracy: 0.6775\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.678 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6449 - accuracy: 0.6389\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6283 - accuracy: 0.6688\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.669 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6441 - accuracy: 0.6358\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6216 - accuracy: 0.6735\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.673 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6528 - accuracy: 0.6252\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6170 - accuracy: 0.6812\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.681 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6497 - accuracy: 0.6328\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6232 - accuracy: 0.6750\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.675 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6450 - accuracy: 0.6404\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6304 - accuracy: 0.6533\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.653 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6519 - accuracy: 0.6306\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6134 - accuracy: 0.6755\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.675 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6518 - accuracy: 0.6314\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6203 - accuracy: 0.6771\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6504 - accuracy: 0.6276\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6395 - accuracy: 0.6564\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.656 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6535 - accuracy: 0.6243\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6142 - accuracy: 0.6812\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.681 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6498 - accuracy: 0.6302\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6281 - accuracy: 0.6811\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.681 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6396 - accuracy: 0.6482\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6231 - accuracy: 0.6696\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.670 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6498 - accuracy: 0.6303\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6169 - accuracy: 0.6637\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.664 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6461 - accuracy: 0.6309\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6285 - accuracy: 0.6679\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.668 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6488 - accuracy: 0.6336\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6306 - accuracy: 0.6660\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.666 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6466 - accuracy: 0.6415\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6118 - accuracy: 0.6833\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.683 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6527 - accuracy: 0.6229\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6346 - accuracy: 0.6506\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.651 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6422 - accuracy: 0.6466\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6271 - accuracy: 0.6750\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.675 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6516 - accuracy: 0.6268\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6201 - accuracy: 0.6820\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.682 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6471 - accuracy: 0.6362\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6178 - accuracy: 0.6770\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6452 - accuracy: 0.6402\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6280 - accuracy: 0.6728\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.673 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6480 - accuracy: 0.6344\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6259 - accuracy: 0.6780\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.678 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6474 - accuracy: 0.6338\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6295 - accuracy: 0.6550\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.655 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6477 - accuracy: 0.6319\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.6643\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.664 total time=   9.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6537 - accuracy: 0.6237\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6295 - accuracy: 0.6469\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.647 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6454 - accuracy: 0.6392\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6235 - accuracy: 0.6757\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.676 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6492 - accuracy: 0.6326\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6221 - accuracy: 0.6744\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6500 - accuracy: 0.6297\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6224 - accuracy: 0.6659\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.666 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6445 - accuracy: 0.6386\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6419 - accuracy: 0.6405\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.640 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6433 - accuracy: 0.6358\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6385 - accuracy: 0.6401\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.640 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6474 - accuracy: 0.6326\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6175 - accuracy: 0.6817\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.682 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6480 - accuracy: 0.6273\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6215 - accuracy: 0.6772\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.677 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6487 - accuracy: 0.6361\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6706\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.671 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6503 - accuracy: 0.6302\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6229 - accuracy: 0.6784\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.678 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6500 - accuracy: 0.6325\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6236 - accuracy: 0.6749\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.675 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6438 - accuracy: 0.6391\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6219 - accuracy: 0.6718\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.672 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6470 - accuracy: 0.6345\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6224 - accuracy: 0.6666\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.667 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6525 - accuracy: 0.6240\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6300 - accuracy: 0.6761\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.676 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6432 - accuracy: 0.6397\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6228 - accuracy: 0.6717\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.672 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6533 - accuracy: 0.6283\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6246 - accuracy: 0.6751\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.675 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6490 - accuracy: 0.6320\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6236 - accuracy: 0.6675\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.668 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6476 - accuracy: 0.6372\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6316 - accuracy: 0.6577\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.658 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6512 - accuracy: 0.6338\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6359 - accuracy: 0.6703\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.670 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6471 - accuracy: 0.6386\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6301 - accuracy: 0.6654\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.665 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6482 - accuracy: 0.6272\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6267 - accuracy: 0.6578\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.658 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6524 - accuracy: 0.6251\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6166 - accuracy: 0.6834\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.683 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6448 - accuracy: 0.6391\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6218 - accuracy: 0.6701\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.670 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6471 - accuracy: 0.6383\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6288 - accuracy: 0.6683\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6474 - accuracy: 0.6331\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6194 - accuracy: 0.6844\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.684 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6457 - accuracy: 0.6348\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6308 - accuracy: 0.6667\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.667 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6449 - accuracy: 0.6404\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6229 - accuracy: 0.6702\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.670 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6504 - accuracy: 0.6353\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6182 - accuracy: 0.6749\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6511 - accuracy: 0.6291\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6302 - accuracy: 0.6686\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   8.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6474 - accuracy: 0.6375\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6262 - accuracy: 0.6704\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6500 - accuracy: 0.6300\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6118 - accuracy: 0.6805\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.681 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6456 - accuracy: 0.6398\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6229 - accuracy: 0.6721\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.672 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6480 - accuracy: 0.6346\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6352 - accuracy: 0.6564\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.656 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6478 - accuracy: 0.6340\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6154 - accuracy: 0.6850\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.685 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6430 - accuracy: 0.6404\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6302 - accuracy: 0.6720\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.672 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6518 - accuracy: 0.6254\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6294 - accuracy: 0.6601\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.660 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6536 - accuracy: 0.6265\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6147 - accuracy: 0.6827\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.683 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6500 - accuracy: 0.6333\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6237 - accuracy: 0.6736\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.674 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6452 - accuracy: 0.6374\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6227 - accuracy: 0.6711\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6497 - accuracy: 0.6320\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6175 - accuracy: 0.6745\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.675 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6538 - accuracy: 0.6207\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6328 - accuracy: 0.6661\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.666 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6481 - accuracy: 0.6326\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6320 - accuracy: 0.6727\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6537 - accuracy: 0.6210\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6794\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.679 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6489 - accuracy: 0.6327\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6343 - accuracy: 0.6591\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.659 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6528 - accuracy: 0.6256\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6223 - accuracy: 0.6728\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.673 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6582 - accuracy: 0.6138\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6186 - accuracy: 0.6804\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.680 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6496 - accuracy: 0.6257\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6248 - accuracy: 0.6670\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.667 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6505 - accuracy: 0.6248\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6252 - accuracy: 0.6707\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.671 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6527 - accuracy: 0.6250\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6144 - accuracy: 0.6801\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.680 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6504 - accuracy: 0.6305\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6214 - accuracy: 0.6749\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.675 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6450 - accuracy: 0.6365\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6214 - accuracy: 0.6704\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.670 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6527 - accuracy: 0.6232\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6254 - accuracy: 0.6826\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.683 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6514 - accuracy: 0.6206\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6212 - accuracy: 0.6791\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.679 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6494 - accuracy: 0.6351\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6238 - accuracy: 0.6713\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.671 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6535 - accuracy: 0.6263\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6232 - accuracy: 0.6794\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.679 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6507 - accuracy: 0.6307\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6251 - accuracy: 0.6693\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6492 - accuracy: 0.6330\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6268 - accuracy: 0.6668\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.667 total time=   8.0s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6500 - accuracy: 0.6284\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6217 - accuracy: 0.6826\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.683 total time=   9.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6541 - accuracy: 0.6207\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6255 - accuracy: 0.6751\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.675 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6467 - accuracy: 0.6292\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6299 - accuracy: 0.6633\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.663 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6478 - accuracy: 0.6363\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6141 - accuracy: 0.6810\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.681 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6533 - accuracy: 0.6225\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6416 - accuracy: 0.6381\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.638 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6541 - accuracy: 0.6252\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6361 - accuracy: 0.6638\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.664 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6528 - accuracy: 0.6274\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6157 - accuracy: 0.6792\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.679 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6523 - accuracy: 0.6250\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6547 - accuracy: 0.6418\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.642 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6494 - accuracy: 0.6335\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6260 - accuracy: 0.6690\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6549 - accuracy: 0.6207\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6167 - accuracy: 0.6803\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.680 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6462 - accuracy: 0.6335\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6234 - accuracy: 0.6745\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6466 - accuracy: 0.6378\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6321 - accuracy: 0.6578\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.658 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6534 - accuracy: 0.6186\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6145 - accuracy: 0.6795\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.679 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6505 - accuracy: 0.6295\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6203 - accuracy: 0.6744\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6483 - accuracy: 0.6307\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6505 - accuracy: 0.6291\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.629 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6515 - accuracy: 0.6290\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6280 - accuracy: 0.6706\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6499 - accuracy: 0.6305\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6292 - accuracy: 0.6697\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.670 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6449 - accuracy: 0.6413\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6310 - accuracy: 0.6592\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.659 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6524 - accuracy: 0.6281\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6258 - accuracy: 0.6706\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6563 - accuracy: 0.6160\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6401 - accuracy: 0.6512\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.651 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6442 - accuracy: 0.6391\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6232 - accuracy: 0.6679\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6500 - accuracy: 0.6320\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6143 - accuracy: 0.6847\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.685 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6463 - accuracy: 0.6364\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6235 - accuracy: 0.6719\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.672 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6456 - accuracy: 0.6371\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6250 - accuracy: 0.6690\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6508 - accuracy: 0.6283\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6122 - accuracy: 0.6822\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.682 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6520 - accuracy: 0.6263\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6237 - accuracy: 0.6751\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6456 - accuracy: 0.6382\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6379 - accuracy: 0.6528\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.653 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6536 - accuracy: 0.6218\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6385 - accuracy: 0.6407\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.641 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6489 - accuracy: 0.6318\n",
      "354/354 [==============================] - 3s 4ms/step - loss: 0.6245 - accuracy: 0.6726\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.673 total time=   9.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6438 - accuracy: 0.6414\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6255 - accuracy: 0.6674\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6527 - accuracy: 0.6239\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6229 - accuracy: 0.6677\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.668 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6464 - accuracy: 0.6372\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6223 - accuracy: 0.6791\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.679 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6513 - accuracy: 0.6295\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6255 - accuracy: 0.6698\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.670 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6531 - accuracy: 0.6230\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6101 - accuracy: 0.6871\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.687 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6516 - accuracy: 0.6260\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6193 - accuracy: 0.6746\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.675 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6463 - accuracy: 0.6403\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6426 - accuracy: 0.6467\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.647 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6550 - accuracy: 0.6221\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6785\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.679 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6500 - accuracy: 0.6311\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6531 - accuracy: 0.6136\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.614 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6456 - accuracy: 0.6361\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6216 - accuracy: 0.6703\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.670 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6524 - accuracy: 0.6306\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6176 - accuracy: 0.6835\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.684 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6511 - accuracy: 0.6286\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6204 - accuracy: 0.6780\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.678 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6524 - accuracy: 0.6239\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6248 - accuracy: 0.6684\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6534 - accuracy: 0.6287\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6146 - accuracy: 0.6822\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.682 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6486 - accuracy: 0.6328\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6226 - accuracy: 0.6706\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.671 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6481 - accuracy: 0.6338\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6213 - accuracy: 0.6793\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.679 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6518 - accuracy: 0.6247\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6286 - accuracy: 0.6688\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6473 - accuracy: 0.6373\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6164 - accuracy: 0.6802\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.680 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6467 - accuracy: 0.6388\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6232 - accuracy: 0.6710\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6496 - accuracy: 0.6300\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6187 - accuracy: 0.6738\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6523 - accuracy: 0.6232\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6259 - accuracy: 0.6646\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.665 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6468 - accuracy: 0.6349\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6366 - accuracy: 0.6475\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.648 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6534 - accuracy: 0.6246\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6197 - accuracy: 0.6755\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6485 - accuracy: 0.6365\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6330 - accuracy: 0.6498\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.650 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6464 - accuracy: 0.6376\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6366 - accuracy: 0.6554\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.655 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6560 - accuracy: 0.6198\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6262 - accuracy: 0.6754\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.675 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6477 - accuracy: 0.6331\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6235 - accuracy: 0.6767\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6457 - accuracy: 0.6368\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6255 - accuracy: 0.6727\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.673 total time=   7.9s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6503 - accuracy: 0.6339\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6202 - accuracy: 0.6869\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.687 total time=   9.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6449 - accuracy: 0.6369\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6203 - accuracy: 0.6765\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6525 - accuracy: 0.6301\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6216 - accuracy: 0.6768\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6523 - accuracy: 0.6350\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6157 - accuracy: 0.6752\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.675 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6525 - accuracy: 0.6241\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6165 - accuracy: 0.6776\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.678 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6484 - accuracy: 0.6349\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6260 - accuracy: 0.6721\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.672 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6539 - accuracy: 0.6227\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6280 - accuracy: 0.6570\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.657 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6500 - accuracy: 0.6312\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6246 - accuracy: 0.6746\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.675 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6456 - accuracy: 0.6353\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6692\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.669 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6587 - accuracy: 0.6123\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6240 - accuracy: 0.6641\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.664 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6452 - accuracy: 0.6406\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6247 - accuracy: 0.6666\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.667 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6524 - accuracy: 0.6256\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6359 - accuracy: 0.6635\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.663 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6517 - accuracy: 0.6224\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6200 - accuracy: 0.6679\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.668 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6510 - accuracy: 0.6276\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6254 - accuracy: 0.6673\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.667 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6476 - accuracy: 0.6375\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6325 - accuracy: 0.6630\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.663 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6542 - accuracy: 0.6173\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6189 - accuracy: 0.6815\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.682 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6504 - accuracy: 0.6256\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6197 - accuracy: 0.6741\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.674 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6446 - accuracy: 0.6446\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6363 - accuracy: 0.6478\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.648 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6513 - accuracy: 0.6273\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6226 - accuracy: 0.6780\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.678 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6549 - accuracy: 0.6169\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6252 - accuracy: 0.6748\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.675 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6487 - accuracy: 0.6348\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6340 - accuracy: 0.6558\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.656 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6559 - accuracy: 0.6223\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6248 - accuracy: 0.6727\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.673 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6519 - accuracy: 0.6269\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6258 - accuracy: 0.6777\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.678 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6492 - accuracy: 0.6332\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6291 - accuracy: 0.6695\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.669 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6481 - accuracy: 0.6345\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6186 - accuracy: 0.6807\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.681 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6491 - accuracy: 0.6344\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6237 - accuracy: 0.6749\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.675 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6509 - accuracy: 0.6320\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.6648\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.665 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6474 - accuracy: 0.6345\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6112 - accuracy: 0.6801\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.680 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6519 - accuracy: 0.6209\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6189 - accuracy: 0.6714\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.671 total time=   8.1s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6484 - accuracy: 0.6369\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6283 - accuracy: 0.6674\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.667 total time=   9.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6530 - accuracy: 0.6290\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6209 - accuracy: 0.6745\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.675 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6476 - accuracy: 0.6354\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6177 - accuracy: 0.6772\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.677 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6491 - accuracy: 0.6315\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6667\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.667 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6551 - accuracy: 0.6170\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6153 - accuracy: 0.6804\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.680 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6467 - accuracy: 0.6374\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6211 - accuracy: 0.6754\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.675 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6421 - accuracy: 0.6402\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6206 - accuracy: 0.6721\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.672 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6509 - accuracy: 0.6298\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6115 - accuracy: 0.6861\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.686 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6497 - accuracy: 0.6338\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6223 - accuracy: 0.6770\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.677 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6517 - accuracy: 0.6246\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6392 - accuracy: 0.6730\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.673 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6510 - accuracy: 0.6266\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6319 - accuracy: 0.6758\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.676 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6515 - accuracy: 0.6278\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6277 - accuracy: 0.6563\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.656 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6474 - accuracy: 0.6309\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6236 - accuracy: 0.6701\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.670 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6494 - accuracy: 0.6322\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6126 - accuracy: 0.6847\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.685 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6440 - accuracy: 0.6401\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6236 - accuracy: 0.6786\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.679 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6469 - accuracy: 0.6357\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6220 - accuracy: 0.6713\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.671 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6548 - accuracy: 0.6176\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6797\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.680 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6484 - accuracy: 0.6311\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6240 - accuracy: 0.6705\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.671 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6498 - accuracy: 0.6326\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6282 - accuracy: 0.6659\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.666 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6487 - accuracy: 0.6326\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6245 - accuracy: 0.6787\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.679 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6465 - accuracy: 0.6358\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6331 - accuracy: 0.6597\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.660 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6466 - accuracy: 0.6333\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6244 - accuracy: 0.6672\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.667 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6491 - accuracy: 0.6311\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6153 - accuracy: 0.6830\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.683 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6512 - accuracy: 0.6242\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6245 - accuracy: 0.6780\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.678 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6414 - accuracy: 0.6409\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6244 - accuracy: 0.6760\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.676 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6485 - accuracy: 0.6305\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6170 - accuracy: 0.6827\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.683 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6512 - accuracy: 0.6298\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6214 - accuracy: 0.6767\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6464 - accuracy: 0.6419\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6356 - accuracy: 0.6597\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.660 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6534 - accuracy: 0.6251\n",
      "354/354 [==============================] - 3s 4ms/step - loss: 0.6154 - accuracy: 0.6830\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.683 total time=   9.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6504 - accuracy: 0.6323\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6182 - accuracy: 0.6819\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.682 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6455 - accuracy: 0.6378\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6353 - accuracy: 0.6471\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.647 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6530 - accuracy: 0.6254\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6204 - accuracy: 0.6811\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.681 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6475 - accuracy: 0.6349\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6348 - accuracy: 0.6390\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.639 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6462 - accuracy: 0.6348\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6222 - accuracy: 0.6710\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.671 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6485 - accuracy: 0.6357\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6161 - accuracy: 0.6824\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.682 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6442 - accuracy: 0.6395\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6359 - accuracy: 0.6549\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.655 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6475 - accuracy: 0.6312\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6290 - accuracy: 0.6598\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.660 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6502 - accuracy: 0.6322\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6160 - accuracy: 0.6808\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.681 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6549 - accuracy: 0.6169\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6323 - accuracy: 0.6712\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6436 - accuracy: 0.6413\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6203 - accuracy: 0.6729\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.673 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6469 - accuracy: 0.6377\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6198 - accuracy: 0.6834\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.683 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6470 - accuracy: 0.6377\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6283 - accuracy: 0.6709\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.671 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6411 - accuracy: 0.6426\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6303 - accuracy: 0.6532\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.653 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6490 - accuracy: 0.6345\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6230 - accuracy: 0.6650\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.665 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6488 - accuracy: 0.6323\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6234 - accuracy: 0.6735\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6474 - accuracy: 0.6406\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6305 - accuracy: 0.6589\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.659 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6525 - accuracy: 0.6298\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6164 - accuracy: 0.6757\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.676 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6458 - accuracy: 0.6415\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6211 - accuracy: 0.6711\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.671 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6439 - accuracy: 0.6435\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6358 - accuracy: 0.6468\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.647 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6476 - accuracy: 0.6377\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6262 - accuracy: 0.6675\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.667 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6496 - accuracy: 0.6305\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6231 - accuracy: 0.6700\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6451 - accuracy: 0.6417\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6303 - accuracy: 0.6627\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.663 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6502 - accuracy: 0.6339\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6249 - accuracy: 0.6635\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.663 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6427 - accuracy: 0.6416\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6223 - accuracy: 0.6739\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6447 - accuracy: 0.6467\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6237 - accuracy: 0.6726\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.673 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6515 - accuracy: 0.6326\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6115 - accuracy: 0.6826\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.683 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6507 - accuracy: 0.6321\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6262 - accuracy: 0.6596\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.660 total time=   8.1s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6454 - accuracy: 0.6375\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6665\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.666 total time=   9.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6516 - accuracy: 0.6332\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6172 - accuracy: 0.6767\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6479 - accuracy: 0.6351\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6308 - accuracy: 0.6651\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.665 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6433 - accuracy: 0.6440\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6638\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.664 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6444 - accuracy: 0.6417\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6261 - accuracy: 0.6759\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.676 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6477 - accuracy: 0.6355\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6215 - accuracy: 0.6772\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.677 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6525 - accuracy: 0.6210\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6231 - accuracy: 0.6722\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6531 - accuracy: 0.6266\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6435 - accuracy: 0.6517\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.652 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6495 - accuracy: 0.6321\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6202 - accuracy: 0.6771\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.677 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6460 - accuracy: 0.6386\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6666\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.667 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6487 - accuracy: 0.6341\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6140 - accuracy: 0.6817\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.682 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6471 - accuracy: 0.6299\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6231 - accuracy: 0.6794\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.679 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6428 - accuracy: 0.6432\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6377 - accuracy: 0.6549\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.655 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6471 - accuracy: 0.6353\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6369 - accuracy: 0.6471\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.647 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6505 - accuracy: 0.6245\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6172 - accuracy: 0.6840\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.684 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6483 - accuracy: 0.6333\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6588\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.659 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6512 - accuracy: 0.6275\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6206 - accuracy: 0.6854\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.685 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6512 - accuracy: 0.6289\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6248 - accuracy: 0.6696\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.670 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6444 - accuracy: 0.6447\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6262 - accuracy: 0.6684\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.668 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6468 - accuracy: 0.6344\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6189 - accuracy: 0.6731\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.673 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6440 - accuracy: 0.6382\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6231 - accuracy: 0.6639\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.664 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6415 - accuracy: 0.6435\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6346 - accuracy: 0.6422\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.642 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6481 - accuracy: 0.6340\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6145 - accuracy: 0.6808\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.681 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6472 - accuracy: 0.6309\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6212 - accuracy: 0.6723\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.672 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6490 - accuracy: 0.6306\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6255 - accuracy: 0.6749\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.675 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6565 - accuracy: 0.6182\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6161 - accuracy: 0.6769\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.677 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6469 - accuracy: 0.6344\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6221 - accuracy: 0.6694\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6494 - accuracy: 0.6319\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6319 - accuracy: 0.6713\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.671 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6487 - accuracy: 0.6325\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6130 - accuracy: 0.6858\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.686 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6458 - accuracy: 0.6429\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6207 - accuracy: 0.6783\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.678 total time=   8.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6442 - accuracy: 0.6402\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6247 - accuracy: 0.6728\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6491 - accuracy: 0.6280\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6177 - accuracy: 0.6852\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.685 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6458 - accuracy: 0.6374\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6226 - accuracy: 0.6767\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.677 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6471 - accuracy: 0.6392\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6285 - accuracy: 0.6685\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.669 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6492 - accuracy: 0.6344\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6285 - accuracy: 0.6814\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.681 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6444 - accuracy: 0.6406\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6214 - accuracy: 0.6732\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.673 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6463 - accuracy: 0.6383\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6307 - accuracy: 0.6567\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.657 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6493 - accuracy: 0.6343\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6203 - accuracy: 0.6837\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.684 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6477 - accuracy: 0.6341\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6190 - accuracy: 0.6812\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.681 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6406 - accuracy: 0.6461\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6418 - accuracy: 0.6620\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.662 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6490 - accuracy: 0.6375\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6134 - accuracy: 0.6806\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.681 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6487 - accuracy: 0.6316\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6306 - accuracy: 0.6680\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6476 - accuracy: 0.6325\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6320 - accuracy: 0.6519\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.652 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6473 - accuracy: 0.6379\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6140 - accuracy: 0.6857\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.686 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6436 - accuracy: 0.6397\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6233 - accuracy: 0.6655\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.665 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6447 - accuracy: 0.6427\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6238 - accuracy: 0.6722\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.672 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6463 - accuracy: 0.6407\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6123 - accuracy: 0.6816\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.682 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6460 - accuracy: 0.6392\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6183 - accuracy: 0.6780\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.678 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6443 - accuracy: 0.6410\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6216 - accuracy: 0.6730\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.673 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6469 - accuracy: 0.6360\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6171 - accuracy: 0.6811\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.681 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6451 - accuracy: 0.6376\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6461 - accuracy: 0.6458\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.646 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6513 - accuracy: 0.6310\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6278 - accuracy: 0.6711\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.671 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6485 - accuracy: 0.6364\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6267 - accuracy: 0.6838\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.684 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6561 - accuracy: 0.6120\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6182 - accuracy: 0.6775\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.678 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6418 - accuracy: 0.6458\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6254 - accuracy: 0.6706\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6487 - accuracy: 0.6340\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6431 - accuracy: 0.6526\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.653 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6416 - accuracy: 0.6430\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6179 - accuracy: 0.6778\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.678 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6468 - accuracy: 0.6376\n",
      "354/354 [==============================] - 3s 4ms/step - loss: 0.6272 - accuracy: 0.6701\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.670 total time=   9.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6541 - accuracy: 0.6240\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6172 - accuracy: 0.6791\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.679 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6434 - accuracy: 0.6446\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6528 - accuracy: 0.6166\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.617 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6427 - accuracy: 0.6400\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6299 - accuracy: 0.6566\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.657 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6497 - accuracy: 0.6323\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6187 - accuracy: 0.6751\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.675 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6424 - accuracy: 0.6432\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6191 - accuracy: 0.6798\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.680 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6407 - accuracy: 0.6452\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6398 - accuracy: 0.6549\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.655 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6481 - accuracy: 0.6361\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6172 - accuracy: 0.6757\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.676 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6433 - accuracy: 0.6449\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6292 - accuracy: 0.6570\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.657 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6439 - accuracy: 0.6420\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6216 - accuracy: 0.6706\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6456 - accuracy: 0.6421\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6156 - accuracy: 0.6764\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.676 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6457 - accuracy: 0.6389\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6213 - accuracy: 0.6748\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.675 total time=   8.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6468 - accuracy: 0.6406\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6340 - accuracy: 0.6579\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.658 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6486 - accuracy: 0.6353\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6187 - accuracy: 0.6815\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.682 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6545 - accuracy: 0.6211\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6251 - accuracy: 0.6750\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6469 - accuracy: 0.6351\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6283 - accuracy: 0.6647\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.665 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6528 - accuracy: 0.6233\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6240 - accuracy: 0.6607\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.661 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6461 - accuracy: 0.6367\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6340 - accuracy: 0.6494\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.649 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6471 - accuracy: 0.6353\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6262 - accuracy: 0.6650\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.665 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6459 - accuracy: 0.6385\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6184 - accuracy: 0.6814\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.681 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6445 - accuracy: 0.6397\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6311 - accuracy: 0.6663\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.666 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6528 - accuracy: 0.6283\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6251 - accuracy: 0.6713\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.671 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6542 - accuracy: 0.6270\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6257 - accuracy: 0.6704\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.670 total time=   8.4s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6577 - accuracy: 0.6142\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6304 - accuracy: 0.6665\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.667 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6464 - accuracy: 0.6382\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6321 - accuracy: 0.6535\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.654 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6539 - accuracy: 0.6240\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6199 - accuracy: 0.6821\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.682 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6517 - accuracy: 0.6262\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6197 - accuracy: 0.6764\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.676 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6496 - accuracy: 0.6322\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6348 - accuracy: 0.6585\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.659 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6591 - accuracy: 0.6081\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6159 - accuracy: 0.6818\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.682 total time=   7.8s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6464 - accuracy: 0.6360\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6781\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.678 total time=   9.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6572 - accuracy: 0.6155\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6226 - accuracy: 0.6729\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.673 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6584 - accuracy: 0.6169\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6434 - accuracy: 0.6310\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.631 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6537 - accuracy: 0.6313\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6268 - accuracy: 0.6713\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.671 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6453 - accuracy: 0.6375\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6259 - accuracy: 0.6618\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.662 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6514 - accuracy: 0.6267\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6223 - accuracy: 0.6761\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.676 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6497 - accuracy: 0.6257\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6254 - accuracy: 0.6654\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.665 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6489 - accuracy: 0.6288\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6289 - accuracy: 0.6679\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6529 - accuracy: 0.6239\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6210 - accuracy: 0.6774\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.677 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6469 - accuracy: 0.6335\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6375 - accuracy: 0.6453\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.645 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6481 - accuracy: 0.6375\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6233 - accuracy: 0.6759\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.676 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6525 - accuracy: 0.6262\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6158 - accuracy: 0.6806\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.681 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6497 - accuracy: 0.6323\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6214 - accuracy: 0.6744\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.674 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6447 - accuracy: 0.6409\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6697\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6505 - accuracy: 0.6292\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6148 - accuracy: 0.6827\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.683 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6521 - accuracy: 0.6241\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6192 - accuracy: 0.6771\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.677 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6469 - accuracy: 0.6379\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6420 - accuracy: 0.6442\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.644 total time=   8.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6503 - accuracy: 0.6281\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6108 - accuracy: 0.6832\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.683 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6500 - accuracy: 0.6297\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6241 - accuracy: 0.6746\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.675 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6507 - accuracy: 0.6257\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6219 - accuracy: 0.6747\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.675 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6541 - accuracy: 0.6239\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6209 - accuracy: 0.6827\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.683 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6491 - accuracy: 0.6330\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6343 - accuracy: 0.6491\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.649 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6451 - accuracy: 0.6330\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6309 - accuracy: 0.6724\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.672 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6513 - accuracy: 0.6311\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6126 - accuracy: 0.6857\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.686 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6546 - accuracy: 0.6224\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6269 - accuracy: 0.6733\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.673 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6501 - accuracy: 0.6311\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6207 - accuracy: 0.6743\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.674 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6519 - accuracy: 0.6284\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6211 - accuracy: 0.6849\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.685 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6494 - accuracy: 0.6288\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6177 - accuracy: 0.6783\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.678 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6506 - accuracy: 0.6273\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6270 - accuracy: 0.6678\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.668 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6506 - accuracy: 0.6298\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6408 - accuracy: 0.6520\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.652 total time=   9.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6498 - accuracy: 0.6353\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6539 - accuracy: 0.6105\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.610 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6447 - accuracy: 0.6397\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6275 - accuracy: 0.6628\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.663 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6540 - accuracy: 0.6283\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6186 - accuracy: 0.6821\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.682 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6495 - accuracy: 0.6347\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6185 - accuracy: 0.6753\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.675 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6485 - accuracy: 0.6321\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6286 - accuracy: 0.6648\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.665 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6507 - accuracy: 0.6316\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6159 - accuracy: 0.6842\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.684 total time=   8.4s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6468 - accuracy: 0.6362\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6191 - accuracy: 0.6776\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.678 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6477 - accuracy: 0.6344\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6737\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.674 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6567 - accuracy: 0.6190\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6183 - accuracy: 0.6859\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.686 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6529 - accuracy: 0.6220\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6208 - accuracy: 0.6709\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.671 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6455 - accuracy: 0.6398\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6295 - accuracy: 0.6642\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.664 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6540 - accuracy: 0.6262\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.7058 - accuracy: 0.5570\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.557 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6492 - accuracy: 0.6279\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6223 - accuracy: 0.6721\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.672 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6453 - accuracy: 0.6362\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6308 - accuracy: 0.6625\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.663 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6496 - accuracy: 0.6317\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6231 - accuracy: 0.6737\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6501 - accuracy: 0.6305\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6265 - accuracy: 0.6697\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.670 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6525 - accuracy: 0.6283\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6241 - accuracy: 0.6688\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6543 - accuracy: 0.6247\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6186 - accuracy: 0.6681\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6519 - accuracy: 0.6277\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6191 - accuracy: 0.6802\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.680 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6494 - accuracy: 0.6312\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6396 - accuracy: 0.6419\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.642 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6510 - accuracy: 0.6277\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6495 - accuracy: 0.6774\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.677 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6472 - accuracy: 0.6365\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6391 - accuracy: 0.6456\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.646 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6481 - accuracy: 0.6342\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6246 - accuracy: 0.6765\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.676 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6469 - accuracy: 0.6362\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6211 - accuracy: 0.6744\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.674 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6563 - accuracy: 0.6210\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6328 - accuracy: 0.6620\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.662 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6478 - accuracy: 0.6320\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6265 - accuracy: 0.6645\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.664 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6534 - accuracy: 0.6261\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6249 - accuracy: 0.6629\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.663 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6503 - accuracy: 0.6312\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6384 - accuracy: 0.6743\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.674 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6480 - accuracy: 0.6303\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6194 - accuracy: 0.6776\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.678 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6489 - accuracy: 0.6322\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6161 - accuracy: 0.6812\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.681 total time=   9.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6492 - accuracy: 0.6308\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6249 - accuracy: 0.6752\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.675 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6449 - accuracy: 0.6397\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6395 - accuracy: 0.6586\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.659 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6506 - accuracy: 0.6326\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6144 - accuracy: 0.6863\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.686 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6456 - accuracy: 0.6359\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6180 - accuracy: 0.6787\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.679 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6501 - accuracy: 0.6300\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6225 - accuracy: 0.6695\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6515 - accuracy: 0.6298\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6162 - accuracy: 0.6794\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.679 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6528 - accuracy: 0.6266\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6213 - accuracy: 0.6734\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.673 total time=   8.4s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6423 - accuracy: 0.6462\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6205 - accuracy: 0.6764\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.676 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6503 - accuracy: 0.6264\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6322 - accuracy: 0.6537\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.654 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6437 - accuracy: 0.6397\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6218 - accuracy: 0.6696\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6459 - accuracy: 0.6344\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6265 - accuracy: 0.6713\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.671 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6476 - accuracy: 0.6353\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6101 - accuracy: 0.6840\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.684 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6457 - accuracy: 0.6391\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6231 - accuracy: 0.6751\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.675 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6506 - accuracy: 0.6293\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6208 - accuracy: 0.6680\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6558 - accuracy: 0.6199\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6235 - accuracy: 0.6616\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.662 total time=   8.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6491 - accuracy: 0.6286\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6307 - accuracy: 0.6416\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.642 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6503 - accuracy: 0.6288\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6230 - accuracy: 0.6736\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6527 - accuracy: 0.6246\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6150 - accuracy: 0.6790\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.679 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6586 - accuracy: 0.6120\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6254 - accuracy: 0.6777\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.678 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6472 - accuracy: 0.6336\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6445 - accuracy: 0.6456\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.646 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6546 - accuracy: 0.6178\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6367 - accuracy: 0.6521\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.652 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6512 - accuracy: 0.6265\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6206 - accuracy: 0.6795\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.680 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6524 - accuracy: 0.6221\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6592 - accuracy: 0.6318\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.632 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6498 - accuracy: 0.6295\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6235 - accuracy: 0.6720\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6571 - accuracy: 0.6151\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6371 - accuracy: 0.6502\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.650 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6497 - accuracy: 0.6306\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6275 - accuracy: 0.6689\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.669 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6527 - accuracy: 0.6266\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6128 - accuracy: 0.6866\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.687 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6491 - accuracy: 0.6352\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6285 - accuracy: 0.6767\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.677 total time=   8.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6456 - accuracy: 0.6377\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6361 - accuracy: 0.6527\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.653 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6493 - accuracy: 0.6378\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6184 - accuracy: 0.6875\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.688 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6462 - accuracy: 0.6361\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6206 - accuracy: 0.6757\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.676 total time=   8.4s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6514 - accuracy: 0.6314\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6447 - accuracy: 0.6386\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.639 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6527 - accuracy: 0.6283\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6193 - accuracy: 0.6715\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.672 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6491 - accuracy: 0.6321\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6268 - accuracy: 0.6726\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.673 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6444 - accuracy: 0.6438\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6272 - accuracy: 0.6632\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.663 total time=   8.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6494 - accuracy: 0.6304\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6242 - accuracy: 0.6708\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6475 - accuracy: 0.6366\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6219 - accuracy: 0.6735\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6429 - accuracy: 0.6435\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6310 - accuracy: 0.6581\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.658 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6541 - accuracy: 0.6216\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6179 - accuracy: 0.6784\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.678 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6480 - accuracy: 0.6369\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6357 - accuracy: 0.6526\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.653 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6534 - accuracy: 0.6228\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6234 - accuracy: 0.6708\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.671 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6569 - accuracy: 0.6168\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6165 - accuracy: 0.6834\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.683 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6527 - accuracy: 0.6241\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6312 - accuracy: 0.6590\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.659 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6479 - accuracy: 0.6326\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6216 - accuracy: 0.6736\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6529 - accuracy: 0.6249\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6131 - accuracy: 0.6823\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.682 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6501 - accuracy: 0.6277\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6374 - accuracy: 0.6558\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.656 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6444 - accuracy: 0.6418\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6479 - accuracy: 0.6316\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.632 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6483 - accuracy: 0.6326\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6158 - accuracy: 0.6847\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.685 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6444 - accuracy: 0.6428\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6193 - accuracy: 0.6765\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6486 - accuracy: 0.6306\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6319 - accuracy: 0.6667\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.667 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6500 - accuracy: 0.6325\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6136 - accuracy: 0.6815\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.682 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6519 - accuracy: 0.6263\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6270 - accuracy: 0.6729\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.673 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6445 - accuracy: 0.6415\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6238 - accuracy: 0.6737\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.674 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6522 - accuracy: 0.6246\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6266 - accuracy: 0.6761\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.676 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6479 - accuracy: 0.6292\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6326 - accuracy: 0.6493\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.649 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6467 - accuracy: 0.6363\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.6606\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.661 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6512 - accuracy: 0.6307\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6100 - accuracy: 0.6838\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.684 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6437 - accuracy: 0.6396\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6231 - accuracy: 0.6720\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.672 total time=   9.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6441 - accuracy: 0.6395\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6266 - accuracy: 0.6697\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.670 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6528 - accuracy: 0.6248\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6311 - accuracy: 0.6546\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.655 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6438 - accuracy: 0.6415\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6381 - accuracy: 0.6386\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.639 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6448 - accuracy: 0.6372\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6261 - accuracy: 0.6684\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.668 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6498 - accuracy: 0.6316\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6202 - accuracy: 0.6849\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.685 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6471 - accuracy: 0.6382\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6274 - accuracy: 0.6733\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.673 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6475 - accuracy: 0.6350\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6584 - accuracy: 0.5976\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.598 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6492 - accuracy: 0.6347\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6184 - accuracy: 0.6797\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.680 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6443 - accuracy: 0.6405\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6317 - accuracy: 0.6583\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.658 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6498 - accuracy: 0.6364\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6312 - accuracy: 0.6658\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.666 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6533 - accuracy: 0.6259\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6143 - accuracy: 0.6819\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.682 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6542 - accuracy: 0.6249\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6189 - accuracy: 0.6773\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6473 - accuracy: 0.6339\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6301 - accuracy: 0.6600\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.660 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6507 - accuracy: 0.6262\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6291 - accuracy: 0.6669\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.667 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6491 - accuracy: 0.6342\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6285 - accuracy: 0.6740\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6431 - accuracy: 0.6429\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6301 - accuracy: 0.6574\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.657 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6491 - accuracy: 0.6356\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6665\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.666 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6465 - accuracy: 0.6374\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6436 - accuracy: 0.6711\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.671 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6479 - accuracy: 0.6376\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6229 - accuracy: 0.6736\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.674 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6549 - accuracy: 0.6219\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6164 - accuracy: 0.6811\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.681 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6463 - accuracy: 0.6381\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6212 - accuracy: 0.6755\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.675 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6502 - accuracy: 0.6308\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6267 - accuracy: 0.6684\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.668 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6496 - accuracy: 0.6306\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6105 - accuracy: 0.6826\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.683 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6447 - accuracy: 0.6410\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6666\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.667 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6453 - accuracy: 0.6352\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6219 - accuracy: 0.6680\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6458 - accuracy: 0.6377\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6120 - accuracy: 0.6797\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.680 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6439 - accuracy: 0.6419\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6318 - accuracy: 0.6733\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.673 total time=   8.0s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6441 - accuracy: 0.6409\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6208 - accuracy: 0.6744\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.674 total time=   8.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6508 - accuracy: 0.6316\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6208 - accuracy: 0.6781\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.678 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6509 - accuracy: 0.6338\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6284 - accuracy: 0.6711\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.671 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6497 - accuracy: 0.6335\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6267 - accuracy: 0.6664\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.666 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6507 - accuracy: 0.6351\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6141 - accuracy: 0.6825\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.682 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6451 - accuracy: 0.6413\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6336 - accuracy: 0.6619\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.662 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6438 - accuracy: 0.6405\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6345 - accuracy: 0.6706\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.671 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6488 - accuracy: 0.6320\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6497 - accuracy: 0.6254\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.625 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6461 - accuracy: 0.6390\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6241 - accuracy: 0.6729\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.673 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6487 - accuracy: 0.6337\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6225 - accuracy: 0.6689\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.669 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6522 - accuracy: 0.6324\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6193 - accuracy: 0.6779\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.678 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6496 - accuracy: 0.6351\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6223 - accuracy: 0.6759\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.676 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6489 - accuracy: 0.6327\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6621 - accuracy: 0.6024\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.602 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6552 - accuracy: 0.6233\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6263 - accuracy: 0.6788\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.679 total time=   8.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6474 - accuracy: 0.6347\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6292 - accuracy: 0.6563\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.656 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6505 - accuracy: 0.6287\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6311 - accuracy: 0.6607\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.661 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6494 - accuracy: 0.6328\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6282 - accuracy: 0.6661\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.666 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6436 - accuracy: 0.6392\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6196 - accuracy: 0.6774\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.677 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6459 - accuracy: 0.6407\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6232 - accuracy: 0.6697\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.670 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6521 - accuracy: 0.6318\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6268 - accuracy: 0.6703\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.670 total time=   8.5s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6499 - accuracy: 0.6301\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6188 - accuracy: 0.6755\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.675 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6458 - accuracy: 0.6411\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6295 - accuracy: 0.6702\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.670 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6561 - accuracy: 0.6171\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6358 - accuracy: 0.6554\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.655 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6499 - accuracy: 0.6312\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6298 - accuracy: 0.6545\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.655 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6422 - accuracy: 0.6413\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6302 - accuracy: 0.6729\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.673 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6446 - accuracy: 0.6393\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6149 - accuracy: 0.6730\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.673 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6467 - accuracy: 0.6331\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6185 - accuracy: 0.6750\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.675 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6437 - accuracy: 0.6407\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6198 - accuracy: 0.6760\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.676 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6502 - accuracy: 0.6321\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6218 - accuracy: 0.6644\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.664 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6485 - accuracy: 0.6341\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6224 - accuracy: 0.6764\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.676 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6464 - accuracy: 0.6310\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6277 - accuracy: 0.6752\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.675 total time=   9.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6507 - accuracy: 0.6323\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6200 - accuracy: 0.6737\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6448 - accuracy: 0.6372\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6239 - accuracy: 0.6658\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.666 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6438 - accuracy: 0.6419\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6334 - accuracy: 0.6539\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.654 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6465 - accuracy: 0.6377\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6182 - accuracy: 0.6826\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.683 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6451 - accuracy: 0.6362\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6208 - accuracy: 0.6765\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.677 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6489 - accuracy: 0.6375\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6258 - accuracy: 0.6712\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.671 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6542 - accuracy: 0.6264\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6183 - accuracy: 0.6789\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.679 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6479 - accuracy: 0.6315\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6243 - accuracy: 0.6804\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.680 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6463 - accuracy: 0.6383\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6321 - accuracy: 0.6628\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.663 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6525 - accuracy: 0.6252\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6139 - accuracy: 0.6823\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.682 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6470 - accuracy: 0.6401\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6647\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.665 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6408 - accuracy: 0.6440\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6289 - accuracy: 0.6677\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.668 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6485 - accuracy: 0.6356\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6113 - accuracy: 0.6839\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.684 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6470 - accuracy: 0.6312\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6208 - accuracy: 0.6732\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6446 - accuracy: 0.6445\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6207 - accuracy: 0.6737\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6503 - accuracy: 0.6307\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6167 - accuracy: 0.6827\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.683 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6485 - accuracy: 0.6353\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6352 - accuracy: 0.6519\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.652 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6414 - accuracy: 0.6439\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6464 - accuracy: 0.6444\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.644 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6478 - accuracy: 0.6344\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6220 - accuracy: 0.6704\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.670 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6414 - accuracy: 0.6452\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6196 - accuracy: 0.6761\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.676 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6404 - accuracy: 0.6444\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6484 - accuracy: 0.6601\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.660 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6493 - accuracy: 0.6348\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6173 - accuracy: 0.6795\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.679 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6441 - accuracy: 0.6410\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6288 - accuracy: 0.6756\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.676 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6449 - accuracy: 0.6407\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6384 - accuracy: 0.6393\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.639 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6490 - accuracy: 0.6341\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6119 - accuracy: 0.6825\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.682 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6503 - accuracy: 0.6325\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6310 - accuracy: 0.6721\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.672 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6469 - accuracy: 0.6356\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6216 - accuracy: 0.6730\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.673 total time=   8.1s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6481 - accuracy: 0.6324\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6106 - accuracy: 0.6826\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.683 total time=   9.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6475 - accuracy: 0.6365\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6217 - accuracy: 0.6741\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6443 - accuracy: 0.6383\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6378 - accuracy: 0.6608\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.661 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6465 - accuracy: 0.6375\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6237 - accuracy: 0.6819\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.682 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6441 - accuracy: 0.6391\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6304 - accuracy: 0.6737\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6486 - accuracy: 0.6357\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6429 - accuracy: 0.6405\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.641 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6482 - accuracy: 0.6389\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6282 - accuracy: 0.6603\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.660 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6517 - accuracy: 0.6300\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6216 - accuracy: 0.6715\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6471 - accuracy: 0.6359\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6319 - accuracy: 0.6631\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.663 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6490 - accuracy: 0.6375\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6097 - accuracy: 0.6846\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.685 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6448 - accuracy: 0.6423\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6300 - accuracy: 0.6757\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.676 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6445 - accuracy: 0.6421\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6281 - accuracy: 0.6628\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.663 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6505 - accuracy: 0.6297\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6163 - accuracy: 0.6721\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.672 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6447 - accuracy: 0.6378\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6495 - accuracy: 0.6462\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.646 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6439 - accuracy: 0.6472\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6209 - accuracy: 0.6744\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.674 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6511 - accuracy: 0.6326\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6165 - accuracy: 0.6831\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.683 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6486 - accuracy: 0.6343\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6505\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.650 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6422 - accuracy: 0.6458\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6484 - accuracy: 0.6418\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.642 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6486 - accuracy: 0.6311\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6140 - accuracy: 0.6864\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.686 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6427 - accuracy: 0.6414\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6608 - accuracy: 0.6342\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.634 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6419 - accuracy: 0.6456\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6255 - accuracy: 0.6682\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6454 - accuracy: 0.6375\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6158 - accuracy: 0.6763\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.676 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6455 - accuracy: 0.6396\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6243 - accuracy: 0.6765\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.677 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6480 - accuracy: 0.6366\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6414 - accuracy: 0.6410\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.641 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6497 - accuracy: 0.6370\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6304 - accuracy: 0.6723\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.672 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6483 - accuracy: 0.6409\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6185 - accuracy: 0.6787\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.679 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6429 - accuracy: 0.6447\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6279 - accuracy: 0.6706\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   8.4s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6463 - accuracy: 0.6410\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6115 - accuracy: 0.6824\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.682 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6442 - accuracy: 0.6412\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6480 - accuracy: 0.6249\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.625 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6432 - accuracy: 0.6443\n",
      "354/354 [==============================] - 3s 4ms/step - loss: 0.6319 - accuracy: 0.6719\n",
      "[CV 1/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.672 total time=   9.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6461 - accuracy: 0.6332\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6135 - accuracy: 0.6807\n",
      "[CV 2/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.681 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6439 - accuracy: 0.6387\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6292 - accuracy: 0.6673\n",
      "[CV 3/3] END activation=softmax, dropout=0.2, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6969 - accuracy: 0.5076\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6909 - accuracy: 0.5517\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.552 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6964 - accuracy: 0.5076\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6925 - accuracy: 0.5153\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.515 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6987 - accuracy: 0.5038\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6924 - accuracy: 0.5014\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.501 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6934 - accuracy: 0.5193\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6835 - accuracy: 0.6253\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.625 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6969 - accuracy: 0.5121\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.6484\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.648 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6957 - accuracy: 0.5040\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6916 - accuracy: 0.5616\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.562 total time=   8.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6953 - accuracy: 0.4998\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6915 - accuracy: 0.5718\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.572 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6937 - accuracy: 0.5202\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6862 - accuracy: 0.6562\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.656 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6930 - accuracy: 0.5125\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6874 - accuracy: 0.5016\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.502 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6947 - accuracy: 0.5145\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6832 - accuracy: 0.6213\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.621 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6948 - accuracy: 0.5031\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6907 - accuracy: 0.5034\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.503 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6930 - accuracy: 0.5228\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6854 - accuracy: 0.6462\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.646 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6950 - accuracy: 0.5128\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6896 - accuracy: 0.5987\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.599 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6967 - accuracy: 0.5059\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6911 - accuracy: 0.5692\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.569 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6947 - accuracy: 0.5174\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6858 - accuracy: 0.5884\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.588 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6958 - accuracy: 0.5046\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6891 - accuracy: 0.6312\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.631 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6922 - accuracy: 0.5177\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6779 - accuracy: 0.6376\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.638 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6981 - accuracy: 0.5040\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6905 - accuracy: 0.6017\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.602 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6954 - accuracy: 0.5031\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6921 - accuracy: 0.5995\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.600 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6911 - accuracy: 0.5255\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6797 - accuracy: 0.6166\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.617 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6912 - accuracy: 0.5295\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6760 - accuracy: 0.6266\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.627 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6938 - accuracy: 0.5131\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6887 - accuracy: 0.5775\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.577 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6963 - accuracy: 0.4995\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5790\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.579 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6910 - accuracy: 0.5303\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6694 - accuracy: 0.6518\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.652 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6903 - accuracy: 0.5305\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6721 - accuracy: 0.6536\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.654 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6940 - accuracy: 0.5100\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6884 - accuracy: 0.5629\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.563 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6944 - accuracy: 0.5242\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6812 - accuracy: 0.6039\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.604 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6943 - accuracy: 0.5085\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5859\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.586 total time=   9.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6961 - accuracy: 0.5036\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6915 - accuracy: 0.5188\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.519 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6924 - accuracy: 0.5246\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6787 - accuracy: 0.6711\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.671 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6972 - accuracy: 0.5037\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5635\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.564 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6930 - accuracy: 0.5163\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6829 - accuracy: 0.6290\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.629 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6936 - accuracy: 0.5096\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6908 - accuracy: 0.5772\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.577 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6891 - accuracy: 0.5378\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6702 - accuracy: 0.6383\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.638 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6942 - accuracy: 0.5009\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6886 - accuracy: 0.5439\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.544 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6946 - accuracy: 0.5163\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6880 - accuracy: 0.5504\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.550 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6929 - accuracy: 0.5230\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6817 - accuracy: 0.6110\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.611 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6918 - accuracy: 0.5241\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6758 - accuracy: 0.6524\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.652 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6926 - accuracy: 0.5144\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6846 - accuracy: 0.6443\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.644 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6959 - accuracy: 0.5058\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5491\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.549 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6952 - accuracy: 0.5138\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.5814\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.581 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6991 - accuracy: 0.4992\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6930 - accuracy: 0.5030\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.503 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6927 - accuracy: 0.5193\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6853 - accuracy: 0.6104\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.610 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6930 - accuracy: 0.5175\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6841 - accuracy: 0.6176\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.618 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6946 - accuracy: 0.5142\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6869 - accuracy: 0.5689\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.569 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6926 - accuracy: 0.5165\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6804 - accuracy: 0.5890\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.589 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6928 - accuracy: 0.5211\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6856 - accuracy: 0.6207\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.621 total time=   8.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6931 - accuracy: 0.5178\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6861 - accuracy: 0.5950\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.595 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6943 - accuracy: 0.5144\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6840 - accuracy: 0.5748\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.575 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6923 - accuracy: 0.5190\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6794 - accuracy: 0.6483\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.648 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6950 - accuracy: 0.5083\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6899 - accuracy: 0.5646\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.565 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6962 - accuracy: 0.5063\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6905 - accuracy: 0.5516\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.552 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6910 - accuracy: 0.5274\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6692 - accuracy: 0.6146\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.615 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6900 - accuracy: 0.5386\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6655 - accuracy: 0.6606\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.661 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6931 - accuracy: 0.5134\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6884 - accuracy: 0.6092\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.609 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6960 - accuracy: 0.4989\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6927 - accuracy: 0.5811\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.581 total time=   7.6s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6944 - accuracy: 0.5038\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6919 - accuracy: 0.6257\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.626 total time=   8.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6857 - accuracy: 0.5511\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6546 - accuracy: 0.6441\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.644 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6944 - accuracy: 0.5012\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6925 - accuracy: 0.5014\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.501 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6894 - accuracy: 0.5283\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6733 - accuracy: 0.6557\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.656 total time=   8.4s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6899 - accuracy: 0.5362\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6682 - accuracy: 0.6277\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.628 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6930 - accuracy: 0.5214\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6793 - accuracy: 0.6475\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.648 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6904 - accuracy: 0.5267\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6681 - accuracy: 0.6626\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.663 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6912 - accuracy: 0.5283\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6736 - accuracy: 0.6335\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.634 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6953 - accuracy: 0.5109\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6892 - accuracy: 0.4989\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.499 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6929 - accuracy: 0.5149\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6866 - accuracy: 0.5819\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.582 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6939 - accuracy: 0.5101\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6884 - accuracy: 0.6412\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.641 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6932 - accuracy: 0.5150\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6843 - accuracy: 0.5474\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.547 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6944 - accuracy: 0.5108\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6900 - accuracy: 0.5548\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.555 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6915 - accuracy: 0.5210\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6789 - accuracy: 0.6357\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.636 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6929 - accuracy: 0.5163\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6836 - accuracy: 0.6087\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.609 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6936 - accuracy: 0.5151\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6825 - accuracy: 0.6539\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.654 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6901 - accuracy: 0.5354\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6683 - accuracy: 0.6419\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.642 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6925 - accuracy: 0.5223\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6803 - accuracy: 0.6010\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.601 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6915 - accuracy: 0.5303\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6770 - accuracy: 0.6523\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.652 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6851 - accuracy: 0.5508\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6603 - accuracy: 0.6071\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.607 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6947 - accuracy: 0.5179\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.6391\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.639 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6928 - accuracy: 0.5190\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6854 - accuracy: 0.6152\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.615 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6892 - accuracy: 0.5354\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6650 - accuracy: 0.6341\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.634 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6915 - accuracy: 0.5278\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6799 - accuracy: 0.5526\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.553 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6920 - accuracy: 0.5220\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6765 - accuracy: 0.6437\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.644 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6964 - accuracy: 0.5010\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6924 - accuracy: 0.5321\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.532 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6929 - accuracy: 0.5117\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6867 - accuracy: 0.5928\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.593 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6971 - accuracy: 0.5038\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5911\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.591 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6959 - accuracy: 0.5047\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6883 - accuracy: 0.5797\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.580 total time=   8.2s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6959 - accuracy: 0.5094\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6910 - accuracy: 0.5986\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.599 total time=   9.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6906 - accuracy: 0.5297\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6757 - accuracy: 0.6652\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.665 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6925 - accuracy: 0.5202\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6818 - accuracy: 0.6335\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.633 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6944 - accuracy: 0.5126\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6892 - accuracy: 0.5851\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.585 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6945 - accuracy: 0.5119\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5563\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.556 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6953 - accuracy: 0.5047\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.6079\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.608 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6865 - accuracy: 0.5479\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6575 - accuracy: 0.6730\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.673 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6925 - accuracy: 0.5185\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6775 - accuracy: 0.6072\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.607 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6952 - accuracy: 0.5167\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6830 - accuracy: 0.6418\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.642 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6934 - accuracy: 0.5145\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6890 - accuracy: 0.6434\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.643 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6928 - accuracy: 0.5181\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6858 - accuracy: 0.5981\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.598 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6938 - accuracy: 0.5142\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6879 - accuracy: 0.6182\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.618 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6930 - accuracy: 0.5164\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6828 - accuracy: 0.6564\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.656 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6899 - accuracy: 0.5361\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6643 - accuracy: 0.6469\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.647 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6939 - accuracy: 0.5144\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6813 - accuracy: 0.6031\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.603 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6927 - accuracy: 0.5165\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6822 - accuracy: 0.6674\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.667 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6958 - accuracy: 0.5011\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6896 - accuracy: 0.6564\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.656 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6948 - accuracy: 0.5094\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6892 - accuracy: 0.6093\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.609 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6958 - accuracy: 0.5066\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.6138\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.614 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6855 - accuracy: 0.5512\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6508 - accuracy: 0.6600\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.660 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6939 - accuracy: 0.5104\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6826 - accuracy: 0.6261\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.626 total time=   8.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6908 - accuracy: 0.5297\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6705 - accuracy: 0.6384\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.638 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6929 - accuracy: 0.5215\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6815 - accuracy: 0.6404\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.640 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6919 - accuracy: 0.5185\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6827 - accuracy: 0.5744\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.574 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6932 - accuracy: 0.5214\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6813 - accuracy: 0.6552\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.655 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6941 - accuracy: 0.5084\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6917 - accuracy: 0.5580\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.558 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6925 - accuracy: 0.5220\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6829 - accuracy: 0.5802\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.580 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6941 - accuracy: 0.5061\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6878 - accuracy: 0.6013\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.601 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6934 - accuracy: 0.5169\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6858 - accuracy: 0.6192\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.619 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6927 - accuracy: 0.5187\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6798 - accuracy: 0.5906\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.591 total time=   8.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6942 - accuracy: 0.5102\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6902 - accuracy: 0.5729\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.573 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6927 - accuracy: 0.5208\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6794 - accuracy: 0.6068\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.607 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6928 - accuracy: 0.5190\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.6227\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.623 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6938 - accuracy: 0.5150\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6892 - accuracy: 0.5724\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.572 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6950 - accuracy: 0.5122\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6853 - accuracy: 0.6429\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.643 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6894 - accuracy: 0.5346\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6652 - accuracy: 0.6391\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.639 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6964 - accuracy: 0.5057\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6912 - accuracy: 0.5425\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.542 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6944 - accuracy: 0.5140\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6855 - accuracy: 0.6029\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.603 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6949 - accuracy: 0.5114\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6899 - accuracy: 0.6185\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.619 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6913 - accuracy: 0.5307\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6679 - accuracy: 0.6611\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.661 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6911 - accuracy: 0.5292\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6704 - accuracy: 0.6648\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.665 total time=   8.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6917 - accuracy: 0.5232\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6744 - accuracy: 0.6446\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.645 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6928 - accuracy: 0.5214\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6811 - accuracy: 0.6653\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.665 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6943 - accuracy: 0.5156\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5757\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.576 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6901 - accuracy: 0.5347\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6677 - accuracy: 0.6360\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.636 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6930 - accuracy: 0.5177\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6852 - accuracy: 0.6136\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.614 total time=   8.5s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6951 - accuracy: 0.5031\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6908 - accuracy: 0.5153\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.515 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6931 - accuracy: 0.5240\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6795 - accuracy: 0.6402\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.640 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6934 - accuracy: 0.5221\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6825 - accuracy: 0.5553\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.555 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6920 - accuracy: 0.5265\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6774 - accuracy: 0.6246\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.625 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6923 - accuracy: 0.5258\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6794 - accuracy: 0.6411\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.641 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6927 - accuracy: 0.5226\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6789 - accuracy: 0.6191\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.619 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6925 - accuracy: 0.5204\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6731 - accuracy: 0.6234\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.623 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6912 - accuracy: 0.5334\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6749 - accuracy: 0.6322\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.632 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6940 - accuracy: 0.5131\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6887 - accuracy: 0.5664\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.566 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6943 - accuracy: 0.5087\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.6330\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.633 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6904 - accuracy: 0.5278\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6743 - accuracy: 0.5931\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.593 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6917 - accuracy: 0.5233\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6809 - accuracy: 0.6395\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.639 total time=   8.5s\n",
      "708/708 [==============================] - 7s 10ms/step - loss: 0.6920 - accuracy: 0.5219\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6786 - accuracy: 0.6061\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.606 total time=   9.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6915 - accuracy: 0.5270\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6801 - accuracy: 0.5778\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.578 total time=   9.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6954 - accuracy: 0.5056\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6915 - accuracy: 0.5579\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.558 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6946 - accuracy: 0.5063\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6843 - accuracy: 0.6461\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.646 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6902 - accuracy: 0.5324\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6696 - accuracy: 0.6405\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.641 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6926 - accuracy: 0.5235\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6762 - accuracy: 0.6472\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.647 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6929 - accuracy: 0.5182\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6823 - accuracy: 0.6332\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.633 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6899 - accuracy: 0.5335\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6599 - accuracy: 0.6580\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.658 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6933 - accuracy: 0.5179\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6877 - accuracy: 0.5963\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.596 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6925 - accuracy: 0.5159\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6826 - accuracy: 0.5538\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.554 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6839 - accuracy: 0.5554\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6487 - accuracy: 0.6585\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.658 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6935 - accuracy: 0.5159\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6853 - accuracy: 0.6295\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.629 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6916 - accuracy: 0.5249\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6738 - accuracy: 0.5727\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.573 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6878 - accuracy: 0.5340\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6515 - accuracy: 0.6460\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.646 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6945 - accuracy: 0.5142\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5564\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.556 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6937 - accuracy: 0.5134\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6857 - accuracy: 0.6046\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.605 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6868 - accuracy: 0.5430\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6551 - accuracy: 0.6252\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.625 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6886 - accuracy: 0.5368\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6543 - accuracy: 0.6630\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.663 total time=   8.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6879 - accuracy: 0.5365\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6607 - accuracy: 0.6420\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.642 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6949 - accuracy: 0.5110\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6905 - accuracy: 0.5920\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.592 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6939 - accuracy: 0.5172\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6879 - accuracy: 0.6653\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.665 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6955 - accuracy: 0.5072\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6893 - accuracy: 0.6225\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.622 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6945 - accuracy: 0.5055\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6909 - accuracy: 0.5305\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.530 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6945 - accuracy: 0.5062\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6909 - accuracy: 0.5639\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.564 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6927 - accuracy: 0.5191\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6890 - accuracy: 0.5561\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.556 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6940 - accuracy: 0.5141\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6821 - accuracy: 0.6315\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.632 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6952 - accuracy: 0.5015\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6917 - accuracy: 0.5986\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.599 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6927 - accuracy: 0.5195\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6827 - accuracy: 0.6634\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.663 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6946 - accuracy: 0.5017\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6927 - accuracy: 0.5705\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.570 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6964 - accuracy: 0.5040\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6919 - accuracy: 0.5628\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.563 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6974 - accuracy: 0.5053\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6911 - accuracy: 0.5995\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.600 total time=   9.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6930 - accuracy: 0.5225\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6857 - accuracy: 0.6446\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.645 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6948 - accuracy: 0.5103\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6855 - accuracy: 0.6540\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.654 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6936 - accuracy: 0.5137\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6848 - accuracy: 0.6181\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.618 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6821 - accuracy: 0.5552\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6514 - accuracy: 0.6366\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.637 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6927 - accuracy: 0.5182\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6760 - accuracy: 0.6536\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.654 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6906 - accuracy: 0.5318\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6696 - accuracy: 0.6472\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.647 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6951 - accuracy: 0.5024\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6914 - accuracy: 0.5493\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.549 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6933 - accuracy: 0.5176\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6839 - accuracy: 0.6549\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.655 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6883 - accuracy: 0.5414\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6530 - accuracy: 0.6650\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.665 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6930 - accuracy: 0.5181\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6855 - accuracy: 0.6321\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.632 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6922 - accuracy: 0.5235\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6808 - accuracy: 0.6503\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.650 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6942 - accuracy: 0.5051\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6917 - accuracy: 0.5433\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.543 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6857 - accuracy: 0.5462\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6538 - accuracy: 0.6316\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.632 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6934 - accuracy: 0.5172\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6832 - accuracy: 0.5535\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.554 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6928 - accuracy: 0.5220\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6802 - accuracy: 0.6447\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.645 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6931 - accuracy: 0.5200\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6766 - accuracy: 0.6031\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.603 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6947 - accuracy: 0.5081\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6881 - accuracy: 0.5881\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.588 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6929 - accuracy: 0.5196\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6849 - accuracy: 0.6188\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.619 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6943 - accuracy: 0.5157\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6839 - accuracy: 0.6344\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.634 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6937 - accuracy: 0.5174\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6839 - accuracy: 0.6565\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.657 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6920 - accuracy: 0.5273\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6780 - accuracy: 0.6447\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.645 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6914 - accuracy: 0.5302\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6771 - accuracy: 0.6107\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.611 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6929 - accuracy: 0.5167\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6813 - accuracy: 0.6750\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.675 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6919 - accuracy: 0.5274\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6730 - accuracy: 0.6228\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.623 total time=   8.4s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6906 - accuracy: 0.5278\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6696 - accuracy: 0.6357\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.636 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6917 - accuracy: 0.5305\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6695 - accuracy: 0.6632\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.663 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6927 - accuracy: 0.5258\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6777 - accuracy: 0.6146\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.615 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6885 - accuracy: 0.5370\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6569 - accuracy: 0.6443\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.644 total time=   7.7s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6897 - accuracy: 0.5351\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6654 - accuracy: 0.6400\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.640 total time=   8.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6960 - accuracy: 0.5117\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5460\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.546 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6879 - accuracy: 0.5357\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6609 - accuracy: 0.6332\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.633 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6956 - accuracy: 0.5071\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5957\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.596 total time=   8.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6949 - accuracy: 0.5116\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6852 - accuracy: 0.6022\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.602 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6924 - accuracy: 0.5186\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6842 - accuracy: 0.6154\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.615 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6962 - accuracy: 0.5119\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6891 - accuracy: 0.5556\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.556 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6944 - accuracy: 0.5064\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5072\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.507 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6928 - accuracy: 0.5147\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6827 - accuracy: 0.5682\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.568 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6896 - accuracy: 0.5340\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.6170\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.617 total time=   7.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6897 - accuracy: 0.5374\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6656 - accuracy: 0.6144\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.614 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6882 - accuracy: 0.5454\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6653 - accuracy: 0.6095\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.610 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6893 - accuracy: 0.5387\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6590 - accuracy: 0.6425\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.642 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6946 - accuracy: 0.5104\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6888 - accuracy: 0.6047\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.605 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6891 - accuracy: 0.5365\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6652 - accuracy: 0.6241\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.624 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6910 - accuracy: 0.5240\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6687 - accuracy: 0.6662\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.666 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6951 - accuracy: 0.5057\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6917 - accuracy: 0.5991\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.599 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6828 - accuracy: 0.5656\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6475 - accuracy: 0.6524\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.652 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6905 - accuracy: 0.5248\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6598 - accuracy: 0.6660\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.666 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6929 - accuracy: 0.5214\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6803 - accuracy: 0.6220\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.622 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6912 - accuracy: 0.5292\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6739 - accuracy: 0.6412\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.641 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6900 - accuracy: 0.5311\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6690 - accuracy: 0.6343\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.634 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6900 - accuracy: 0.5328\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6749 - accuracy: 0.5795\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.579 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6894 - accuracy: 0.5306\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6642 - accuracy: 0.6296\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.630 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6865 - accuracy: 0.5412\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6437 - accuracy: 0.6740\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.674 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6898 - accuracy: 0.5291\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6588 - accuracy: 0.6608\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.661 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6773 - accuracy: 0.5695\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6314 - accuracy: 0.6579\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.658 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6935 - accuracy: 0.5137\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6838 - accuracy: 0.6205\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.620 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6876 - accuracy: 0.5399\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6517 - accuracy: 0.6592\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.659 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6863 - accuracy: 0.5493\n",
      "354/354 [==============================] - 3s 4ms/step - loss: 0.6646 - accuracy: 0.6042\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.604 total time=   9.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6896 - accuracy: 0.5343\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6545 - accuracy: 0.6630\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.663 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6869 - accuracy: 0.5381\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6519 - accuracy: 0.6405\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.640 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6808 - accuracy: 0.5630\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6508 - accuracy: 0.6377\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.638 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6880 - accuracy: 0.5425\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6534 - accuracy: 0.6623\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.662 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6885 - accuracy: 0.5348\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6605 - accuracy: 0.6339\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.634 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6773 - accuracy: 0.5758\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6409 - accuracy: 0.6460\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.646 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6887 - accuracy: 0.5365\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6616 - accuracy: 0.6299\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.630 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6884 - accuracy: 0.5391\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6509 - accuracy: 0.6522\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.652 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6834 - accuracy: 0.5547\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6607 - accuracy: 0.6068\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.607 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6862 - accuracy: 0.5410\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6427 - accuracy: 0.6736\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6751 - accuracy: 0.5750\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.6618\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.662 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6954 - accuracy: 0.5081\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6881 - accuracy: 0.6357\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.636 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6947 - accuracy: 0.5137\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6864 - accuracy: 0.5852\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.585 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6963 - accuracy: 0.5104\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6883 - accuracy: 0.5901\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.590 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6957 - accuracy: 0.5099\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6912 - accuracy: 0.5845\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.585 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6952 - accuracy: 0.5136\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6875 - accuracy: 0.6090\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.609 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6983 - accuracy: 0.5017\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6924 - accuracy: 0.5026\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.503 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6947 - accuracy: 0.5111\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6863 - accuracy: 0.6226\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.623 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6947 - accuracy: 0.5123\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6883 - accuracy: 0.6261\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.626 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6957 - accuracy: 0.5063\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.6052\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.605 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6952 - accuracy: 0.5161\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6868 - accuracy: 0.6349\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.635 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6961 - accuracy: 0.5093\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6866 - accuracy: 0.5950\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.595 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6950 - accuracy: 0.5159\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6882 - accuracy: 0.5875\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.587 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6974 - accuracy: 0.5122\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6874 - accuracy: 0.6433\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.643 total time=   7.5s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6963 - accuracy: 0.5116\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6888 - accuracy: 0.5972\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.597 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6931 - accuracy: 0.5196\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6815 - accuracy: 0.6471\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.647 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6948 - accuracy: 0.5146\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6886 - accuracy: 0.5993\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.599 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6955 - accuracy: 0.5110\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6886 - accuracy: 0.6247\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.625 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6945 - accuracy: 0.5248\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6842 - accuracy: 0.6358\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.636 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6934 - accuracy: 0.5208\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6889 - accuracy: 0.6048\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.605 total time=   9.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6941 - accuracy: 0.5115\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.6151\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.615 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6936 - accuracy: 0.5200\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6805 - accuracy: 0.6437\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.644 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6931 - accuracy: 0.5260\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6806 - accuracy: 0.6155\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.616 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6940 - accuracy: 0.5168\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6860 - accuracy: 0.5972\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.597 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6944 - accuracy: 0.5133\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6856 - accuracy: 0.5520\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.552 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6940 - accuracy: 0.5190\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6864 - accuracy: 0.5788\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.579 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6887 - accuracy: 0.5426\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6576 - accuracy: 0.6657\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.666 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6936 - accuracy: 0.5247\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6780 - accuracy: 0.6574\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.657 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6937 - accuracy: 0.5137\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6878 - accuracy: 0.6147\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.615 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6957 - accuracy: 0.5144\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.6139\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.614 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6984 - accuracy: 0.5001\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6914 - accuracy: 0.5857\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.586 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6963 - accuracy: 0.5049\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6915 - accuracy: 0.5026\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.503 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6944 - accuracy: 0.5197\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6818 - accuracy: 0.6309\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.631 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6939 - accuracy: 0.5139\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6854 - accuracy: 0.5877\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.588 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6933 - accuracy: 0.5151\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6818 - accuracy: 0.6440\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.644 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6921 - accuracy: 0.5221\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6784 - accuracy: 0.6388\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.639 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6946 - accuracy: 0.5005\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6922 - accuracy: 0.5528\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.553 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6895 - accuracy: 0.5395\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6608 - accuracy: 0.6457\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.646 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6939 - accuracy: 0.5161\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.6638\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.664 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6958 - accuracy: 0.5023\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5072\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.507 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6937 - accuracy: 0.5195\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6837 - accuracy: 0.5921\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.592 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6936 - accuracy: 0.5199\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6803 - accuracy: 0.6149\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.615 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6931 - accuracy: 0.5200\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6813 - accuracy: 0.6521\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.652 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6934 - accuracy: 0.5211\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6756 - accuracy: 0.6418\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.642 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6946 - accuracy: 0.5106\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5817\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.582 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6943 - accuracy: 0.5132\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6884 - accuracy: 0.5958\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.596 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6936 - accuracy: 0.5214\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6844 - accuracy: 0.6267\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.627 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6928 - accuracy: 0.5172\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6827 - accuracy: 0.6250\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.625 total time=   8.1s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6943 - accuracy: 0.5109\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6914 - accuracy: 0.6007\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.601 total time=   9.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6952 - accuracy: 0.5127\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6883 - accuracy: 0.5104\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.510 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6915 - accuracy: 0.5252\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6779 - accuracy: 0.6249\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.625 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6875 - accuracy: 0.5412\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6457 - accuracy: 0.6636\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.664 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6924 - accuracy: 0.5299\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6682 - accuracy: 0.6570\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.657 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6892 - accuracy: 0.5354\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6656 - accuracy: 0.6356\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.636 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6940 - accuracy: 0.5140\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6832 - accuracy: 0.6053\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.605 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6940 - accuracy: 0.5065\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6917 - accuracy: 0.5662\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.566 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6947 - accuracy: 0.5059\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6914 - accuracy: 0.5694\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.569 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6920 - accuracy: 0.5158\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6770 - accuracy: 0.6478\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.648 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6936 - accuracy: 0.5141\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6902 - accuracy: 0.5303\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.530 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6879 - accuracy: 0.5414\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6590 - accuracy: 0.6577\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.658 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6934 - accuracy: 0.5156\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6843 - accuracy: 0.5594\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.559 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6924 - accuracy: 0.5189\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6834 - accuracy: 0.6380\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.638 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6939 - accuracy: 0.5101\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6893 - accuracy: 0.5636\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.564 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6921 - accuracy: 0.5239\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6760 - accuracy: 0.6525\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.653 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6956 - accuracy: 0.5066\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5291\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.529 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6914 - accuracy: 0.5317\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6713 - accuracy: 0.6727\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.673 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6918 - accuracy: 0.5290\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6762 - accuracy: 0.6008\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.601 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6847 - accuracy: 0.5475\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6502 - accuracy: 0.6471\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.647 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6928 - accuracy: 0.5190\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6762 - accuracy: 0.5703\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.570 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6950 - accuracy: 0.5122\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.6235\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.624 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6922 - accuracy: 0.5259\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6776 - accuracy: 0.6413\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.641 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6861 - accuracy: 0.5467\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6478 - accuracy: 0.6507\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.651 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6922 - accuracy: 0.5247\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6838 - accuracy: 0.5897\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.590 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6894 - accuracy: 0.5355\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6673 - accuracy: 0.6182\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.618 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6927 - accuracy: 0.5205\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6770 - accuracy: 0.6262\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.626 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6924 - accuracy: 0.5236\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6750 - accuracy: 0.6334\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.633 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6903 - accuracy: 0.5305\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6684 - accuracy: 0.6479\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.648 total time=   8.1s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6892 - accuracy: 0.5378\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6582 - accuracy: 0.6765\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.676 total time=   9.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6891 - accuracy: 0.5337\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6663 - accuracy: 0.5982\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.598 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6915 - accuracy: 0.5246\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6698 - accuracy: 0.6275\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.628 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6910 - accuracy: 0.5303\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6723 - accuracy: 0.5941\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.594 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6863 - accuracy: 0.5456\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6503 - accuracy: 0.6473\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.647 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6960 - accuracy: 0.5061\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5102\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.510 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6936 - accuracy: 0.5225\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6853 - accuracy: 0.6433\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.643 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6933 - accuracy: 0.5213\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6873 - accuracy: 0.5924\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.592 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6968 - accuracy: 0.5130\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6883 - accuracy: 0.5651\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.565 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6957 - accuracy: 0.5107\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6874 - accuracy: 0.5795\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.579 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6970 - accuracy: 0.5134\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6856 - accuracy: 0.6159\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.616 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6940 - accuracy: 0.5170\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6856 - accuracy: 0.5791\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.579 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6958 - accuracy: 0.5068\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5315\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.532 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6897 - accuracy: 0.5367\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6723 - accuracy: 0.6098\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.610 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6933 - accuracy: 0.5231\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6816 - accuracy: 0.5825\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.582 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6960 - accuracy: 0.5184\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6888 - accuracy: 0.6130\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.613 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6955 - accuracy: 0.5047\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6910 - accuracy: 0.5926\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.593 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6951 - accuracy: 0.5135\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6886 - accuracy: 0.6385\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.638 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6927 - accuracy: 0.5234\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6804 - accuracy: 0.6226\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.623 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6950 - accuracy: 0.5106\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6881 - accuracy: 0.6686\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.669 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6940 - accuracy: 0.5138\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6834 - accuracy: 0.6354\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.635 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6938 - accuracy: 0.5200\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6854 - accuracy: 0.6206\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.621 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6886 - accuracy: 0.5455\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6555 - accuracy: 0.6484\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.648 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6906 - accuracy: 0.5310\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6720 - accuracy: 0.6433\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.643 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6924 - accuracy: 0.5248\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6760 - accuracy: 0.6616\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.662 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6944 - accuracy: 0.5141\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5995\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.600 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6944 - accuracy: 0.5140\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6884 - accuracy: 0.5642\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.564 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6954 - accuracy: 0.5167\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6857 - accuracy: 0.5458\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.546 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6969 - accuracy: 0.5053\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.6434\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.643 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6872 - accuracy: 0.5421\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6528 - accuracy: 0.6395\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.639 total time=   8.6s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6899 - accuracy: 0.5335\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6560 - accuracy: 0.6732\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.673 total time=   9.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6938 - accuracy: 0.5204\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6867 - accuracy: 0.6167\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.617 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6933 - accuracy: 0.5229\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6800 - accuracy: 0.6386\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.639 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6954 - accuracy: 0.5127\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6862 - accuracy: 0.6364\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.636 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6941 - accuracy: 0.5104\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6870 - accuracy: 0.6419\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.642 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6954 - accuracy: 0.5133\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6889 - accuracy: 0.5749\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.575 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6973 - accuracy: 0.5077\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6896 - accuracy: 0.5224\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.522 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6918 - accuracy: 0.5292\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6713 - accuracy: 0.6644\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.664 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6916 - accuracy: 0.5233\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6732 - accuracy: 0.6547\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.655 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6919 - accuracy: 0.5214\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6735 - accuracy: 0.6396\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.640 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6933 - accuracy: 0.5254\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6800 - accuracy: 0.6413\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.641 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6911 - accuracy: 0.5265\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6686 - accuracy: 0.6611\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.661 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6891 - accuracy: 0.5324\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6672 - accuracy: 0.6217\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.622 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6931 - accuracy: 0.5250\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6791 - accuracy: 0.6232\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.623 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6899 - accuracy: 0.5347\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6689 - accuracy: 0.6129\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.613 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6938 - accuracy: 0.5207\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6776 - accuracy: 0.6585\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.658 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6896 - accuracy: 0.5435\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6620 - accuracy: 0.6582\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.658 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6920 - accuracy: 0.5254\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6780 - accuracy: 0.5929\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.593 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6905 - accuracy: 0.5329\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6627 - accuracy: 0.6455\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.645 total time=   8.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6943 - accuracy: 0.5109\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6835 - accuracy: 0.6071\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.607 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6930 - accuracy: 0.5271\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6770 - accuracy: 0.6199\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.620 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6930 - accuracy: 0.5228\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6746 - accuracy: 0.6549\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.655 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6946 - accuracy: 0.5202\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6824 - accuracy: 0.6116\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.612 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6951 - accuracy: 0.5134\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6827 - accuracy: 0.6108\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.611 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6880 - accuracy: 0.5392\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6582 - accuracy: 0.6547\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.655 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6927 - accuracy: 0.5229\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6745 - accuracy: 0.6497\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.650 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6878 - accuracy: 0.5375\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6661 - accuracy: 0.5912\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.591 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6922 - accuracy: 0.5252\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6691 - accuracy: 0.6542\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.654 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6947 - accuracy: 0.5163\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6834 - accuracy: 0.6181\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.618 total time=   8.2s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6918 - accuracy: 0.5278\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6761 - accuracy: 0.6010\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.601 total time=   9.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6959 - accuracy: 0.5064\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6886 - accuracy: 0.6546\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.655 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6933 - accuracy: 0.5196\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6816 - accuracy: 0.6619\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.662 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6916 - accuracy: 0.5293\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6740 - accuracy: 0.6453\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.645 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6876 - accuracy: 0.5422\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6575 - accuracy: 0.6333\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.633 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6910 - accuracy: 0.5314\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6688 - accuracy: 0.6482\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.648 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6780 - accuracy: 0.5671\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6397 - accuracy: 0.6587\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.659 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6888 - accuracy: 0.5343\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6553 - accuracy: 0.6520\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.652 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6902 - accuracy: 0.5342\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6655 - accuracy: 0.6259\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.626 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6905 - accuracy: 0.5321\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6665 - accuracy: 0.6226\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.623 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6880 - accuracy: 0.5449\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6619 - accuracy: 0.6391\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.639 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6959 - accuracy: 0.5152\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6867 - accuracy: 0.5529\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.553 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6925 - accuracy: 0.5230\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6729 - accuracy: 0.6478\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.648 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6927 - accuracy: 0.5257\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6728 - accuracy: 0.6642\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.664 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6940 - accuracy: 0.5174\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.6539\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.654 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6833 - accuracy: 0.5539\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6533 - accuracy: 0.6262\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.626 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6934 - accuracy: 0.5150\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6808 - accuracy: 0.5895\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.589 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6917 - accuracy: 0.5288\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6723 - accuracy: 0.6091\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.609 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6926 - accuracy: 0.5232\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6784 - accuracy: 0.6123\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.612 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6872 - accuracy: 0.5390\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6659\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.666 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6917 - accuracy: 0.5263\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6803 - accuracy: 0.6123\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.612 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6932 - accuracy: 0.5212\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6793 - accuracy: 0.6313\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.631 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6949 - accuracy: 0.5167\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6824 - accuracy: 0.6203\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.620 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6919 - accuracy: 0.5226\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6688 - accuracy: 0.6554\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.655 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6926 - accuracy: 0.5245\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6731 - accuracy: 0.6606\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.661 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6922 - accuracy: 0.5235\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6728 - accuracy: 0.6583\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.658 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6779 - accuracy: 0.5670\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6621\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.662 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6945 - accuracy: 0.5183\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6862 - accuracy: 0.5778\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.578 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6931 - accuracy: 0.5184\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6826 - accuracy: 0.5990\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.599 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6951 - accuracy: 0.5075\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6901 - accuracy: 0.6102\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.610 total time=   8.0s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6940 - accuracy: 0.5151\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6872 - accuracy: 0.5868\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.587 total time=   9.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6948 - accuracy: 0.5093\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6898 - accuracy: 0.5545\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.554 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6945 - accuracy: 0.5247\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6775 - accuracy: 0.6550\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.655 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6936 - accuracy: 0.5205\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6873 - accuracy: 0.6249\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.625 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6924 - accuracy: 0.5151\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6794 - accuracy: 0.6634\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.663 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6937 - accuracy: 0.5151\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6796 - accuracy: 0.6583\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.658 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6925 - accuracy: 0.5248\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6726 - accuracy: 0.6483\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.648 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6942 - accuracy: 0.5209\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6873 - accuracy: 0.6158\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.616 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6927 - accuracy: 0.5189\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6784 - accuracy: 0.6525\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.653 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6918 - accuracy: 0.5245\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6700 - accuracy: 0.6520\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.652 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6940 - accuracy: 0.5171\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6875 - accuracy: 0.6458\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.646 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6953 - accuracy: 0.5129\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6887 - accuracy: 0.6354\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.635 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6937 - accuracy: 0.5176\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6830 - accuracy: 0.6403\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.640 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6863 - accuracy: 0.5450\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6471 - accuracy: 0.6456\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.646 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6915 - accuracy: 0.5237\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6708 - accuracy: 0.6545\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.655 total time=   8.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6950 - accuracy: 0.5079\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6911 - accuracy: 0.5661\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.566 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6885 - accuracy: 0.5373\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6524 - accuracy: 0.6675\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.667 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6929 - accuracy: 0.5215\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6744 - accuracy: 0.6505\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.650 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6924 - accuracy: 0.5226\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6740 - accuracy: 0.6418\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.642 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6925 - accuracy: 0.5224\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6760 - accuracy: 0.6182\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.618 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6896 - accuracy: 0.5340\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6497 - accuracy: 0.6631\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.663 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6896 - accuracy: 0.5330\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6620 - accuracy: 0.6433\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.643 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6918 - accuracy: 0.5236\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6698 - accuracy: 0.5754\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.575 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6901 - accuracy: 0.5292\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6615 - accuracy: 0.6593\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.659 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6938 - accuracy: 0.5126\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6854 - accuracy: 0.6282\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.628 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6928 - accuracy: 0.5236\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6811 - accuracy: 0.6446\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.645 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6924 - accuracy: 0.5257\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6725 - accuracy: 0.6332\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.633 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6944 - accuracy: 0.5210\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6778 - accuracy: 0.6225\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.623 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6927 - accuracy: 0.5247\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6776 - accuracy: 0.6163\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.616 total time=   8.0s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6858 - accuracy: 0.5512\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6474 - accuracy: 0.6681\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.668 total time=   9.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6907 - accuracy: 0.5261\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6696 - accuracy: 0.6009\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.601 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6926 - accuracy: 0.5227\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6758 - accuracy: 0.6322\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.632 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6857 - accuracy: 0.5490\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6519 - accuracy: 0.6543\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.654 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6898 - accuracy: 0.5344\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6616 - accuracy: 0.6619\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.662 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6960 - accuracy: 0.5031\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5014\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.501 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6911 - accuracy: 0.5295\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6681 - accuracy: 0.6630\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.663 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6938 - accuracy: 0.5213\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6775 - accuracy: 0.6136\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.614 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6932 - accuracy: 0.5264\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6698 - accuracy: 0.6617\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.662 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6913 - accuracy: 0.5298\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6692 - accuracy: 0.6630\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.663 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6887 - accuracy: 0.5377\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6601 - accuracy: 0.6474\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.647 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6928 - accuracy: 0.5221\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6781 - accuracy: 0.6210\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.621 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6910 - accuracy: 0.5286\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6691 - accuracy: 0.6181\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.618 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6890 - accuracy: 0.5355\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6552 - accuracy: 0.6634\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.663 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6926 - accuracy: 0.5204\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6828 - accuracy: 0.6043\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.604 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6924 - accuracy: 0.5272\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6767 - accuracy: 0.6590\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.659 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6884 - accuracy: 0.5380\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6507 - accuracy: 0.6513\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.651 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6916 - accuracy: 0.5271\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6661 - accuracy: 0.6494\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.649 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6943 - accuracy: 0.5129\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6839 - accuracy: 0.6487\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.649 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6803 - accuracy: 0.5670\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6360 - accuracy: 0.6588\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.659 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6821 - accuracy: 0.5580\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6443 - accuracy: 0.6451\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.645 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6933 - accuracy: 0.5284\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6691 - accuracy: 0.6141\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.614 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6921 - accuracy: 0.5274\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6776 - accuracy: 0.6233\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.623 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6943 - accuracy: 0.5112\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6903 - accuracy: 0.5644\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.564 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6934 - accuracy: 0.5183\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5851\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.585 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6876 - accuracy: 0.5410\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6604 - accuracy: 0.6349\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.635 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6876 - accuracy: 0.5447\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6504 - accuracy: 0.6544\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.654 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6924 - accuracy: 0.5226\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6725 - accuracy: 0.6502\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.650 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6901 - accuracy: 0.5301\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6635 - accuracy: 0.6095\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.610 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6876 - accuracy: 0.5384\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6574 - accuracy: 0.6004\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.600 total time=   8.1s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6885 - accuracy: 0.5369\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6616 - accuracy: 0.6157\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.616 total time=   9.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6883 - accuracy: 0.5386\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6518 - accuracy: 0.6418\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.642 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6949 - accuracy: 0.5127\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5148\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.515 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6890 - accuracy: 0.5386\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6554 - accuracy: 0.6620\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.662 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6863 - accuracy: 0.5494\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6502 - accuracy: 0.6481\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.648 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6899 - accuracy: 0.5256\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6600 - accuracy: 0.6488\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.649 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6867 - accuracy: 0.5482\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6494 - accuracy: 0.6656\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.666 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6877 - accuracy: 0.5376\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6548 - accuracy: 0.6465\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.647 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6931 - accuracy: 0.5196\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6818 - accuracy: 0.6422\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.642 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6853 - accuracy: 0.5455\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6391 - accuracy: 0.6594\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.659 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6891 - accuracy: 0.5347\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6616 - accuracy: 0.6117\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.612 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6850 - accuracy: 0.5470\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6527 - accuracy: 0.6510\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.651 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6898 - accuracy: 0.5314\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6571 - accuracy: 0.6520\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.652 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6722 - accuracy: 0.5810\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.6616\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.662 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6883 - accuracy: 0.5373\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6514 - accuracy: 0.6673\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.667 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6914 - accuracy: 0.5290\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6695 - accuracy: 0.6279\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.628 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6837 - accuracy: 0.5510\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6429 - accuracy: 0.6479\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.648 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6937 - accuracy: 0.5204\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6762 - accuracy: 0.6580\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.658 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6850 - accuracy: 0.5482\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6451 - accuracy: 0.6498\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.650 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6947 - accuracy: 0.5148\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6859 - accuracy: 0.6288\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.629 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6964 - accuracy: 0.5145\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6863 - accuracy: 0.6252\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.625 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6972 - accuracy: 0.5098\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6911 - accuracy: 0.5498\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.550 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6939 - accuracy: 0.5177\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6835 - accuracy: 0.6457\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.646 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6940 - accuracy: 0.5121\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.6309\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.631 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6968 - accuracy: 0.5050\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.6098\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.610 total time=   8.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6934 - accuracy: 0.5246\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6847 - accuracy: 0.6437\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.644 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6937 - accuracy: 0.5190\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6852 - accuracy: 0.6680\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.668 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6945 - accuracy: 0.5076\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6879 - accuracy: 0.6147\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.615 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6930 - accuracy: 0.5312\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6735 - accuracy: 0.6452\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.645 total time=   8.4s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6958 - accuracy: 0.5095\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6909 - accuracy: 0.6486\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.649 total time=   9.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6909 - accuracy: 0.5297\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6758 - accuracy: 0.6594\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.659 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6928 - accuracy: 0.5240\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6776 - accuracy: 0.6521\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.652 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6961 - accuracy: 0.5128\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6862 - accuracy: 0.6041\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.604 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6963 - accuracy: 0.5157\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6880 - accuracy: 0.5447\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.545 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6890 - accuracy: 0.5403\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6617 - accuracy: 0.6607\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.661 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6941 - accuracy: 0.5164\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6854 - accuracy: 0.6562\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.656 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6957 - accuracy: 0.5147\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6854 - accuracy: 0.5974\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.597 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6936 - accuracy: 0.5251\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6811 - accuracy: 0.5739\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.574 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6922 - accuracy: 0.5218\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6796 - accuracy: 0.6325\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.632 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6946 - accuracy: 0.5131\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6897 - accuracy: 0.6113\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.611 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6937 - accuracy: 0.5157\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6851 - accuracy: 0.6269\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.627 total time=   8.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6948 - accuracy: 0.5146\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6884 - accuracy: 0.5165\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.517 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6898 - accuracy: 0.5364\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6604 - accuracy: 0.6535\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.653 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6937 - accuracy: 0.5217\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6770 - accuracy: 0.6586\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.659 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6945 - accuracy: 0.5104\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6858 - accuracy: 0.5950\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.595 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6917 - accuracy: 0.5284\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6728 - accuracy: 0.6180\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.618 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6924 - accuracy: 0.5206\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6795 - accuracy: 0.5888\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.589 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6956 - accuracy: 0.5051\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6924 - accuracy: 0.5223\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.522 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6953 - accuracy: 0.5111\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6895 - accuracy: 0.6497\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.650 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6940 - accuracy: 0.5104\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5711\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.571 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6926 - accuracy: 0.5160\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6803 - accuracy: 0.5449\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.545 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6942 - accuracy: 0.5133\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.6303\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.630 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6901 - accuracy: 0.5331\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6656 - accuracy: 0.6623\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.662 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6919 - accuracy: 0.5273\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6750 - accuracy: 0.6529\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.653 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6924 - accuracy: 0.5230\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6751 - accuracy: 0.6488\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.649 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6943 - accuracy: 0.5108\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6817 - accuracy: 0.6573\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.657 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6947 - accuracy: 0.5136\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6880 - accuracy: 0.6433\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.643 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6944 - accuracy: 0.5136\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6898 - accuracy: 0.6036\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.604 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6949 - accuracy: 0.5114\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5683\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.568 total time=   8.1s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6931 - accuracy: 0.5160\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6797 - accuracy: 0.6055\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.606 total time=   9.5s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6906 - accuracy: 0.5295\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6708 - accuracy: 0.6312\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.631 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6895 - accuracy: 0.5371\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6595 - accuracy: 0.6594\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.659 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6904 - accuracy: 0.5321\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6657 - accuracy: 0.6502\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.650 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6912 - accuracy: 0.5293\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6714 - accuracy: 0.6658\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.666 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6938 - accuracy: 0.5197\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6822 - accuracy: 0.5927\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.593 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6915 - accuracy: 0.5263\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6693 - accuracy: 0.6140\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.614 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6931 - accuracy: 0.5182\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6817 - accuracy: 0.6242\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.624 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6935 - accuracy: 0.5162\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6825 - accuracy: 0.5975\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.598 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6947 - accuracy: 0.5101\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.5535\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.554 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6942 - accuracy: 0.5170\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6843 - accuracy: 0.5670\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.567 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6923 - accuracy: 0.5225\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6753 - accuracy: 0.6053\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.605 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6910 - accuracy: 0.5314\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6624 - accuracy: 0.6712\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.671 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6919 - accuracy: 0.5304\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6739 - accuracy: 0.6444\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.644 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6890 - accuracy: 0.5368\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6649 - accuracy: 0.6490\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.649 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6914 - accuracy: 0.5281\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6779 - accuracy: 0.5667\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.567 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6929 - accuracy: 0.5193\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6872 - accuracy: 0.5810\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.581 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6881 - accuracy: 0.5375\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6585 - accuracy: 0.6334\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.633 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6913 - accuracy: 0.5279\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6709 - accuracy: 0.6650\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.665 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6928 - accuracy: 0.5188\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6761 - accuracy: 0.6685\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.668 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6871 - accuracy: 0.5441\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6489 - accuracy: 0.6570\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.657 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6847 - accuracy: 0.5549\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6415\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.641 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6922 - accuracy: 0.5240\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6765 - accuracy: 0.6055\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.606 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6874 - accuracy: 0.5357\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6539 - accuracy: 0.6585\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.659 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6916 - accuracy: 0.5310\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6762 - accuracy: 0.5942\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.594 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6945 - accuracy: 0.5062\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.6019\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.602 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6921 - accuracy: 0.5266\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6750 - accuracy: 0.6399\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.640 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6911 - accuracy: 0.5198\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6708 - accuracy: 0.6536\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.654 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6907 - accuracy: 0.5279\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6621 - accuracy: 0.6726\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.673 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6928 - accuracy: 0.5169\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6778 - accuracy: 0.6365\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.636 total time=   8.2s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6918 - accuracy: 0.5280\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6668 - accuracy: 0.6688\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.669 total time=   9.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6874 - accuracy: 0.5420\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6456 - accuracy: 0.6695\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.669 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6909 - accuracy: 0.5293\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6750 - accuracy: 0.5947\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.595 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6951 - accuracy: 0.5194\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6835 - accuracy: 0.5983\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.598 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6890 - accuracy: 0.5301\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6608 - accuracy: 0.6391\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.639 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6926 - accuracy: 0.5212\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6768 - accuracy: 0.6614\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.661 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6886 - accuracy: 0.5373\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6530 - accuracy: 0.6751\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.675 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6923 - accuracy: 0.5283\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6701 - accuracy: 0.6557\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.656 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6915 - accuracy: 0.5261\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6664 - accuracy: 0.6663\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.666 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6906 - accuracy: 0.5287\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6633 - accuracy: 0.6416\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.642 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6880 - accuracy: 0.5357\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6597 - accuracy: 0.6218\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.622 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6956 - accuracy: 0.5146\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.6536\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.654 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6983 - accuracy: 0.5055\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6906 - accuracy: 0.6398\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.640 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6955 - accuracy: 0.5175\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6781 - accuracy: 0.6077\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.608 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6969 - accuracy: 0.5017\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6910 - accuracy: 0.6288\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.629 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6939 - accuracy: 0.5127\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6814 - accuracy: 0.6508\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.651 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6933 - accuracy: 0.5194\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6791 - accuracy: 0.6294\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.629 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6960 - accuracy: 0.5101\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6880 - accuracy: 0.6119\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.612 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6943 - accuracy: 0.5149\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6789 - accuracy: 0.6767\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.677 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6925 - accuracy: 0.5239\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6751 - accuracy: 0.6655\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.665 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6944 - accuracy: 0.5186\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6840 - accuracy: 0.6202\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.620 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6960 - accuracy: 0.5135\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6870 - accuracy: 0.6518\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.652 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6922 - accuracy: 0.5317\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6687 - accuracy: 0.6615\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.662 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6944 - accuracy: 0.5176\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6838 - accuracy: 0.6542\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.654 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6942 - accuracy: 0.5160\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6791 - accuracy: 0.6386\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.639 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6944 - accuracy: 0.5178\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6801 - accuracy: 0.6330\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.633 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6927 - accuracy: 0.5261\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6762 - accuracy: 0.6558\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.656 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6940 - accuracy: 0.5164\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6799 - accuracy: 0.5793\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.579 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6943 - accuracy: 0.5189\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6810 - accuracy: 0.6499\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.650 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6948 - accuracy: 0.5074\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6919 - accuracy: 0.5039\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.504 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6955 - accuracy: 0.5099\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6894 - accuracy: 0.5308\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.531 total time=   9.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6886 - accuracy: 0.5403\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6636 - accuracy: 0.6333\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.633 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6933 - accuracy: 0.5189\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6813 - accuracy: 0.5829\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.583 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6925 - accuracy: 0.5227\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6682 - accuracy: 0.6525\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.653 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6908 - accuracy: 0.5318\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6690 - accuracy: 0.6205\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.621 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6933 - accuracy: 0.5220\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6752 - accuracy: 0.6464\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.646 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6916 - accuracy: 0.5307\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6692 - accuracy: 0.6697\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.670 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6922 - accuracy: 0.5313\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6626 - accuracy: 0.6710\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6940 - accuracy: 0.5222\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6796 - accuracy: 0.6396\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.640 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6937 - accuracy: 0.5233\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6833 - accuracy: 0.5890\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.589 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6949 - accuracy: 0.5107\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5688\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.569 total time=   8.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6914 - accuracy: 0.5350\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6676 - accuracy: 0.6654\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.665 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6930 - accuracy: 0.5238\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6714 - accuracy: 0.6297\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.630 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6925 - accuracy: 0.5288\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6702 - accuracy: 0.6210\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.621 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6932 - accuracy: 0.5223\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6793 - accuracy: 0.6538\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.654 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6905 - accuracy: 0.5348\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6606 - accuracy: 0.6747\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.675 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6913 - accuracy: 0.5323\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6688 - accuracy: 0.6440\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.644 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6945 - accuracy: 0.5122\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6816 - accuracy: 0.6523\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.652 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6914 - accuracy: 0.5308\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6623 - accuracy: 0.6679\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.668 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6940 - accuracy: 0.5192\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6778 - accuracy: 0.6586\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.659 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6896 - accuracy: 0.5422\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6625 - accuracy: 0.6508\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.651 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6914 - accuracy: 0.5232\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6656 - accuracy: 0.6601\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.660 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6926 - accuracy: 0.5281\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6750 - accuracy: 0.6536\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.654 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6938 - accuracy: 0.5192\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6774 - accuracy: 0.6259\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.626 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6901 - accuracy: 0.5386\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6643 - accuracy: 0.6131\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.613 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6935 - accuracy: 0.5193\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6743 - accuracy: 0.6430\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.643 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6821 - accuracy: 0.5556\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6444 - accuracy: 0.6377\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.638 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6920 - accuracy: 0.5218\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6726 - accuracy: 0.6853\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.685 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6941 - accuracy: 0.5179\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6808 - accuracy: 0.6244\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.624 total time=   8.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6836 - accuracy: 0.5539\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6620\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.662 total time=   9.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6935 - accuracy: 0.5185\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6784 - accuracy: 0.6502\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.650 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6934 - accuracy: 0.5246\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6710 - accuracy: 0.6620\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.662 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6917 - accuracy: 0.5316\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6632 - accuracy: 0.6200\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.620 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6910 - accuracy: 0.5286\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6685 - accuracy: 0.6169\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.617 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6843 - accuracy: 0.5493\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6488 - accuracy: 0.6279\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.628 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6879 - accuracy: 0.5396\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6655 - accuracy: 0.5992\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.599 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6929 - accuracy: 0.5180\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6728 - accuracy: 0.6549\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.655 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6923 - accuracy: 0.5253\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6770 - accuracy: 0.6387\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.639 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6944 - accuracy: 0.5089\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6876 - accuracy: 0.5682\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.568 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6916 - accuracy: 0.5275\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6762 - accuracy: 0.5963\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.596 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6944 - accuracy: 0.5107\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6817 - accuracy: 0.6437\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.644 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6872 - accuracy: 0.5375\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6483 - accuracy: 0.6638\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.664 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6868 - accuracy: 0.5510\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6458 - accuracy: 0.6609\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.661 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6881 - accuracy: 0.5363\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6512 - accuracy: 0.6453\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.645 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6940 - accuracy: 0.5126\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6810 - accuracy: 0.6447\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.645 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6888 - accuracy: 0.5400\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6636 - accuracy: 0.6163\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.616 total time=   8.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6931 - accuracy: 0.5204\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6770 - accuracy: 0.6153\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.615 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6870 - accuracy: 0.5427\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6425 - accuracy: 0.6606\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.661 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6879 - accuracy: 0.5438\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6564 - accuracy: 0.6227\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.623 total time=   8.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6922 - accuracy: 0.5220\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6742 - accuracy: 0.6089\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.609 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6889 - accuracy: 0.5375\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6569 - accuracy: 0.6507\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.651 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6900 - accuracy: 0.5346\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6500 - accuracy: 0.6688\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.669 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6923 - accuracy: 0.5215\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6699 - accuracy: 0.6440\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.644 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6840 - accuracy: 0.5487\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6454 - accuracy: 0.6486\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.649 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6842 - accuracy: 0.5452\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6372 - accuracy: 0.6748\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6916 - accuracy: 0.5306\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6651 - accuracy: 0.6686\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6857 - accuracy: 0.5482\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6444 - accuracy: 0.6659\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.666 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6854 - accuracy: 0.5486\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6422 - accuracy: 0.6670\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.667 total time=   8.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6849 - accuracy: 0.5514\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6504 - accuracy: 0.6204\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.620 total time=   8.0s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6872 - accuracy: 0.5441\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6473 - accuracy: 0.6524\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.652 total time=   9.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6871 - accuracy: 0.5470\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6485 - accuracy: 0.6620\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.662 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6873 - accuracy: 0.5414\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6440 - accuracy: 0.6627\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.663 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6966 - accuracy: 0.5085\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6885 - accuracy: 0.6438\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.644 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6938 - accuracy: 0.5168\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6760 - accuracy: 0.6679\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6944 - accuracy: 0.5173\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6822 - accuracy: 0.6615\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.662 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6919 - accuracy: 0.5288\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6767 - accuracy: 0.5950\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.595 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6920 - accuracy: 0.5286\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6694 - accuracy: 0.6332\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.633 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6932 - accuracy: 0.5222\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6799 - accuracy: 0.6199\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.620 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6922 - accuracy: 0.5269\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6751 - accuracy: 0.6462\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.646 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6927 - accuracy: 0.5259\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6732 - accuracy: 0.6344\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.634 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6929 - accuracy: 0.5220\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6804 - accuracy: 0.6596\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.660 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6935 - accuracy: 0.5218\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6754 - accuracy: 0.5753\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.575 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6947 - accuracy: 0.5187\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6812 - accuracy: 0.5871\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.587 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6895 - accuracy: 0.5356\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6535 - accuracy: 0.6634\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.663 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6912 - accuracy: 0.5290\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6699 - accuracy: 0.6402\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.640 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6923 - accuracy: 0.5302\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6674 - accuracy: 0.6544\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.654 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6940 - accuracy: 0.5185\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6807 - accuracy: 0.6547\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.655 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6923 - accuracy: 0.5256\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6713 - accuracy: 0.6355\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.635 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6919 - accuracy: 0.5281\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6671 - accuracy: 0.6715\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.672 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6926 - accuracy: 0.5257\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6735 - accuracy: 0.6076\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.608 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6897 - accuracy: 0.5311\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6593 - accuracy: 0.6576\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.658 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6906 - accuracy: 0.5269\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6641 - accuracy: 0.6696\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.670 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6937 - accuracy: 0.5157\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6825 - accuracy: 0.6366\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.637 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6927 - accuracy: 0.5255\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6736 - accuracy: 0.6545\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.654 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6921 - accuracy: 0.5311\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6647 - accuracy: 0.6653\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.665 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6883 - accuracy: 0.5415\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6505 - accuracy: 0.6658\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.666 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6872 - accuracy: 0.5425\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6451 - accuracy: 0.6517\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.652 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6862 - accuracy: 0.5510\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6374 - accuracy: 0.6737\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6915 - accuracy: 0.5301\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6704 - accuracy: 0.6628\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.663 total time=   8.1s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6920 - accuracy: 0.5252\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6737 - accuracy: 0.6577\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.658 total time=   9.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6920 - accuracy: 0.5316\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6648 - accuracy: 0.6660\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.666 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6947 - accuracy: 0.5160\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6855 - accuracy: 0.6120\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.612 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6927 - accuracy: 0.5273\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6768 - accuracy: 0.6586\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.659 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6898 - accuracy: 0.5354\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6605 - accuracy: 0.6353\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.635 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6932 - accuracy: 0.5186\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6769 - accuracy: 0.6166\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.617 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6892 - accuracy: 0.5367\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6595 - accuracy: 0.6425\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.642 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6921 - accuracy: 0.5272\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6689 - accuracy: 0.6660\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.666 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6928 - accuracy: 0.5184\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6759 - accuracy: 0.6484\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.648 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6915 - accuracy: 0.5376\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6592 - accuracy: 0.6569\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.657 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6901 - accuracy: 0.5365\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6586 - accuracy: 0.6611\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.661 total time=   8.5s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6933 - accuracy: 0.5199\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6743 - accuracy: 0.6576\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.658 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6933 - accuracy: 0.5239\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6772 - accuracy: 0.6345\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.635 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6928 - accuracy: 0.5261\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6765 - accuracy: 0.5987\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.599 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6878 - accuracy: 0.5469\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6636 - accuracy: 0.5869\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.587 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6933 - accuracy: 0.5260\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6719 - accuracy: 0.6585\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.659 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6943 - accuracy: 0.5183\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6855 - accuracy: 0.5530\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.553 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6909 - accuracy: 0.5315\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6596 - accuracy: 0.6490\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.649 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6919 - accuracy: 0.5256\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6728 - accuracy: 0.6380\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.638 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6870 - accuracy: 0.5462\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6496 - accuracy: 0.6405\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.641 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6795 - accuracy: 0.5603\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6345 - accuracy: 0.6634\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.663 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6893 - accuracy: 0.5348\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6528 - accuracy: 0.6562\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.656 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6930 - accuracy: 0.5220\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6694 - accuracy: 0.6026\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.603 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6877 - accuracy: 0.5428\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6399 - accuracy: 0.6731\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.673 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6874 - accuracy: 0.5431\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.6462\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.646 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6875 - accuracy: 0.5393\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6558 - accuracy: 0.5914\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.591 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6897 - accuracy: 0.5395\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6545 - accuracy: 0.6219\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.622 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6925 - accuracy: 0.5298\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6735 - accuracy: 0.6299\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.630 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6928 - accuracy: 0.5191\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6716 - accuracy: 0.6757\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.676 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6881 - accuracy: 0.5412\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6591 - accuracy: 0.6246\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.625 total time=   8.4s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6911 - accuracy: 0.5320\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6704 - accuracy: 0.6203\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.620 total time=   9.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6845 - accuracy: 0.5494\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6403 - accuracy: 0.6710\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6898 - accuracy: 0.5378\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6567 - accuracy: 0.6655\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.665 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6882 - accuracy: 0.5382\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6508 - accuracy: 0.6572\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.657 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6869 - accuracy: 0.5416\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6642\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.664 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6929 - accuracy: 0.5177\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6807 - accuracy: 0.5434\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.543 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6855 - accuracy: 0.5477\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6459 - accuracy: 0.6550\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.655 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6891 - accuracy: 0.5373\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6584 - accuracy: 0.6243\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.624 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6830 - accuracy: 0.5575\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6338 - accuracy: 0.6746\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.675 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6807 - accuracy: 0.5619\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6431 - accuracy: 0.6446\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.645 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6905 - accuracy: 0.5340\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6586 - accuracy: 0.6307\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.631 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6891 - accuracy: 0.5380\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6556\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.656 total time=   8.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6850 - accuracy: 0.5479\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6384 - accuracy: 0.6615\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.661 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6933 - accuracy: 0.5197\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6750 - accuracy: 0.6267\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.627 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6804 - accuracy: 0.5622\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6319 - accuracy: 0.6656\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.666 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6877 - accuracy: 0.5418\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6579 - accuracy: 0.6320\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.632 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6896 - accuracy: 0.5316\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6552\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.655 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6855 - accuracy: 0.5480\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6443 - accuracy: 0.6541\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.654 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6819 - accuracy: 0.5580\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6358 - accuracy: 0.6730\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.673 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6844 - accuracy: 0.5485\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6286 - accuracy: 0.6788\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.679 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6893 - accuracy: 0.5413\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6471 - accuracy: 0.6698\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6900 - accuracy: 0.5306\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6612\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.661 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6798 - accuracy: 0.5610\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6256 - accuracy: 0.6760\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.676 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6876 - accuracy: 0.5440\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6451 - accuracy: 0.6604\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.0001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.660 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6644 - accuracy: 0.6059\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6404 - accuracy: 0.6626\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.663 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6630 - accuracy: 0.6052\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6205 - accuracy: 0.6866\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.687 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6655 - accuracy: 0.5987\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6332 - accuracy: 0.6530\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.653 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6600 - accuracy: 0.6080\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6389 - accuracy: 0.6683\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6669 - accuracy: 0.5963\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6248 - accuracy: 0.6701\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.670 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6654 - accuracy: 0.5981\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6291 - accuracy: 0.6625\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.662 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6539 - accuracy: 0.6244\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6272 - accuracy: 0.6682\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.668 total time=   9.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6609 - accuracy: 0.6080\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6307 - accuracy: 0.6801\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.680 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6715 - accuracy: 0.5831\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6358 - accuracy: 0.6624\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.662 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6633 - accuracy: 0.6034\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6257 - accuracy: 0.6766\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.677 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6618 - accuracy: 0.6117\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6163 - accuracy: 0.6812\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.681 total time=   8.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6661 - accuracy: 0.5935\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6251 - accuracy: 0.6689\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.669 total time=   8.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6635 - accuracy: 0.6029\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6450 - accuracy: 0.6532\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.653 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6682 - accuracy: 0.5938\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6577 - accuracy: 0.5946\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.595 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6662 - accuracy: 0.5989\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6430 - accuracy: 0.6477\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.648 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6552 - accuracy: 0.6259\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6323 - accuracy: 0.6686\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.669 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6617 - accuracy: 0.6094\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6289 - accuracy: 0.6649\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.665 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6645 - accuracy: 0.5957\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6238 - accuracy: 0.6708\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6593 - accuracy: 0.6116\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6270 - accuracy: 0.6688\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6656 - accuracy: 0.6023\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6127 - accuracy: 0.6840\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.684 total time=   8.4s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6639 - accuracy: 0.6080\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6355 - accuracy: 0.6614\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.661 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6630 - accuracy: 0.6049\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6267 - accuracy: 0.6682\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.668 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6646 - accuracy: 0.6011\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6206 - accuracy: 0.6705\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6634 - accuracy: 0.6030\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6671\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.667 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6560 - accuracy: 0.6192\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6361 - accuracy: 0.6573\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.657 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6617 - accuracy: 0.6054\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6272 - accuracy: 0.6809\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.681 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6599 - accuracy: 0.6125\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6268 - accuracy: 0.6726\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.673 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6756 - accuracy: 0.5737\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6341 - accuracy: 0.6617\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.662 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6638 - accuracy: 0.6038\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6148 - accuracy: 0.6812\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.681 total time=   8.1s\n",
      "708/708 [==============================] - 6s 7ms/step - loss: 0.6638 - accuracy: 0.6077\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6252 - accuracy: 0.6718\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.672 total time=   6.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6630 - accuracy: 0.6023\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6270 - accuracy: 0.6694\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.669 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6613 - accuracy: 0.6108\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.6776\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.678 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6617 - accuracy: 0.6089\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6270 - accuracy: 0.6661\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.666 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6589 - accuracy: 0.6099\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6299 - accuracy: 0.6610\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.661 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6631 - accuracy: 0.6057\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6527 - accuracy: 0.6078\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.608 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6595 - accuracy: 0.6098\n",
      "354/354 [==============================] - 3s 4ms/step - loss: 0.6237 - accuracy: 0.6731\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   9.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6600 - accuracy: 0.6121\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6361 - accuracy: 0.6562\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.656 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6608 - accuracy: 0.6146\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6313 - accuracy: 0.6591\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.659 total time=   8.2s\n",
      "708/708 [==============================] - 7s 10ms/step - loss: 0.6660 - accuracy: 0.5958\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6281 - accuracy: 0.6724\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   8.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6572 - accuracy: 0.6217\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6267 - accuracy: 0.6681\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.668 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6590 - accuracy: 0.6111\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6242 - accuracy: 0.6683\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.668 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6611 - accuracy: 0.6079\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6265 - accuracy: 0.6677\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.668 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6625 - accuracy: 0.6045\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6426 - accuracy: 0.6515\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.651 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6652 - accuracy: 0.5940\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6766\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.677 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6630 - accuracy: 0.6055\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6344 - accuracy: 0.6748\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.675 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6628 - accuracy: 0.6094\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6310 - accuracy: 0.6675\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.667 total time=   8.5s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6598 - accuracy: 0.6184\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6252 - accuracy: 0.6834\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.683 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6551 - accuracy: 0.6254\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6281 - accuracy: 0.6729\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.673 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6605 - accuracy: 0.6093\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6326 - accuracy: 0.6669\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.667 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6633 - accuracy: 0.6055\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6347 - accuracy: 0.6549\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.655 total time=   8.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6571 - accuracy: 0.6205\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6416 - accuracy: 0.6358\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.636 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6571 - accuracy: 0.6123\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6375 - accuracy: 0.6595\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.660 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6553 - accuracy: 0.6248\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6256 - accuracy: 0.6800\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.680 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6565 - accuracy: 0.6212\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6346 - accuracy: 0.6574\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.657 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6536 - accuracy: 0.6274\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6272 - accuracy: 0.6680\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6631 - accuracy: 0.6079\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6316 - accuracy: 0.6631\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.663 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6683 - accuracy: 0.5922\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6259 - accuracy: 0.6718\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.672 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6584 - accuracy: 0.6160\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6242 - accuracy: 0.6685\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.669 total time=   8.4s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6571 - accuracy: 0.6146\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6196 - accuracy: 0.6810\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.681 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6610 - accuracy: 0.6127\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6222 - accuracy: 0.6729\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.673 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6598 - accuracy: 0.6176\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6279 - accuracy: 0.6676\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.668 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6648 - accuracy: 0.6040\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6344 - accuracy: 0.6730\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6548 - accuracy: 0.6191\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6357 - accuracy: 0.6753\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.675 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6588 - accuracy: 0.6211\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6550 - accuracy: 0.6216\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.622 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6634 - accuracy: 0.6080\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6298 - accuracy: 0.6751\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.675 total time=   8.0s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6624 - accuracy: 0.6055\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6376 - accuracy: 0.6672\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.667 total time=   9.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6562 - accuracy: 0.6216\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6279 - accuracy: 0.6638\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.664 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6721 - accuracy: 0.5839\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6286 - accuracy: 0.6713\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6647 - accuracy: 0.5987\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6246 - accuracy: 0.6771\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.677 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6536 - accuracy: 0.6256\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6368 - accuracy: 0.6590\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.659 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6623 - accuracy: 0.6056\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6289 - accuracy: 0.6647\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.665 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6612 - accuracy: 0.6076\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6497 - accuracy: 0.6234\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.623 total time=   8.4s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6622 - accuracy: 0.6119\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6283 - accuracy: 0.6654\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.665 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6684 - accuracy: 0.5933\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6286 - accuracy: 0.6711\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6636 - accuracy: 0.6070\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6344 - accuracy: 0.6771\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6649 - accuracy: 0.6064\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6282 - accuracy: 0.6682\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6629 - accuracy: 0.6072\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6199 - accuracy: 0.6784\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.678 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6592 - accuracy: 0.6096\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6229 - accuracy: 0.6691\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.669 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6565 - accuracy: 0.6131\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6374 - accuracy: 0.6562\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.656 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6595 - accuracy: 0.6138\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6296 - accuracy: 0.6805\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.681 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6623 - accuracy: 0.6029\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6334 - accuracy: 0.6682\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.668 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6750 - accuracy: 0.5673\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6404 - accuracy: 0.6439\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.644 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6632 - accuracy: 0.6092\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6246 - accuracy: 0.6835\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.684 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6611 - accuracy: 0.6152\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6245 - accuracy: 0.6803\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.680 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6599 - accuracy: 0.6180\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6411 - accuracy: 0.6421\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.642 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6664 - accuracy: 0.5956\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6208 - accuracy: 0.6764\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.676 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6657 - accuracy: 0.5946\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6314 - accuracy: 0.6662\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.666 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6563 - accuracy: 0.6201\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6421 - accuracy: 0.6606\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.661 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6672 - accuracy: 0.5960\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6147 - accuracy: 0.6869\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.687 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6660 - accuracy: 0.5971\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6223 - accuracy: 0.6771\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.677 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6609 - accuracy: 0.6147\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6268 - accuracy: 0.6669\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.667 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6653 - accuracy: 0.6034\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6315 - accuracy: 0.6677\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6778 - accuracy: 0.5577\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6302 - accuracy: 0.6730\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.673 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6573 - accuracy: 0.6084\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6268 - accuracy: 0.6696\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.670 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6582 - accuracy: 0.6203\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6234 - accuracy: 0.6800\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.680 total time=   8.5s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6595 - accuracy: 0.6101\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6260 - accuracy: 0.6705\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.671 total time=   9.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6589 - accuracy: 0.6119\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6267 - accuracy: 0.6750\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.675 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6614 - accuracy: 0.6079\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6256 - accuracy: 0.6768\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.677 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6602 - accuracy: 0.6146\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6298 - accuracy: 0.6703\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.670 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6604 - accuracy: 0.6059\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6312 - accuracy: 0.6644\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.664 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6609 - accuracy: 0.6115\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6290 - accuracy: 0.6607\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.661 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6584 - accuracy: 0.6172\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6359 - accuracy: 0.6681\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.668 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6633 - accuracy: 0.6049\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6265 - accuracy: 0.6728\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.673 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6616 - accuracy: 0.6074\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6211 - accuracy: 0.6832\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.683 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6564 - accuracy: 0.6184\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.6688\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6575 - accuracy: 0.6197\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6324 - accuracy: 0.6654\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.665 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6581 - accuracy: 0.6158\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6280 - accuracy: 0.6676\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.668 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6631 - accuracy: 0.6023\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6237 - accuracy: 0.6725\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.672 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6602 - accuracy: 0.6094\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6253 - accuracy: 0.6724\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.672 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6630 - accuracy: 0.6107\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6222 - accuracy: 0.6804\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.680 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6755 - accuracy: 0.5762\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.6699\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.670 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6586 - accuracy: 0.6200\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6321 - accuracy: 0.6621\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.662 total time=   8.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6592 - accuracy: 0.6128\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6233 - accuracy: 0.6827\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.683 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6527 - accuracy: 0.6308\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6190 - accuracy: 0.6774\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.677 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6523 - accuracy: 0.6289\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6284 - accuracy: 0.6602\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.660 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6582 - accuracy: 0.6124\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6191 - accuracy: 0.6805\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.681 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6580 - accuracy: 0.6189\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6392 - accuracy: 0.6476\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.648 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6609 - accuracy: 0.6142\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6339 - accuracy: 0.6707\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.671 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6595 - accuracy: 0.6156\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6198 - accuracy: 0.6886\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.689 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6666 - accuracy: 0.5869\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6245 - accuracy: 0.6633\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.663 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6504 - accuracy: 0.6373\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6255 - accuracy: 0.6746\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.675 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6604 - accuracy: 0.6121\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6185 - accuracy: 0.6802\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.680 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6620 - accuracy: 0.6065\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6283 - accuracy: 0.6709\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.671 total time=   8.4s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6601 - accuracy: 0.6034\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6327 - accuracy: 0.6513\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.651 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6623 - accuracy: 0.6043\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6236 - accuracy: 0.6756\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.676 total time=   7.8s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6593 - accuracy: 0.6099\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6271 - accuracy: 0.6682\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   9.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6554 - accuracy: 0.6210\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6324 - accuracy: 0.6524\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.652 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6599 - accuracy: 0.6114\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6227 - accuracy: 0.6801\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.680 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6624 - accuracy: 0.6084\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6212 - accuracy: 0.6735\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.674 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6557 - accuracy: 0.6126\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6284 - accuracy: 0.6594\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.659 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6584 - accuracy: 0.6210\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6438 - accuracy: 0.6414\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.641 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6579 - accuracy: 0.6188\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6396 - accuracy: 0.6670\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.667 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6560 - accuracy: 0.6202\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6348 - accuracy: 0.6599\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.660 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6583 - accuracy: 0.6125\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6371 - accuracy: 0.6502\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.650 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6552 - accuracy: 0.6188\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6296 - accuracy: 0.6675\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.668 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6656 - accuracy: 0.5970\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6302 - accuracy: 0.6626\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.663 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6625 - accuracy: 0.6081\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6737\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.674 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6578 - accuracy: 0.6161\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6265 - accuracy: 0.6725\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.672 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6538 - accuracy: 0.6235\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6251 - accuracy: 0.6707\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.671 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6607 - accuracy: 0.6071\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6242 - accuracy: 0.6720\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.672 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6584 - accuracy: 0.6172\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6204 - accuracy: 0.6771\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6548 - accuracy: 0.6199\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6416 - accuracy: 0.6643\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.664 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6587 - accuracy: 0.6147\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6268 - accuracy: 0.6693\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.669 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6570 - accuracy: 0.6117\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6261 - accuracy: 0.6714\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.671 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6555 - accuracy: 0.6244\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6400 - accuracy: 0.6630\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.663 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6623 - accuracy: 0.6003\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6213 - accuracy: 0.6862\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.686 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6558 - accuracy: 0.6210\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6251 - accuracy: 0.6717\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   8.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6580 - accuracy: 0.6191\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6281 - accuracy: 0.6688\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.669 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6600 - accuracy: 0.6125\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6319 - accuracy: 0.6630\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.663 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6583 - accuracy: 0.6186\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6247 - accuracy: 0.6696\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.670 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6570 - accuracy: 0.6169\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6333 - accuracy: 0.6569\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.657 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6630 - accuracy: 0.6067\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6188 - accuracy: 0.6804\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.680 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6568 - accuracy: 0.6203\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6290 - accuracy: 0.6587\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.659 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6642 - accuracy: 0.6046\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6363 - accuracy: 0.6593\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.659 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6639 - accuracy: 0.6065\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6230 - accuracy: 0.6772\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.677 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6660 - accuracy: 0.5915\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6605 - accuracy: 0.5836\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.584 total time=   9.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6539 - accuracy: 0.6289\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6265 - accuracy: 0.6693\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.669 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6589 - accuracy: 0.6178\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6243 - accuracy: 0.6748\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.675 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6617 - accuracy: 0.6028\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6457 - accuracy: 0.6287\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.629 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6518 - accuracy: 0.6269\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6201 - accuracy: 0.6756\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.676 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6545 - accuracy: 0.6232\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6317 - accuracy: 0.6700\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.670 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6575 - accuracy: 0.6187\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6307 - accuracy: 0.6723\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.672 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6559 - accuracy: 0.6231\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6352 - accuracy: 0.6577\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.658 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6600 - accuracy: 0.6168\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6206 - accuracy: 0.6821\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.682 total time=   8.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6577 - accuracy: 0.6204\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6290 - accuracy: 0.6730\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.673 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6584 - accuracy: 0.6164\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6331 - accuracy: 0.6581\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.658 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6589 - accuracy: 0.6188\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6445 - accuracy: 0.6222\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.622 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6581 - accuracy: 0.6177\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6320 - accuracy: 0.6650\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.665 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6576 - accuracy: 0.6174\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6276 - accuracy: 0.6641\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.664 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6611 - accuracy: 0.6092\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6413 - accuracy: 0.6672\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.667 total time=   8.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6564 - accuracy: 0.6173\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6636\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.664 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6615 - accuracy: 0.6081\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6286 - accuracy: 0.6698\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.670 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6683 - accuracy: 0.5950\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6328 - accuracy: 0.6813\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.681 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6655 - accuracy: 0.6002\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6206 - accuracy: 0.6753\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.675 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6551 - accuracy: 0.6212\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6625\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.663 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6618 - accuracy: 0.6068\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6315 - accuracy: 0.6781\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.678 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6575 - accuracy: 0.6177\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6287 - accuracy: 0.6711\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6521 - accuracy: 0.6304\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6269 - accuracy: 0.6699\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.670 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6632 - accuracy: 0.6044\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6572 - accuracy: 0.6149\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.615 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6605 - accuracy: 0.6066\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6257 - accuracy: 0.6796\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.680 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6610 - accuracy: 0.6115\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6297 - accuracy: 0.6660\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.666 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6636 - accuracy: 0.6084\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6255 - accuracy: 0.6845\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.685 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6629 - accuracy: 0.6069\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6331 - accuracy: 0.6647\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.665 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6594 - accuracy: 0.6114\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6666\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.667 total time=   7.7s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6630 - accuracy: 0.6020\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6354 - accuracy: 0.6613\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.661 total time=   9.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6525 - accuracy: 0.6289\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6357 - accuracy: 0.6463\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.646 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6558 - accuracy: 0.6178\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6267 - accuracy: 0.6664\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.666 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6640 - accuracy: 0.6016\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6310 - accuracy: 0.6755\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.675 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6610 - accuracy: 0.6085\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6275 - accuracy: 0.6691\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.669 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6590 - accuracy: 0.6181\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6297 - accuracy: 0.6698\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.670 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6576 - accuracy: 0.6178\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6193 - accuracy: 0.6744\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.674 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6572 - accuracy: 0.6206\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6391 - accuracy: 0.6459\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.646 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6587 - accuracy: 0.6162\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6312 - accuracy: 0.6539\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.654 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6662 - accuracy: 0.5960\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6302 - accuracy: 0.6772\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.677 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6612 - accuracy: 0.6117\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6356 - accuracy: 0.6557\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.656 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6557 - accuracy: 0.6207\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6383 - accuracy: 0.6502\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.650 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6689 - accuracy: 0.5946\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6281 - accuracy: 0.6728\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6597 - accuracy: 0.6061\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6397 - accuracy: 0.6516\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.652 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6541 - accuracy: 0.6260\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6302 - accuracy: 0.6683\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.668 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6633 - accuracy: 0.6125\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6229 - accuracy: 0.6770\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.677 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6602 - accuracy: 0.6130\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6262 - accuracy: 0.6718\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6601 - accuracy: 0.6120\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6295 - accuracy: 0.6624\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.662 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6647 - accuracy: 0.6013\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6246 - accuracy: 0.6818\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.682 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6519 - accuracy: 0.6290\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6260 - accuracy: 0.6693\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.669 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6602 - accuracy: 0.6073\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6393 - accuracy: 0.6504\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.650 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6550 - accuracy: 0.6224\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6177 - accuracy: 0.6785\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.679 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6488 - accuracy: 0.6312\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6271 - accuracy: 0.6646\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.665 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6571 - accuracy: 0.6202\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6405 - accuracy: 0.6737\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.674 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6611 - accuracy: 0.6165\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6301 - accuracy: 0.6761\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.676 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6623 - accuracy: 0.6016\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.6705\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.671 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6496 - accuracy: 0.6313\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6259 - accuracy: 0.6646\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.665 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6569 - accuracy: 0.6198\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6180 - accuracy: 0.6838\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.684 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6520 - accuracy: 0.6299\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6313 - accuracy: 0.6731\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.673 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6516 - accuracy: 0.6294\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6295 - accuracy: 0.6673\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6531 - accuracy: 0.6258\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6329 - accuracy: 0.6735\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.673 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6509 - accuracy: 0.6284\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6316 - accuracy: 0.6649\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.665 total time=   9.4s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6552 - accuracy: 0.6205\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6360 - accuracy: 0.6653\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.665 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6587 - accuracy: 0.6187\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6465 - accuracy: 0.6576\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.658 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6579 - accuracy: 0.6186\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6333 - accuracy: 0.6537\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.654 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6605 - accuracy: 0.6183\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6377 - accuracy: 0.6534\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.653 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6596 - accuracy: 0.6189\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6244 - accuracy: 0.6674\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.667 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6547 - accuracy: 0.6263\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6367 - accuracy: 0.6469\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.647 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6554 - accuracy: 0.6209\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6325 - accuracy: 0.6573\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.657 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6552 - accuracy: 0.6230\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6153 - accuracy: 0.6773\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.677 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6600 - accuracy: 0.6170\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6205\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.621 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6581 - accuracy: 0.6260\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6342 - accuracy: 0.6691\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.669 total time=   8.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6574 - accuracy: 0.6204\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6217 - accuracy: 0.6781\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.678 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6634 - accuracy: 0.6141\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6379 - accuracy: 0.6666\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.667 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6534 - accuracy: 0.6305\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6360 - accuracy: 0.6599\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.660 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6589 - accuracy: 0.6162\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6442 - accuracy: 0.6776\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.678 total time=   8.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6576 - accuracy: 0.6186\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6298 - accuracy: 0.6720\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.672 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6469 - accuracy: 0.6313\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6278 - accuracy: 0.6754\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.675 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6501 - accuracy: 0.6325\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6268 - accuracy: 0.6564\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.656 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6535 - accuracy: 0.6268\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6223 - accuracy: 0.6783\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.678 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6616 - accuracy: 0.6080\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6358 - accuracy: 0.6565\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.657 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6645 - accuracy: 0.6046\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6254 - accuracy: 0.6776\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.678 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6543 - accuracy: 0.6288\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6308 - accuracy: 0.6642\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.664 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6519 - accuracy: 0.6273\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6374 - accuracy: 0.6487\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.649 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6617 - accuracy: 0.6147\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6276 - accuracy: 0.6763\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.676 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6530 - accuracy: 0.6266\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6407 - accuracy: 0.6737\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6510 - accuracy: 0.6288\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6217 - accuracy: 0.6701\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.670 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6590 - accuracy: 0.6114\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6200 - accuracy: 0.6827\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.683 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6559 - accuracy: 0.6217\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6240 - accuracy: 0.6670\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=32, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6647 - accuracy: 0.6056\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6507 - accuracy: 0.6101\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.610 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6697 - accuracy: 0.5909\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6255 - accuracy: 0.6849\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.685 total time=   8.2s\n",
      "708/708 [==============================] - 9s 9ms/step - loss: 0.6676 - accuracy: 0.5973\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6236 - accuracy: 0.6711\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.671 total time=  10.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6611 - accuracy: 0.6121\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6299 - accuracy: 0.6653\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.665 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6617 - accuracy: 0.6087\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6148 - accuracy: 0.6826\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.683 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6661 - accuracy: 0.5964\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6301 - accuracy: 0.6594\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.659 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6607 - accuracy: 0.6110\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6267 - accuracy: 0.6693\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.669 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6636 - accuracy: 0.6008\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.5985\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.598 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6603 - accuracy: 0.6087\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6235 - accuracy: 0.6734\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6609 - accuracy: 0.6106\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6291 - accuracy: 0.6631\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.663 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6649 - accuracy: 0.6070\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6257 - accuracy: 0.6704\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.670 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6718 - accuracy: 0.5841\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6278 - accuracy: 0.6774\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.677 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6625 - accuracy: 0.6075\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6301 - accuracy: 0.6605\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.660 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6612 - accuracy: 0.6091\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6404 - accuracy: 0.6565\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.657 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6572 - accuracy: 0.6197\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6371 - accuracy: 0.6639\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.664 total time=   8.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6569 - accuracy: 0.6215\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6332 - accuracy: 0.6505\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.651 total time=   8.6s\n",
      "708/708 [==============================] - 7s 10ms/step - loss: 0.6594 - accuracy: 0.6142\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6189 - accuracy: 0.6810\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.681 total time=   8.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6573 - accuracy: 0.6172\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6274 - accuracy: 0.6723\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.672 total time=   8.6s\n",
      "708/708 [==============================] - 7s 10ms/step - loss: 0.6602 - accuracy: 0.6174\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6292 - accuracy: 0.6657\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.666 total time=   8.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6630 - accuracy: 0.6043\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6151 - accuracy: 0.6814\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.681 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6642 - accuracy: 0.5964\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6302 - accuracy: 0.6753\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6600 - accuracy: 0.6092\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6306 - accuracy: 0.6706\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6655 - accuracy: 0.5994\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6294 - accuracy: 0.6778\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.678 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6569 - accuracy: 0.6175\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6275 - accuracy: 0.6628\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.663 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6614 - accuracy: 0.6079\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6389 - accuracy: 0.6670\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6611 - accuracy: 0.6054\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6294 - accuracy: 0.6823\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.682 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6596 - accuracy: 0.6092\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6315 - accuracy: 0.6696\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.670 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6613 - accuracy: 0.6128\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6283 - accuracy: 0.6691\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6684 - accuracy: 0.5974\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.6201 - accuracy: 0.6841\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.684 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6649 - accuracy: 0.6006\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6200 - accuracy: 0.6762\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.676 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6632 - accuracy: 0.6065\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6438 - accuracy: 0.6358\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.636 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6623 - accuracy: 0.6107\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6226 - accuracy: 0.6767\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.677 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6599 - accuracy: 0.6117\n",
      "354/354 [==============================] - 3s 4ms/step - loss: 0.6287 - accuracy: 0.6598\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.660 total time=   9.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6540 - accuracy: 0.6229\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6260 - accuracy: 0.6741\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.674 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6576 - accuracy: 0.6184\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6321 - accuracy: 0.6676\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6546 - accuracy: 0.6259\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6580 - accuracy: 0.6143\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.614 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6588 - accuracy: 0.6190\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6233 - accuracy: 0.6747\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.675 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6677 - accuracy: 0.6033\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6298 - accuracy: 0.6738\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.674 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6605 - accuracy: 0.6116\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6406 - accuracy: 0.6535\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.653 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6584 - accuracy: 0.6130\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6632\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.663 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6613 - accuracy: 0.6060\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6173 - accuracy: 0.6804\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.680 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6613 - accuracy: 0.6116\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6290 - accuracy: 0.6742\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.674 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6594 - accuracy: 0.6137\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6368 - accuracy: 0.6512\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.651 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6607 - accuracy: 0.6144\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6223 - accuracy: 0.6744\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.674 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6568 - accuracy: 0.6195\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6213 - accuracy: 0.6701\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.670 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6545 - accuracy: 0.6316\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6293 - accuracy: 0.6702\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.670 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6619 - accuracy: 0.6075\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6258 - accuracy: 0.6828\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.683 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6569 - accuracy: 0.6238\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6316 - accuracy: 0.6651\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.665 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6520 - accuracy: 0.6280\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6293 - accuracy: 0.6615\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.662 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6600 - accuracy: 0.6111\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6168 - accuracy: 0.6843\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.684 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6658 - accuracy: 0.5954\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6761\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.676 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6578 - accuracy: 0.6169\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6267 - accuracy: 0.6711\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6633 - accuracy: 0.6050\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6314 - accuracy: 0.6693\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6503 - accuracy: 0.6276\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6235 - accuracy: 0.6750\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.675 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6659 - accuracy: 0.5981\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6286 - accuracy: 0.6691\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.669 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6659 - accuracy: 0.5989\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6213 - accuracy: 0.6820\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.682 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6551 - accuracy: 0.6276\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6317 - accuracy: 0.6597\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.660 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6642 - accuracy: 0.6020\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6456 - accuracy: 0.6399\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.640 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6570 - accuracy: 0.6201\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6290 - accuracy: 0.6698\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.670 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6534 - accuracy: 0.6288\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6280 - accuracy: 0.6735\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6585 - accuracy: 0.6106\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6262 - accuracy: 0.6693\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.669 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6552 - accuracy: 0.6229\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6194 - accuracy: 0.6756\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.676 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6536 - accuracy: 0.6200\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6170 - accuracy: 0.6738\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.674 total time=   7.6s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6580 - accuracy: 0.6135\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6326 - accuracy: 0.6645\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.665 total time=   9.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6665 - accuracy: 0.6014\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6466 - accuracy: 0.6575\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.657 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6619 - accuracy: 0.6057\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6308 - accuracy: 0.6741\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.674 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6524 - accuracy: 0.6313\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6410 - accuracy: 0.6493\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.649 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6581 - accuracy: 0.6152\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6232 - accuracy: 0.6766\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6586 - accuracy: 0.6165\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6287 - accuracy: 0.6656\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.666 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6552 - accuracy: 0.6226\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6425 - accuracy: 0.6438\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.644 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6566 - accuracy: 0.6223\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6324 - accuracy: 0.6634\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.663 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6536 - accuracy: 0.6256\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6432 - accuracy: 0.6525\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.653 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6530 - accuracy: 0.6247\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6277 - accuracy: 0.6654\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.665 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6618 - accuracy: 0.6086\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6351 - accuracy: 0.6589\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.659 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6572 - accuracy: 0.6198\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6237 - accuracy: 0.6695\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6535 - accuracy: 0.6264\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6293 - accuracy: 0.6697\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6544 - accuracy: 0.6261\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6134 - accuracy: 0.6838\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.684 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6545 - accuracy: 0.6203\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6189 - accuracy: 0.6726\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.673 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6600 - accuracy: 0.6087\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6664\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.666 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6544 - accuracy: 0.6277\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6346 - accuracy: 0.6760\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.676 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6556 - accuracy: 0.6239\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6273 - accuracy: 0.6704\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.670 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6629 - accuracy: 0.6088\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6316 - accuracy: 0.6575\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.657 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6696 - accuracy: 0.5944\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6242 - accuracy: 0.6749\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.675 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6661 - accuracy: 0.6030\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6238 - accuracy: 0.6763\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.676 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6557 - accuracy: 0.6163\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6259 - accuracy: 0.6675\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6650 - accuracy: 0.6015\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6171 - accuracy: 0.6807\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.681 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6573 - accuracy: 0.6218\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6228 - accuracy: 0.6810\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.681 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6612 - accuracy: 0.6061\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6328 - accuracy: 0.6690\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6589 - accuracy: 0.6119\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6167 - accuracy: 0.6793\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.679 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6605 - accuracy: 0.6101\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6203 - accuracy: 0.6734\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6573 - accuracy: 0.6149\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6380 - accuracy: 0.6691\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.669 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6655 - accuracy: 0.5997\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6303 - accuracy: 0.6727\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.673 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6641 - accuracy: 0.6027\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6226 - accuracy: 0.6752\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.675 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6621 - accuracy: 0.6126\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6305 - accuracy: 0.6607\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.661 total time=   9.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6621 - accuracy: 0.6076\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6262 - accuracy: 0.6789\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.679 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6595 - accuracy: 0.6117\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.6715\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.671 total time=   8.4s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6614 - accuracy: 0.6037\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6680\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.668 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6605 - accuracy: 0.6100\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6244 - accuracy: 0.6781\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.678 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6610 - accuracy: 0.6058\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6271 - accuracy: 0.6750\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.675 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6614 - accuracy: 0.6143\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6297 - accuracy: 0.6686\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6650 - accuracy: 0.6048\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6297 - accuracy: 0.6734\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.673 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6595 - accuracy: 0.6142\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6255 - accuracy: 0.6765\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.677 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6608 - accuracy: 0.6120\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6260 - accuracy: 0.6700\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6600 - accuracy: 0.6132\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6173 - accuracy: 0.6807\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.681 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6612 - accuracy: 0.6080\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6231 - accuracy: 0.6741\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6555 - accuracy: 0.6211\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6260 - accuracy: 0.6708\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.671 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6602 - accuracy: 0.6094\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6581\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.658 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6630 - accuracy: 0.6053\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6742\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6594 - accuracy: 0.6132\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6313 - accuracy: 0.6655\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.666 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6632 - accuracy: 0.6108\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6271 - accuracy: 0.6816\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.682 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6613 - accuracy: 0.6116\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.6676\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.668 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6571 - accuracy: 0.6174\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6343 - accuracy: 0.6668\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.667 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6654 - accuracy: 0.6029\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6279 - accuracy: 0.6762\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.676 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6633 - accuracy: 0.6078\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6456 - accuracy: 0.6285\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.628 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6547 - accuracy: 0.6194\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6657\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.666 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6671 - accuracy: 0.5957\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6423 - accuracy: 0.6626\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.663 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6567 - accuracy: 0.6162\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6226 - accuracy: 0.6729\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6611 - accuracy: 0.6111\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6319 - accuracy: 0.6577\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.658 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6652 - accuracy: 0.6011\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6325 - accuracy: 0.6770\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.677 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6557 - accuracy: 0.6230\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6239 - accuracy: 0.6681\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.668 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6585 - accuracy: 0.6162\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6293 - accuracy: 0.6630\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.663 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6602 - accuracy: 0.6137\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6297 - accuracy: 0.6748\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.675 total time=   8.2s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6551 - accuracy: 0.6203\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6198 - accuracy: 0.6764\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.676 total time=   9.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6536 - accuracy: 0.6241\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6342 - accuracy: 0.6628\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.663 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6572 - accuracy: 0.6185\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6213 - accuracy: 0.6829\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.683 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6575 - accuracy: 0.6161\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6422 - accuracy: 0.6446\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.645 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6581 - accuracy: 0.6151\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6667\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.667 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6582 - accuracy: 0.6171\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6287 - accuracy: 0.6696\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.670 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6633 - accuracy: 0.6077\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6303 - accuracy: 0.6665\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.667 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6587 - accuracy: 0.6097\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6233 - accuracy: 0.6677\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.668 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6571 - accuracy: 0.6191\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6182 - accuracy: 0.6819\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.682 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6564 - accuracy: 0.6202\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6227 - accuracy: 0.6736\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6547 - accuracy: 0.6218\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6237 - accuracy: 0.6729\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.673 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6561 - accuracy: 0.6230\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6242 - accuracy: 0.6785\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.679 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6560 - accuracy: 0.6214\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6322 - accuracy: 0.6743\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6570 - accuracy: 0.6250\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6336 - accuracy: 0.6596\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.660 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6639 - accuracy: 0.6094\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6201 - accuracy: 0.6819\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.682 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6560 - accuracy: 0.6285\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6715\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.671 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6563 - accuracy: 0.6141\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6243 - accuracy: 0.6675\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.668 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6527 - accuracy: 0.6269\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6267 - accuracy: 0.6600\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.660 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6596 - accuracy: 0.6170\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6263 - accuracy: 0.6752\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.675 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6520 - accuracy: 0.6290\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6295 - accuracy: 0.6623\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.662 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6558 - accuracy: 0.6214\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6240 - accuracy: 0.6792\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.679 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6561 - accuracy: 0.6176\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6226 - accuracy: 0.6763\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.676 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6623 - accuracy: 0.6033\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6236 - accuracy: 0.6737\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.674 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6631 - accuracy: 0.6049\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6148 - accuracy: 0.6789\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.679 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6555 - accuracy: 0.6227\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6282 - accuracy: 0.6715\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.671 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6533 - accuracy: 0.6269\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6299 - accuracy: 0.6586\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.659 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6587 - accuracy: 0.6167\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6172 - accuracy: 0.6782\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.678 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6573 - accuracy: 0.6135\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6361 - accuracy: 0.6555\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.656 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6540 - accuracy: 0.6200\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6226 - accuracy: 0.6725\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.672 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6533 - accuracy: 0.6252\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6171 - accuracy: 0.6823\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.682 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6501 - accuracy: 0.6296\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6295 - accuracy: 0.6616\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.662 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6560 - accuracy: 0.6268\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6264 - accuracy: 0.6753\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   9.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6563 - accuracy: 0.6252\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6590 - accuracy: 0.5884\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.588 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6558 - accuracy: 0.6204\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6355 - accuracy: 0.6585\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.659 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6533 - accuracy: 0.6258\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6233 - accuracy: 0.6689\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.669 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6557 - accuracy: 0.6206\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6340 - accuracy: 0.6789\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.679 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6575 - accuracy: 0.6161\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6206 - accuracy: 0.6729\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.673 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6529 - accuracy: 0.6260\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6377 - accuracy: 0.6456\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.646 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6491 - accuracy: 0.6335\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6223 - accuracy: 0.6806\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.681 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6514 - accuracy: 0.6278\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6283 - accuracy: 0.6675\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.668 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6609 - accuracy: 0.6153\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6319 - accuracy: 0.6648\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.665 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6607 - accuracy: 0.6147\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6307 - accuracy: 0.6717\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.672 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6591 - accuracy: 0.6191\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6209 - accuracy: 0.6763\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.676 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6592 - accuracy: 0.6158\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6251 - accuracy: 0.6702\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.670 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6594 - accuracy: 0.6160\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6312 - accuracy: 0.6700\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.670 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6610 - accuracy: 0.6069\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6188 - accuracy: 0.6749\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.675 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6542 - accuracy: 0.6270\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6341 - accuracy: 0.6533\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.653 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6599 - accuracy: 0.6092\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6233 - accuracy: 0.6814\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.681 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6643 - accuracy: 0.6029\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6454 - accuracy: 0.6427\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.643 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6614 - accuracy: 0.6072\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6282 - accuracy: 0.6720\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6607 - accuracy: 0.6114\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6206 - accuracy: 0.6719\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6696 - accuracy: 0.5886\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6338 - accuracy: 0.6597\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.660 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6559 - accuracy: 0.6239\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6287 - accuracy: 0.6680\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.668 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6581 - accuracy: 0.6164\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6292 - accuracy: 0.6691\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6597 - accuracy: 0.6092\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6201 - accuracy: 0.6725\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.672 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6519 - accuracy: 0.6292\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6540\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.654 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6581 - accuracy: 0.6197\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6308 - accuracy: 0.6819\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.682 total time=   7.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6477 - accuracy: 0.6345\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6363 - accuracy: 0.6378\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.638 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6584 - accuracy: 0.6104\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6263 - accuracy: 0.6723\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.672 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6577 - accuracy: 0.6205\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6306 - accuracy: 0.6655\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.666 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6641 - accuracy: 0.6055\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6257 - accuracy: 0.6721\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.672 total time=   7.6s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6514 - accuracy: 0.6272\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6276 - accuracy: 0.6634\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.663 total time=   9.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6546 - accuracy: 0.6280\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6172 - accuracy: 0.6772\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6563 - accuracy: 0.6192\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6221 - accuracy: 0.6737\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6532 - accuracy: 0.6263\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6307 - accuracy: 0.6738\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6563 - accuracy: 0.6226\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6141 - accuracy: 0.6859\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.686 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6526 - accuracy: 0.6266\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6198 - accuracy: 0.6742\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6604 - accuracy: 0.6095\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6307 - accuracy: 0.6591\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.659 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6652 - accuracy: 0.6008\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6246 - accuracy: 0.6674\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.667 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6623 - accuracy: 0.6113\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6277 - accuracy: 0.6650\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.665 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6540 - accuracy: 0.6239\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6223 - accuracy: 0.6721\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.672 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6575 - accuracy: 0.6183\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6308 - accuracy: 0.6685\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.669 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6523 - accuracy: 0.6256\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6676\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.668 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6520 - accuracy: 0.6243\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6284 - accuracy: 0.6711\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.671 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6588 - accuracy: 0.6127\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6300 - accuracy: 0.6824\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.682 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6535 - accuracy: 0.6247\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6320 - accuracy: 0.6685\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.668 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6544 - accuracy: 0.6246\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6646\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.665 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6582 - accuracy: 0.6200\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6181 - accuracy: 0.6726\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.673 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6591 - accuracy: 0.6108\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6294 - accuracy: 0.6617\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.662 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6508 - accuracy: 0.6313\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6239 - accuracy: 0.6740\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.674 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6583 - accuracy: 0.6141\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6122 - accuracy: 0.6804\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.680 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6575 - accuracy: 0.6150\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6288 - accuracy: 0.6651\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.665 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6486 - accuracy: 0.6277\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6230 - accuracy: 0.6668\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.667 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6587 - accuracy: 0.6109\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6164 - accuracy: 0.6803\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.680 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6514 - accuracy: 0.6267\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6230 - accuracy: 0.6757\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.676 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6540 - accuracy: 0.6223\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6334 - accuracy: 0.6525\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.653 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6558 - accuracy: 0.6196\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6304 - accuracy: 0.6601\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.660 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6617 - accuracy: 0.6052\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6243 - accuracy: 0.6759\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.676 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6526 - accuracy: 0.6236\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6279 - accuracy: 0.6628\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.663 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6567 - accuracy: 0.6243\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6164 - accuracy: 0.6832\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.683 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6566 - accuracy: 0.6211\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6288 - accuracy: 0.6766\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.677 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6488 - accuracy: 0.6337\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6378 - accuracy: 0.6536\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.654 total time=   9.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6548 - accuracy: 0.6252\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6354 - accuracy: 0.6637\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.664 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6558 - accuracy: 0.6166\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6248 - accuracy: 0.6696\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.670 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6554 - accuracy: 0.6215\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6664\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.666 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6632 - accuracy: 0.6097\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6635 - accuracy: 0.5739\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.574 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6618 - accuracy: 0.6137\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6246 - accuracy: 0.6747\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.675 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6510 - accuracy: 0.6278\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6292 - accuracy: 0.6643\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.664 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6556 - accuracy: 0.6247\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6299 - accuracy: 0.6880\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.688 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6573 - accuracy: 0.6176\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6265 - accuracy: 0.6690\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.669 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6502 - accuracy: 0.6342\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6387 - accuracy: 0.6509\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.651 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6564 - accuracy: 0.6175\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6153 - accuracy: 0.6796\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.680 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6524 - accuracy: 0.6306\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6291 - accuracy: 0.6716\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.672 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6560 - accuracy: 0.6238\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6500 - accuracy: 0.6267\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.627 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6617 - accuracy: 0.6111\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6225 - accuracy: 0.6852\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.685 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6513 - accuracy: 0.6295\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6263 - accuracy: 0.6585\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.659 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6526 - accuracy: 0.6247\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6295 - accuracy: 0.6600\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.660 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6607 - accuracy: 0.6127\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6213 - accuracy: 0.6804\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.680 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6529 - accuracy: 0.6272\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6328 - accuracy: 0.6708\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.671 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6542 - accuracy: 0.6203\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6431 - accuracy: 0.6454\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.645 total time=   8.4s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6541 - accuracy: 0.6237\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6252 - accuracy: 0.6689\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.669 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6556 - accuracy: 0.6230\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6208 - accuracy: 0.6758\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.676 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6531 - accuracy: 0.6285\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6298 - accuracy: 0.6657\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.666 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6606 - accuracy: 0.6135\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6326 - accuracy: 0.6752\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6558 - accuracy: 0.6288\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6360 - accuracy: 0.6676\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.668 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6531 - accuracy: 0.6249\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6739\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   7.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6519 - accuracy: 0.6274\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6298 - accuracy: 0.6739\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6520 - accuracy: 0.6316\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6239 - accuracy: 0.6741\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6522 - accuracy: 0.6261\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6222 - accuracy: 0.6761\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.676 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6570 - accuracy: 0.6175\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6228 - accuracy: 0.6700\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.670 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6507 - accuracy: 0.6282\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6290 - accuracy: 0.6597\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=64, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.660 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6672 - accuracy: 0.5979\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6294 - accuracy: 0.6768\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.677 total time=   8.1s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6730 - accuracy: 0.5851\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6417 - accuracy: 0.6812\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.681 total time=   9.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6708 - accuracy: 0.5888\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6283 - accuracy: 0.6779\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.678 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6600 - accuracy: 0.6132\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6324 - accuracy: 0.6547\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.655 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6673 - accuracy: 0.5934\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6578\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.658 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6693 - accuracy: 0.5896\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6500 - accuracy: 0.6248\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.625 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6580 - accuracy: 0.6131\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6253 - accuracy: 0.6687\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.669 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6576 - accuracy: 0.6156\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6296 - accuracy: 0.6769\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6618 - accuracy: 0.6062\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6433 - accuracy: 0.6488\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.649 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6617 - accuracy: 0.6129\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6253 - accuracy: 0.6701\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.670 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6654 - accuracy: 0.6076\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6270 - accuracy: 0.6721\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6654 - accuracy: 0.6007\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6264 - accuracy: 0.6729\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.673 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6641 - accuracy: 0.5960\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6311 - accuracy: 0.6633\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.663 total time=   7.5s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6713 - accuracy: 0.5828\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6263 - accuracy: 0.6713\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.671 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6673 - accuracy: 0.5885\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6239 - accuracy: 0.6765\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.677 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6605 - accuracy: 0.6096\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6282 - accuracy: 0.6737\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6584 - accuracy: 0.6138\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6215 - accuracy: 0.6852\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.685 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6617 - accuracy: 0.6010\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6328 - accuracy: 0.6613\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.661 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6608 - accuracy: 0.6027\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6278 - accuracy: 0.6550\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.655 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6676 - accuracy: 0.5947\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6243 - accuracy: 0.6731\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.673 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6642 - accuracy: 0.6053\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6271 - accuracy: 0.6747\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6555 - accuracy: 0.6242\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6261 - accuracy: 0.6710\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6647 - accuracy: 0.6009\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6173 - accuracy: 0.6870\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.687 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6653 - accuracy: 0.5956\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6234 - accuracy: 0.6689\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.669 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6566 - accuracy: 0.6202\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6306 - accuracy: 0.6660\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.666 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6611 - accuracy: 0.6079\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6127 - accuracy: 0.6816\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.682 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6639 - accuracy: 0.5992\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6357 - accuracy: 0.6588\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.659 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6586 - accuracy: 0.6180\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6351 - accuracy: 0.6548\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.655 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6723 - accuracy: 0.5865\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6195 - accuracy: 0.6766\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.677 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6690 - accuracy: 0.5941\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6487 - accuracy: 0.6402\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.640 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6600 - accuracy: 0.6104\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6258 - accuracy: 0.6626\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.663 total time=   7.6s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6635 - accuracy: 0.5999\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6343 - accuracy: 0.6595\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.660 total time=   8.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6712 - accuracy: 0.5760\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6250 - accuracy: 0.6735\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6603 - accuracy: 0.6112\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6253 - accuracy: 0.6671\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.667 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6663 - accuracy: 0.6016\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6190 - accuracy: 0.6772\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6592 - accuracy: 0.6108\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6265 - accuracy: 0.6684\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.668 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6601 - accuracy: 0.6110\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6303 - accuracy: 0.6654\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.665 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6689 - accuracy: 0.5958\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6206 - accuracy: 0.6799\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.680 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6642 - accuracy: 0.6048\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6342 - accuracy: 0.6561\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.656 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6628 - accuracy: 0.6003\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6361 - accuracy: 0.6683\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.668 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6634 - accuracy: 0.6067\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6227 - accuracy: 0.6660\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.666 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6589 - accuracy: 0.6167\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6236 - accuracy: 0.6762\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.676 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6572 - accuracy: 0.6113\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6304 - accuracy: 0.6723\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.672 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6595 - accuracy: 0.6176\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6326 - accuracy: 0.6637\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.664 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6587 - accuracy: 0.6071\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6217 - accuracy: 0.6763\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.676 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6601 - accuracy: 0.6153\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6356 - accuracy: 0.6524\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.652 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6599 - accuracy: 0.6156\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6280 - accuracy: 0.6786\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.679 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6603 - accuracy: 0.6155\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6728\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.673 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6577 - accuracy: 0.6160\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.6699\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6574 - accuracy: 0.6150\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6159 - accuracy: 0.6793\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.679 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6585 - accuracy: 0.6198\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6230 - accuracy: 0.6755\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.675 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6545 - accuracy: 0.6191\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6354 - accuracy: 0.6594\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.659 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6590 - accuracy: 0.6129\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6156 - accuracy: 0.6826\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.683 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6542 - accuracy: 0.6188\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6314 - accuracy: 0.6713\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.671 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6594 - accuracy: 0.6199\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6263 - accuracy: 0.6748\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.675 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6611 - accuracy: 0.6115\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6339 - accuracy: 0.6704\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.670 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6576 - accuracy: 0.6196\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6282 - accuracy: 0.6775\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.678 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6536 - accuracy: 0.6269\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6229 - accuracy: 0.6709\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.671 total time=   8.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6597 - accuracy: 0.6190\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6161 - accuracy: 0.6806\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.681 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6574 - accuracy: 0.6181\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6266 - accuracy: 0.6671\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.667 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6566 - accuracy: 0.6179\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6489 - accuracy: 0.6279\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.628 total time=   8.3s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6575 - accuracy: 0.6216\n",
      "354/354 [==============================] - 3s 4ms/step - loss: 0.6122 - accuracy: 0.6829\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.683 total time=   9.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6570 - accuracy: 0.6165\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6243 - accuracy: 0.6711\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.671 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6550 - accuracy: 0.6254\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6253 - accuracy: 0.6667\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.667 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6568 - accuracy: 0.6247\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6517 - accuracy: 0.6364\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.636 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6651 - accuracy: 0.6015\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6243 - accuracy: 0.6645\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.665 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6527 - accuracy: 0.6270\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6341 - accuracy: 0.6621\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.662 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6614 - accuracy: 0.6109\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6226 - accuracy: 0.6739\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6584 - accuracy: 0.6149\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6296 - accuracy: 0.6766\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.677 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6572 - accuracy: 0.6169\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6524 - accuracy: 0.6024\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.602 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6597 - accuracy: 0.6099\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6150 - accuracy: 0.6831\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.683 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6603 - accuracy: 0.6113\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6266 - accuracy: 0.6673\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.667 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6612 - accuracy: 0.6100\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6356 - accuracy: 0.6561\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.656 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6631 - accuracy: 0.6081\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6303 - accuracy: 0.6819\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.682 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6573 - accuracy: 0.6214\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6689\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6576 - accuracy: 0.6196\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6332 - accuracy: 0.6553\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.655 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6524 - accuracy: 0.6260\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6113 - accuracy: 0.6816\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.682 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6569 - accuracy: 0.6153\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6352 - accuracy: 0.6528\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.653 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6537 - accuracy: 0.6246\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6264 - accuracy: 0.6721\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.672 total time=   8.2s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6561 - accuracy: 0.6172\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6161 - accuracy: 0.6842\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.684 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6525 - accuracy: 0.6241\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6227 - accuracy: 0.6775\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=64, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.678 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6595 - accuracy: 0.6187\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6470 - accuracy: 0.6303\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.630 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6715 - accuracy: 0.5871\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6181 - accuracy: 0.6800\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.680 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6605 - accuracy: 0.6104\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6225 - accuracy: 0.6777\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.678 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6589 - accuracy: 0.6118\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6683\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.668 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6601 - accuracy: 0.6121\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6232 - accuracy: 0.6600\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.660 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6607 - accuracy: 0.6104\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6212 - accuracy: 0.6791\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.679 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6572 - accuracy: 0.6148\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6285 - accuracy: 0.6740\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.674 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6644 - accuracy: 0.5984\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6378 - accuracy: 0.6552\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.655 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6593 - accuracy: 0.6121\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6292 - accuracy: 0.6666\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.667 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6559 - accuracy: 0.6227\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6411 - accuracy: 0.6536\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.654 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6656 - accuracy: 0.5993\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6132 - accuracy: 0.6841\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.684 total time=   8.1s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6649 - accuracy: 0.5987\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6278 - accuracy: 0.6658\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.666 total time=   9.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6577 - accuracy: 0.6181\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6258 - accuracy: 0.6745\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.675 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6658 - accuracy: 0.6026\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6300 - accuracy: 0.6688\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.669 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6549 - accuracy: 0.6229\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6258 - accuracy: 0.6691\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.669 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6588 - accuracy: 0.6126\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6318 - accuracy: 0.6528\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.653 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6616 - accuracy: 0.6077\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6157 - accuracy: 0.6848\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.685 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6567 - accuracy: 0.6169\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6223 - accuracy: 0.6774\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.677 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6571 - accuracy: 0.6200\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6349 - accuracy: 0.6517\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.652 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6654 - accuracy: 0.6031\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6249 - accuracy: 0.6713\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.671 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6630 - accuracy: 0.6037\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6467 - accuracy: 0.6299\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.630 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6580 - accuracy: 0.6198\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6500 - accuracy: 0.6338\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.634 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6569 - accuracy: 0.6174\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6118 - accuracy: 0.6854\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.685 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6565 - accuracy: 0.6195\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6263 - accuracy: 0.6769\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.677 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6508 - accuracy: 0.6259\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6230 - accuracy: 0.6719\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.672 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6583 - accuracy: 0.6139\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6367 - accuracy: 0.6663\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.666 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6571 - accuracy: 0.6124\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6211 - accuracy: 0.6784\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.678 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6616 - accuracy: 0.6095\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6345 - accuracy: 0.6675\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.667 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6683 - accuracy: 0.5955\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6327 - accuracy: 0.6711\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.671 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6595 - accuracy: 0.6198\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6203 - accuracy: 0.6743\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6601 - accuracy: 0.6099\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6638\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.664 total time=   8.5s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6652 - accuracy: 0.5995\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6214 - accuracy: 0.6695\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.669 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6659 - accuracy: 0.5953\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6311 - accuracy: 0.6656\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.666 total time=   8.4s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6564 - accuracy: 0.6179\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6272 - accuracy: 0.6603\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.660 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6576 - accuracy: 0.6191\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6251 - accuracy: 0.6720\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.672 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6557 - accuracy: 0.6167\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6353 - accuracy: 0.6610\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.661 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6599 - accuracy: 0.6141\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6238 - accuracy: 0.6723\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.672 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6612 - accuracy: 0.6132\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6242 - accuracy: 0.6744\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6598 - accuracy: 0.6121\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6402 - accuracy: 0.6445\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.644 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6522 - accuracy: 0.6281\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6290 - accuracy: 0.6683\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.668 total time=   8.1s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6620 - accuracy: 0.6083\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6174 - accuracy: 0.6827\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.683 total time=   8.0s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6559 - accuracy: 0.6257\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6246 - accuracy: 0.6698\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.670 total time=   9.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6541 - accuracy: 0.6163\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6435 - accuracy: 0.6480\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.648 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6567 - accuracy: 0.6178\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6192 - accuracy: 0.6785\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.679 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6513 - accuracy: 0.6281\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6558 - accuracy: 0.5954\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.595 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6595 - accuracy: 0.6196\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6408 - accuracy: 0.6312\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.631 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6689 - accuracy: 0.5951\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6193 - accuracy: 0.6812\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.681 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6552 - accuracy: 0.6229\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6249 - accuracy: 0.6699\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.670 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6531 - accuracy: 0.6250\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6260 - accuracy: 0.6675\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.668 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6571 - accuracy: 0.6206\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6296 - accuracy: 0.6663\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.666 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6546 - accuracy: 0.6275\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6195 - accuracy: 0.6745\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6548 - accuracy: 0.6209\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6218 - accuracy: 0.6744\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.674 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6549 - accuracy: 0.6202\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6262 - accuracy: 0.6755\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.675 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6504 - accuracy: 0.6325\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6191 - accuracy: 0.6770\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.677 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6574 - accuracy: 0.6188\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6224 - accuracy: 0.6738\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6620 - accuracy: 0.6053\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6206 - accuracy: 0.6797\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.680 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6580 - accuracy: 0.6182\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6216 - accuracy: 0.6770\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.677 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6516 - accuracy: 0.6301\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6346 - accuracy: 0.6613\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.661 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6584 - accuracy: 0.6210\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6298 - accuracy: 0.6794\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.679 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6547 - accuracy: 0.6233\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6357 - accuracy: 0.6738\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6527 - accuracy: 0.6305\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6311 - accuracy: 0.6739\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.674 total time=   8.2s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6544 - accuracy: 0.6203\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6198 - accuracy: 0.6842\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.684 total time=   7.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6568 - accuracy: 0.6198\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.6734\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6554 - accuracy: 0.6204\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6305 - accuracy: 0.6675\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.668 total time=   8.1s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6640 - accuracy: 0.6080\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6232 - accuracy: 0.6817\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.682 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6590 - accuracy: 0.6161\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6279 - accuracy: 0.6691\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.669 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6519 - accuracy: 0.6321\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6368 - accuracy: 0.6569\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.657 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6547 - accuracy: 0.6269\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6145 - accuracy: 0.6837\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.684 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6537 - accuracy: 0.6237\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6193 - accuracy: 0.6746\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.675 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6571 - accuracy: 0.6197\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6315 - accuracy: 0.6585\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.658 total time=   8.3s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6576 - accuracy: 0.6169\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6182 - accuracy: 0.6709\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.671 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6529 - accuracy: 0.6240\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6296 - accuracy: 0.6650\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.665 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6534 - accuracy: 0.6287\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6249 - accuracy: 0.6717\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.672 total time=   9.9s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6571 - accuracy: 0.6227\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6251 - accuracy: 0.6811\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.681 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6605 - accuracy: 0.6095\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6241 - accuracy: 0.6715\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.671 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6523 - accuracy: 0.6288\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6269 - accuracy: 0.6652\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.665 total time=   8.4s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6585 - accuracy: 0.6141\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6166 - accuracy: 0.6827\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.683 total time=   8.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6524 - accuracy: 0.6269\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6291 - accuracy: 0.6668\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.667 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6528 - accuracy: 0.6269\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6402 - accuracy: 0.6420\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.642 total time=   8.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6562 - accuracy: 0.6173\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6249 - accuracy: 0.6682\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.668 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6526 - accuracy: 0.6221\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6221 - accuracy: 0.6723\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=128, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.672 total time=   8.5s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6547 - accuracy: 0.6298\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6220 - accuracy: 0.6735\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.673 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6651 - accuracy: 0.6042\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6287 - accuracy: 0.6833\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.683 total time=   8.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6620 - accuracy: 0.6125\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6658 - accuracy: 0.5688\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=32;, score=0.569 total time=   8.5s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6647 - accuracy: 0.5937\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6512\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.651 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6608 - accuracy: 0.6143\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6190 - accuracy: 0.6844\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.684 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6618 - accuracy: 0.6136\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6219 - accuracy: 0.6746\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=64;, score=0.675 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6509 - accuracy: 0.6314\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6389 - accuracy: 0.6582\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.658 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6638 - accuracy: 0.5999\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6249 - accuracy: 0.6819\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.682 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6609 - accuracy: 0.6104\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6242 - accuracy: 0.6644\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=64, n_nodes5=128;, score=0.664 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6598 - accuracy: 0.6103\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6292 - accuracy: 0.6666\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.667 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6638 - accuracy: 0.6129\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6289 - accuracy: 0.6525\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.653 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6587 - accuracy: 0.6174\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6338 - accuracy: 0.6586\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=32;, score=0.659 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6574 - accuracy: 0.6157\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6310 - accuracy: 0.6554\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.655 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6625 - accuracy: 0.6041\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6168 - accuracy: 0.6787\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.679 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6505 - accuracy: 0.6314\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6179 - accuracy: 0.6771\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=64;, score=0.677 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6505 - accuracy: 0.6308\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6713\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.671 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6577 - accuracy: 0.6188\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6215 - accuracy: 0.6840\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.684 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6533 - accuracy: 0.6240\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6343 - accuracy: 0.6714\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=128, n_nodes5=128;, score=0.671 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6567 - accuracy: 0.6231\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6255 - accuracy: 0.6630\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.663 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6617 - accuracy: 0.6121\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6185 - accuracy: 0.6801\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.680 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6616 - accuracy: 0.6077\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6255 - accuracy: 0.6781\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=32;, score=0.678 total time=   7.7s\n",
      "708/708 [==============================] - 7s 8ms/step - loss: 0.6530 - accuracy: 0.6226\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6233 - accuracy: 0.6712\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   8.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6598 - accuracy: 0.6172\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6201 - accuracy: 0.6821\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.682 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6507 - accuracy: 0.6307\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6278 - accuracy: 0.6739\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=64;, score=0.674 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6535 - accuracy: 0.6216\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6337 - accuracy: 0.6624\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.662 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6580 - accuracy: 0.6162\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6165 - accuracy: 0.6809\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.681 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6554 - accuracy: 0.6199\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6320 - accuracy: 0.6712\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=128, n_nodes4=256, n_nodes5=128;, score=0.671 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6596 - accuracy: 0.6137\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6251 - accuracy: 0.6664\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.666 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6604 - accuracy: 0.6153\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6262 - accuracy: 0.6610\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.661 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6626 - accuracy: 0.6132\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6267 - accuracy: 0.6729\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=32;, score=0.673 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6503 - accuracy: 0.6327\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6336 - accuracy: 0.6586\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.659 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6654 - accuracy: 0.5979\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6247 - accuracy: 0.6711\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.671 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6584 - accuracy: 0.6149\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6247 - accuracy: 0.6696\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=64;, score=0.670 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6559 - accuracy: 0.6190\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6282 - accuracy: 0.6654\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.665 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6533 - accuracy: 0.6258\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6292 - accuracy: 0.6675\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.667 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6574 - accuracy: 0.6181\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6356 - accuracy: 0.6591\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=64, n_nodes5=128;, score=0.659 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6604 - accuracy: 0.6110\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6422 - accuracy: 0.6494\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.649 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6580 - accuracy: 0.6191\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6294 - accuracy: 0.6566\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.657 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6568 - accuracy: 0.6193\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6227 - accuracy: 0.6726\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=32;, score=0.673 total time=   7.7s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6504 - accuracy: 0.6334\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6240 - accuracy: 0.6698\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.670 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6583 - accuracy: 0.6186\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6170 - accuracy: 0.6804\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.680 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6529 - accuracy: 0.6283\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6344 - accuracy: 0.6581\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=64;, score=0.658 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6518 - accuracy: 0.6273\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6236 - accuracy: 0.6744\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.674 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6562 - accuracy: 0.6211\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6230 - accuracy: 0.6759\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.676 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6513 - accuracy: 0.6306\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6251 - accuracy: 0.6718\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=128, n_nodes5=128;, score=0.672 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6552 - accuracy: 0.6205\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6312 - accuracy: 0.6755\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6559 - accuracy: 0.6293\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6321 - accuracy: 0.6814\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.681 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6562 - accuracy: 0.6259\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6342 - accuracy: 0.6747\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=32;, score=0.675 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6513 - accuracy: 0.6290\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6239 - accuracy: 0.6703\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.670 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6567 - accuracy: 0.6221\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6438 - accuracy: 0.6489\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.649 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6523 - accuracy: 0.6278\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6425 - accuracy: 0.6347\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=64;, score=0.635 total time=   7.8s\n",
      "708/708 [==============================] - 8s 9ms/step - loss: 0.6498 - accuracy: 0.6338\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6218 - accuracy: 0.6769\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.677 total time=   9.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6545 - accuracy: 0.6200\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6188 - accuracy: 0.6845\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.685 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6530 - accuracy: 0.6270\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6301 - accuracy: 0.6677\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=256, n_nodes4=256, n_nodes5=128;, score=0.668 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6549 - accuracy: 0.6319\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6305 - accuracy: 0.6696\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.670 total time=   8.0s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6622 - accuracy: 0.6124\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6233 - accuracy: 0.6720\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.672 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6593 - accuracy: 0.6132\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6226 - accuracy: 0.6757\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=32;, score=0.676 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6559 - accuracy: 0.6207\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6450 - accuracy: 0.6397\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.640 total time=   7.8s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6591 - accuracy: 0.6118\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6188 - accuracy: 0.6736\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.674 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6570 - accuracy: 0.6201\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6370 - accuracy: 0.6586\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=64;, score=0.659 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6505 - accuracy: 0.6309\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6333 - accuracy: 0.6734\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.673 total time=   7.6s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6576 - accuracy: 0.6206\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6246 - accuracy: 0.6774\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.677 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6559 - accuracy: 0.6215\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6249 - accuracy: 0.6775\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=64, n_nodes5=128;, score=0.678 total time=   8.3s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6541 - accuracy: 0.6257\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6624\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.662 total time=   8.2s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6561 - accuracy: 0.6248\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6229 - accuracy: 0.6706\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.671 total time=   8.1s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6566 - accuracy: 0.6209\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6295 - accuracy: 0.6745\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=32;, score=0.674 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6507 - accuracy: 0.6350\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6324 - accuracy: 0.6590\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.659 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6557 - accuracy: 0.6246\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6277 - accuracy: 0.6645\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.665 total time=   7.8s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6555 - accuracy: 0.6238\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6230 - accuracy: 0.6770\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=64;, score=0.677 total time=   8.0s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6504 - accuracy: 0.6285\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6331 - accuracy: 0.6565\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.657 total time=   7.6s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6540 - accuracy: 0.6285\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6822\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.682 total time=   7.7s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6509 - accuracy: 0.6326\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6538\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=128, n_nodes5=128;, score=0.654 total time=   7.9s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6541 - accuracy: 0.6323\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6251 - accuracy: 0.6686\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.669 total time=   7.7s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6547 - accuracy: 0.6268\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6604\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.660 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6497 - accuracy: 0.6372\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 0.6197 - accuracy: 0.6741\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=32;, score=0.674 total time=   7.6s\n",
      "708/708 [==============================] - 6s 9ms/step - loss: 0.6498 - accuracy: 0.6325\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6226 - accuracy: 0.6711\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.671 total time=   7.9s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6558 - accuracy: 0.6228\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6329 - accuracy: 0.6602\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.660 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6523 - accuracy: 0.6322\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6237 - accuracy: 0.6666\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=64;, score=0.667 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6454 - accuracy: 0.6400\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6241 - accuracy: 0.6674\n",
      "[CV 1/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.667 total time=   7.8s\n",
      "708/708 [==============================] - 6s 8ms/step - loss: 0.6540 - accuracy: 0.6257\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.6308 - accuracy: 0.6629\n",
      "[CV 2/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.663 total time=   8.0s\n",
      "708/708 [==============================] - 7s 9ms/step - loss: 0.6527 - accuracy: 0.6307\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 0.6300 - accuracy: 0.6723\n",
      "[CV 3/3] END activation=softmax, dropout=0.3, lr=0.001, n_nodes1=128, n_nodes2=256, n_nodes3=512, n_nodes4=256, n_nodes5=128;, score=0.672 total time=   8.0s\n",
      "1062/1062 [==============================] - 9s 9ms/step - loss: 0.6450 - accuracy: 0.6416\n",
      "Best Parameters: {'activation': 'sigmoid', 'dropout': 0.2, 'lr': 0.001, 'n_nodes1': 128, 'n_nodes2': 64, 'n_nodes3': 512, 'n_nodes4': 256, 'n_nodes5': 32}\n",
      "Best Accuracy: 0.68\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_model_opt_dense)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid_2, cv=3, verbose=3, n_jobs=1)\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "\n",
    "print(f'Best Parameters: {grid_result.best_params_}')\n",
    "print(f'Best Accuracy: {grid_result.best_score_:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1062/1062 [==============================] - 8s 7ms/step - loss: 0.6484 - accuracy: 0.6311 - val_loss: 0.6177 - val_accuracy: 0.6812 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "1062/1062 [==============================] - 7s 7ms/step - loss: 0.6331 - accuracy: 0.6611 - val_loss: 0.6348 - val_accuracy: 0.6810 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "1062/1062 [==============================] - 7s 7ms/step - loss: 0.6277 - accuracy: 0.6643 - val_loss: 0.6211 - val_accuracy: 0.6602 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "1062/1062 [==============================] - 7s 7ms/step - loss: 0.6237 - accuracy: 0.6701 - val_loss: 0.6126 - val_accuracy: 0.6856 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "1062/1062 [==============================] - 7s 6ms/step - loss: 0.6231 - accuracy: 0.6697 - val_loss: 0.6099 - val_accuracy: 0.6885 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "1062/1062 [==============================] - 7s 6ms/step - loss: 0.6207 - accuracy: 0.6723 - val_loss: 0.6074 - val_accuracy: 0.6898 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "1062/1062 [==============================] - 7s 7ms/step - loss: 0.6196 - accuracy: 0.6741 - val_loss: 0.6179 - val_accuracy: 0.6892 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "1062/1062 [==============================] - 7s 6ms/step - loss: 0.6198 - accuracy: 0.6742 - val_loss: 0.6146 - val_accuracy: 0.6895 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "1062/1062 [==============================] - 7s 7ms/step - loss: 0.6205 - accuracy: 0.6725 - val_loss: 0.6079 - val_accuracy: 0.6914 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "1062/1062 [==============================] - 7s 6ms/step - loss: 0.6221 - accuracy: 0.6707 - val_loss: 0.6169 - val_accuracy: 0.6684 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "1062/1062 [==============================] - 7s 7ms/step - loss: 0.6188 - accuracy: 0.6767 - val_loss: 0.6113 - val_accuracy: 0.6929 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "1062/1062 [==============================] - 7s 6ms/step - loss: 0.6133 - accuracy: 0.6834 - val_loss: 0.6055 - val_accuracy: 0.6924 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "1062/1062 [==============================] - 7s 6ms/step - loss: 0.6103 - accuracy: 0.6845 - val_loss: 0.6062 - val_accuracy: 0.6927 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "1062/1062 [==============================] - 7s 6ms/step - loss: 0.6104 - accuracy: 0.6845 - val_loss: 0.6051 - val_accuracy: 0.6942 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "1062/1062 [==============================] - 9s 8ms/step - loss: 0.6089 - accuracy: 0.6847 - val_loss: 0.6058 - val_accuracy: 0.6925 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "1062/1062 [==============================] - 9s 8ms/step - loss: 0.6095 - accuracy: 0.6849 - val_loss: 0.6063 - val_accuracy: 0.6923 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "1062/1062 [==============================] - 8s 8ms/step - loss: 0.6085 - accuracy: 0.6853 - val_loss: 0.6053 - val_accuracy: 0.6927 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "1062/1062 [==============================] - 8s 8ms/step - loss: 0.6082 - accuracy: 0.6859 - val_loss: 0.6057 - val_accuracy: 0.6947 - lr: 1.0000e-04\n",
      "Epoch 19/20\n",
      "1062/1062 [==============================] - 9s 8ms/step - loss: 0.6079 - accuracy: 0.6861 - val_loss: 0.6053 - val_accuracy: 0.6943 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "1062/1062 [==============================] - 8s 8ms/step - loss: 0.6080 - accuracy: 0.6864 - val_loss: 0.6054 - val_accuracy: 0.6947 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "dropout = 0.2\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(len(x_train.columns),)),\n",
    "    Dropout(dropout),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(dropout),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(dropout),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(dropout),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(dropout),\n",
    "    Dense(len(le.classes_), activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-03), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val), callbacks=[early_stop, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# plot training and validation loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332/332 [==============================] - 0s 1ms/step\n",
      "Accuracy: 0.6837494112105511\n",
      "F1-score: 0.6603258120004047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1c4610e0dc0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_display = ConfusionMatrixDisplay(cm, display_labels=le.classes_)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Calculating precision, recall, and F1-score\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"F1-score:\", f1_score)\n",
    "cm_display.plot(cmap=\"Reds\", xticks_rotation=90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
